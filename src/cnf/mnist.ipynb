{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = mpimg.imread('cameraman.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f29c41d4e80>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = torch.tensor(image, dtype=torch.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.4431]]]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.grid_sample(image_tensor.unsqueeze(0).unsqueeze(0), torch.tensor([[[[1., 1.]]]], dtype=torch.float32), align_corners=True, mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim = 1024):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc_in = nn.Linear(dim, hidden_dim)\n",
    "        self.layers = nn.Sequential(\n",
    "            *[\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "            ] * 8\n",
    "        )\n",
    "        self.fc_out = nn.Sequential(nn.Linear(hidden_dim, 1))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        x = F.gelu(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            x = x + F.gelu(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "    \n",
    "mlp = MLP(1024).cuda()\n",
    "image_tensor = image_tensor.cuda()\n",
    "\n",
    "adam = optim.Adam(mlp.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1, loss: 0.2550225853919983\n",
      "iteration 2, loss: 0.09700912237167358\n",
      "iteration 3, loss: 0.06845198571681976\n",
      "iteration 4, loss: 0.11017363518476486\n",
      "iteration 5, loss: 0.09397271275520325\n",
      "iteration 6, loss: 0.06277044117450714\n",
      "iteration 7, loss: 0.05902509391307831\n",
      "iteration 8, loss: 0.06891441345214844\n",
      "iteration 9, loss: 0.07611513137817383\n",
      "iteration 10, loss: 0.07731074094772339\n",
      "iteration 11, loss: 0.07263994961977005\n",
      "iteration 12, loss: 0.06680569052696228\n",
      "iteration 13, loss: 0.06288133561611176\n",
      "iteration 14, loss: 0.05986185371875763\n",
      "iteration 15, loss: 0.058744706213474274\n",
      "iteration 16, loss: 0.0650705099105835\n",
      "iteration 17, loss: 0.06347059458494186\n",
      "iteration 18, loss: 0.0632769986987114\n",
      "iteration 19, loss: 0.05838264152407646\n",
      "iteration 20, loss: 0.05439828336238861\n",
      "iteration 21, loss: 0.056892380118370056\n",
      "iteration 22, loss: 0.05560576543211937\n",
      "iteration 23, loss: 0.05710481107234955\n",
      "iteration 24, loss: 0.061673007905483246\n",
      "iteration 25, loss: 0.05550363287329674\n",
      "iteration 26, loss: 0.05489666759967804\n",
      "iteration 27, loss: 0.05590467154979706\n",
      "iteration 28, loss: 0.05223110690712929\n",
      "iteration 29, loss: 0.050946712493896484\n",
      "iteration 30, loss: 0.04761548340320587\n",
      "iteration 31, loss: 0.05282175540924072\n",
      "iteration 32, loss: 0.05489620566368103\n",
      "iteration 33, loss: 0.04834394156932831\n",
      "iteration 34, loss: 0.0481165274977684\n",
      "iteration 35, loss: 0.05138818174600601\n",
      "iteration 36, loss: 0.04891631752252579\n",
      "iteration 37, loss: 0.049407195299863815\n",
      "iteration 38, loss: 0.05049072206020355\n",
      "iteration 39, loss: 0.05222827568650246\n",
      "iteration 40, loss: 0.048236243426799774\n",
      "iteration 41, loss: 0.04508121311664581\n",
      "iteration 42, loss: 0.04488111287355423\n",
      "iteration 43, loss: 0.04591996967792511\n",
      "iteration 44, loss: 0.04556740075349808\n",
      "iteration 45, loss: 0.04538561403751373\n",
      "iteration 46, loss: 0.044826485216617584\n",
      "iteration 47, loss: 0.041041623800992966\n",
      "iteration 48, loss: 0.04253099486231804\n",
      "iteration 49, loss: 0.043430328369140625\n",
      "iteration 50, loss: 0.045710720121860504\n",
      "iteration 51, loss: 0.040307655930519104\n",
      "iteration 52, loss: 0.041599687188863754\n",
      "iteration 53, loss: 0.04315510392189026\n",
      "iteration 54, loss: 0.04175179451704025\n",
      "iteration 55, loss: 0.04000168666243553\n",
      "iteration 56, loss: 0.04372650012373924\n",
      "iteration 57, loss: 0.042249757796525955\n",
      "iteration 58, loss: 0.03688574582338333\n",
      "iteration 59, loss: 0.04118707403540611\n",
      "iteration 60, loss: 0.03987797349691391\n",
      "iteration 61, loss: 0.03668401017785072\n",
      "iteration 62, loss: 0.0388011708855629\n",
      "iteration 63, loss: 0.040153805166482925\n",
      "iteration 64, loss: 0.04076414555311203\n",
      "iteration 65, loss: 0.04243745654821396\n",
      "iteration 66, loss: 0.037621840834617615\n",
      "iteration 67, loss: 0.04023142531514168\n",
      "iteration 68, loss: 0.039606355130672455\n",
      "iteration 69, loss: 0.03790725767612457\n",
      "iteration 70, loss: 0.0372336246073246\n",
      "iteration 71, loss: 0.03809146583080292\n",
      "iteration 72, loss: 0.03648693859577179\n",
      "iteration 73, loss: 0.038962408900260925\n",
      "iteration 74, loss: 0.035993918776512146\n",
      "iteration 75, loss: 0.03599996119737625\n",
      "iteration 76, loss: 0.040081530809402466\n",
      "iteration 77, loss: 0.03661937266588211\n",
      "iteration 78, loss: 0.03845008462667465\n",
      "iteration 79, loss: 0.04019142687320709\n",
      "iteration 80, loss: 0.036030061542987823\n",
      "iteration 81, loss: 0.03819351643323898\n",
      "iteration 82, loss: 0.04087849333882332\n",
      "iteration 83, loss: 0.04054730385541916\n",
      "iteration 84, loss: 0.040625832974910736\n",
      "iteration 85, loss: 0.037245769053697586\n",
      "iteration 86, loss: 0.03834328055381775\n",
      "iteration 87, loss: 0.0380513072013855\n",
      "iteration 88, loss: 0.03541329875588417\n",
      "iteration 89, loss: 0.04083961248397827\n",
      "iteration 90, loss: 0.03885556384921074\n",
      "iteration 91, loss: 0.036500751972198486\n",
      "iteration 92, loss: 0.035664789378643036\n",
      "iteration 93, loss: 0.03439942002296448\n",
      "iteration 94, loss: 0.03396926820278168\n",
      "iteration 95, loss: 0.03675296530127525\n",
      "iteration 96, loss: 0.03720086067914963\n",
      "iteration 97, loss: 0.037181392312049866\n",
      "iteration 98, loss: 0.03553943336009979\n",
      "iteration 99, loss: 0.04037697613239288\n",
      "iteration 100, loss: 0.0403255820274353\n",
      "iteration 101, loss: 0.03995920717716217\n",
      "iteration 102, loss: 0.03444308042526245\n",
      "iteration 103, loss: 0.03452257439494133\n",
      "iteration 104, loss: 0.036857400089502335\n",
      "iteration 105, loss: 0.03704342991113663\n",
      "iteration 106, loss: 0.035650983452796936\n",
      "iteration 107, loss: 0.0368739515542984\n",
      "iteration 108, loss: 0.039310261607170105\n",
      "iteration 109, loss: 0.03727172315120697\n",
      "iteration 110, loss: 0.03527102246880531\n",
      "iteration 111, loss: 0.03642477095127106\n",
      "iteration 112, loss: 0.037405163049697876\n",
      "iteration 113, loss: 0.03397524729371071\n",
      "iteration 114, loss: 0.03804212436079979\n",
      "iteration 115, loss: 0.037300143390893936\n",
      "iteration 116, loss: 0.03887717425823212\n",
      "iteration 117, loss: 0.035366255789995193\n",
      "iteration 118, loss: 0.03681240230798721\n",
      "iteration 119, loss: 0.03562689572572708\n",
      "iteration 120, loss: 0.03688991814851761\n",
      "iteration 121, loss: 0.03785748779773712\n",
      "iteration 122, loss: 0.03710845112800598\n",
      "iteration 123, loss: 0.036940738558769226\n",
      "iteration 124, loss: 0.034732475876808167\n",
      "iteration 125, loss: 0.03552280366420746\n",
      "iteration 126, loss: 0.03426406905055046\n",
      "iteration 127, loss: 0.039051420986652374\n",
      "iteration 128, loss: 0.0350092276930809\n",
      "iteration 129, loss: 0.03508639335632324\n",
      "iteration 130, loss: 0.03479740023612976\n",
      "iteration 131, loss: 0.037072401493787766\n",
      "iteration 132, loss: 0.036332033574581146\n",
      "iteration 133, loss: 0.035346172749996185\n",
      "iteration 134, loss: 0.035473473370075226\n",
      "iteration 135, loss: 0.03331771492958069\n",
      "iteration 136, loss: 0.03379545733332634\n",
      "iteration 137, loss: 0.03563780337572098\n",
      "iteration 138, loss: 0.03542716056108475\n",
      "iteration 139, loss: 0.037793952971696854\n",
      "iteration 140, loss: 0.03470892831683159\n",
      "iteration 141, loss: 0.03593197092413902\n",
      "iteration 142, loss: 0.03920872509479523\n",
      "iteration 143, loss: 0.03502383083105087\n",
      "iteration 144, loss: 0.03489554300904274\n",
      "iteration 145, loss: 0.03322818875312805\n",
      "iteration 146, loss: 0.036107137799263\n",
      "iteration 147, loss: 0.03471561521291733\n",
      "iteration 148, loss: 0.036231525242328644\n",
      "iteration 149, loss: 0.03375740349292755\n",
      "iteration 150, loss: 0.03577658534049988\n",
      "iteration 151, loss: 0.03569064661860466\n",
      "iteration 152, loss: 0.03711013123393059\n",
      "iteration 153, loss: 0.03380768001079559\n",
      "iteration 154, loss: 0.03527965024113655\n",
      "iteration 155, loss: 0.03577960282564163\n",
      "iteration 156, loss: 0.03401581197977066\n",
      "iteration 157, loss: 0.036885589361190796\n",
      "iteration 158, loss: 0.03673747926950455\n",
      "iteration 159, loss: 0.033622294664382935\n",
      "iteration 160, loss: 0.03512146323919296\n",
      "iteration 161, loss: 0.03820924460887909\n",
      "iteration 162, loss: 0.03372568637132645\n",
      "iteration 163, loss: 0.033611804246902466\n",
      "iteration 164, loss: 0.03565780445933342\n",
      "iteration 165, loss: 0.03513116016983986\n",
      "iteration 166, loss: 0.03692622482776642\n",
      "iteration 167, loss: 0.034836817532777786\n",
      "iteration 168, loss: 0.03358541429042816\n",
      "iteration 169, loss: 0.0330984927713871\n",
      "iteration 170, loss: 0.03320084884762764\n",
      "iteration 171, loss: 0.034323979169130325\n",
      "iteration 172, loss: 0.032554350793361664\n",
      "iteration 173, loss: 0.0374164953827858\n",
      "iteration 174, loss: 0.03434944525361061\n",
      "iteration 175, loss: 0.03641363978385925\n",
      "iteration 176, loss: 0.036241985857486725\n",
      "iteration 177, loss: 0.036006003618240356\n",
      "iteration 178, loss: 0.03566466271877289\n",
      "iteration 179, loss: 0.03610502928495407\n",
      "iteration 180, loss: 0.0354798287153244\n",
      "iteration 181, loss: 0.0374593585729599\n",
      "iteration 182, loss: 0.03378425911068916\n",
      "iteration 183, loss: 0.03689081221818924\n",
      "iteration 184, loss: 0.03634694963693619\n",
      "iteration 185, loss: 0.03790336102247238\n",
      "iteration 186, loss: 0.034334007650613785\n",
      "iteration 187, loss: 0.035439491271972656\n",
      "iteration 188, loss: 0.03363289311528206\n",
      "iteration 189, loss: 0.03513675183057785\n",
      "iteration 190, loss: 0.0345851294696331\n",
      "iteration 191, loss: 0.03543032333254814\n",
      "iteration 192, loss: 0.03628493845462799\n",
      "iteration 193, loss: 0.03362620249390602\n",
      "iteration 194, loss: 0.033678628504276276\n",
      "iteration 195, loss: 0.035053811967372894\n",
      "iteration 196, loss: 0.03506910800933838\n",
      "iteration 197, loss: 0.03688226640224457\n",
      "iteration 198, loss: 0.034477632492780685\n",
      "iteration 199, loss: 0.03361620008945465\n",
      "iteration 200, loss: 0.03468620032072067\n",
      "iteration 201, loss: 0.030795887112617493\n",
      "iteration 202, loss: 0.032504841685295105\n",
      "iteration 203, loss: 0.03455318510532379\n",
      "iteration 204, loss: 0.0327649861574173\n",
      "iteration 205, loss: 0.03504278510808945\n",
      "iteration 206, loss: 0.03151681646704674\n",
      "iteration 207, loss: 0.03356854245066643\n",
      "iteration 208, loss: 0.03103826940059662\n",
      "iteration 209, loss: 0.03588898852467537\n",
      "iteration 210, loss: 0.03729839622974396\n",
      "iteration 211, loss: 0.035661131143569946\n",
      "iteration 212, loss: 0.03656372055411339\n",
      "iteration 213, loss: 0.03484490141272545\n",
      "iteration 214, loss: 0.033561334013938904\n",
      "iteration 215, loss: 0.036152154207229614\n",
      "iteration 216, loss: 0.034219324588775635\n",
      "iteration 217, loss: 0.033515941351652145\n",
      "iteration 218, loss: 0.033711254596710205\n",
      "iteration 219, loss: 0.03280138224363327\n",
      "iteration 220, loss: 0.03457524627447128\n",
      "iteration 221, loss: 0.032975830137729645\n",
      "iteration 222, loss: 0.03548332303762436\n",
      "iteration 223, loss: 0.03454679250717163\n",
      "iteration 224, loss: 0.0340726301074028\n",
      "iteration 225, loss: 0.03587763383984566\n",
      "iteration 226, loss: 0.03482632711529732\n",
      "iteration 227, loss: 0.0346795991063118\n",
      "iteration 228, loss: 0.032235920429229736\n",
      "iteration 229, loss: 0.035298094153404236\n",
      "iteration 230, loss: 0.033411599695682526\n",
      "iteration 231, loss: 0.03479095548391342\n",
      "iteration 232, loss: 0.030841603875160217\n",
      "iteration 233, loss: 0.03505593538284302\n",
      "iteration 234, loss: 0.035050347447395325\n",
      "iteration 235, loss: 0.036970458924770355\n",
      "iteration 236, loss: 0.03536839038133621\n",
      "iteration 237, loss: 0.034037262201309204\n",
      "iteration 238, loss: 0.035299547016620636\n",
      "iteration 239, loss: 0.03275461122393608\n",
      "iteration 240, loss: 0.03591058403253555\n",
      "iteration 241, loss: 0.033551156520843506\n",
      "iteration 242, loss: 0.033238112926483154\n",
      "iteration 243, loss: 0.03279300034046173\n",
      "iteration 244, loss: 0.03430186212062836\n",
      "iteration 245, loss: 0.035555850714445114\n",
      "iteration 246, loss: 0.03369763121008873\n",
      "iteration 247, loss: 0.03586076945066452\n",
      "iteration 248, loss: 0.03346581012010574\n",
      "iteration 249, loss: 0.03593912348151207\n",
      "iteration 250, loss: 0.033503491431474686\n",
      "iteration 251, loss: 0.029933258891105652\n",
      "iteration 252, loss: 0.032348789274692535\n",
      "iteration 253, loss: 0.03126681223511696\n",
      "iteration 254, loss: 0.03486068546772003\n",
      "iteration 255, loss: 0.03355606645345688\n",
      "iteration 256, loss: 0.03471675142645836\n",
      "iteration 257, loss: 0.0365898534655571\n",
      "iteration 258, loss: 0.037324026226997375\n",
      "iteration 259, loss: 0.03253135830163956\n",
      "iteration 260, loss: 0.03428461775183678\n",
      "iteration 261, loss: 0.03552546352148056\n",
      "iteration 262, loss: 0.03445328027009964\n",
      "iteration 263, loss: 0.034207116812467575\n",
      "iteration 264, loss: 0.03300604596734047\n",
      "iteration 265, loss: 0.03400479257106781\n",
      "iteration 266, loss: 0.03469249606132507\n",
      "iteration 267, loss: 0.032242532819509506\n",
      "iteration 268, loss: 0.03286918252706528\n",
      "iteration 269, loss: 0.03410555422306061\n",
      "iteration 270, loss: 0.036365918815135956\n",
      "iteration 271, loss: 0.03773784637451172\n",
      "iteration 272, loss: 0.0322209969162941\n",
      "iteration 273, loss: 0.03069588914513588\n",
      "iteration 274, loss: 0.03265616297721863\n",
      "iteration 275, loss: 0.0362456813454628\n",
      "iteration 276, loss: 0.033298566937446594\n",
      "iteration 277, loss: 0.034500882029533386\n",
      "iteration 278, loss: 0.03349326550960541\n",
      "iteration 279, loss: 0.03297312557697296\n",
      "iteration 280, loss: 0.029844898730516434\n",
      "iteration 281, loss: 0.03223299980163574\n",
      "iteration 282, loss: 0.037458159029483795\n",
      "iteration 283, loss: 0.0324314683675766\n",
      "iteration 284, loss: 0.03223567456007004\n",
      "iteration 285, loss: 0.0357765331864357\n",
      "iteration 286, loss: 0.03339935839176178\n",
      "iteration 287, loss: 0.03047250397503376\n",
      "iteration 288, loss: 0.03337806835770607\n",
      "iteration 289, loss: 0.03674027696251869\n",
      "iteration 290, loss: 0.0334026962518692\n",
      "iteration 291, loss: 0.03187832236289978\n",
      "iteration 292, loss: 0.03745777904987335\n",
      "iteration 293, loss: 0.03594895079731941\n",
      "iteration 294, loss: 0.03241606801748276\n",
      "iteration 295, loss: 0.03236394375562668\n",
      "iteration 296, loss: 0.03274079039692879\n",
      "iteration 297, loss: 0.03425519913434982\n",
      "iteration 298, loss: 0.03424416482448578\n",
      "iteration 299, loss: 0.032280534505844116\n",
      "iteration 300, loss: 0.03641393035650253\n",
      "iteration 301, loss: 0.035449493676424026\n",
      "iteration 302, loss: 0.035148363560438156\n",
      "iteration 303, loss: 0.03172063082456589\n",
      "iteration 304, loss: 0.03357154503464699\n",
      "iteration 305, loss: 0.03519789129495621\n",
      "iteration 306, loss: 0.03296991065144539\n",
      "iteration 307, loss: 0.03367847949266434\n",
      "iteration 308, loss: 0.030729591846466064\n",
      "iteration 309, loss: 0.03221134468913078\n",
      "iteration 310, loss: 0.03486219793558121\n",
      "iteration 311, loss: 0.03318215161561966\n",
      "iteration 312, loss: 0.030543072149157524\n",
      "iteration 313, loss: 0.033022500574588776\n",
      "iteration 314, loss: 0.03446357324719429\n",
      "iteration 315, loss: 0.034200519323349\n",
      "iteration 316, loss: 0.03320109844207764\n",
      "iteration 317, loss: 0.034583039581775665\n",
      "iteration 318, loss: 0.03324351832270622\n",
      "iteration 319, loss: 0.030948203057050705\n",
      "iteration 320, loss: 0.03219913691282272\n",
      "iteration 321, loss: 0.029443947598338127\n",
      "iteration 322, loss: 0.034033238887786865\n",
      "iteration 323, loss: 0.03604814410209656\n",
      "iteration 324, loss: 0.032328881323337555\n",
      "iteration 325, loss: 0.035585880279541016\n",
      "iteration 326, loss: 0.03203200176358223\n",
      "iteration 327, loss: 0.03081219084560871\n",
      "iteration 328, loss: 0.036763884127140045\n",
      "iteration 329, loss: 0.03365565091371536\n",
      "iteration 330, loss: 0.03326039016246796\n",
      "iteration 331, loss: 0.03277812525629997\n",
      "iteration 332, loss: 0.03273545950651169\n",
      "iteration 333, loss: 0.030558664351701736\n",
      "iteration 334, loss: 0.03100338764488697\n",
      "iteration 335, loss: 0.02896297536790371\n",
      "iteration 336, loss: 0.033262886106967926\n",
      "iteration 337, loss: 0.03090752474963665\n",
      "iteration 338, loss: 0.03210907429456711\n",
      "iteration 339, loss: 0.03396568447351456\n",
      "iteration 340, loss: 0.034706249833106995\n",
      "iteration 341, loss: 0.031915970146656036\n",
      "iteration 342, loss: 0.03276268392801285\n",
      "iteration 343, loss: 0.03139167279005051\n",
      "iteration 344, loss: 0.030678559094667435\n",
      "iteration 345, loss: 0.0356355682015419\n",
      "iteration 346, loss: 0.037643395364284515\n",
      "iteration 347, loss: 0.03416727855801582\n",
      "iteration 348, loss: 0.030641619116067886\n",
      "iteration 349, loss: 0.03070094808936119\n",
      "iteration 350, loss: 0.03359678387641907\n",
      "iteration 351, loss: 0.03158992901444435\n",
      "iteration 352, loss: 0.033773139119148254\n",
      "iteration 353, loss: 0.035862818360328674\n",
      "iteration 354, loss: 0.029922205954790115\n",
      "iteration 355, loss: 0.029226798564195633\n",
      "iteration 356, loss: 0.03200533241033554\n",
      "iteration 357, loss: 0.03158099204301834\n",
      "iteration 358, loss: 0.03144833818078041\n",
      "iteration 359, loss: 0.03262799233198166\n",
      "iteration 360, loss: 0.031087156385183334\n",
      "iteration 361, loss: 0.032515525817871094\n",
      "iteration 362, loss: 0.03418422117829323\n",
      "iteration 363, loss: 0.031298622488975525\n",
      "iteration 364, loss: 0.028657831251621246\n",
      "iteration 365, loss: 0.031235694885253906\n",
      "iteration 366, loss: 0.03156963735818863\n",
      "iteration 367, loss: 0.02990838512778282\n",
      "iteration 368, loss: 0.030071517452597618\n",
      "iteration 369, loss: 0.030999671667814255\n",
      "iteration 370, loss: 0.030039966106414795\n",
      "iteration 371, loss: 0.036309510469436646\n",
      "iteration 372, loss: 0.03370898962020874\n",
      "iteration 373, loss: 0.032073162496089935\n",
      "iteration 374, loss: 0.03166916221380234\n",
      "iteration 375, loss: 0.03050481155514717\n",
      "iteration 376, loss: 0.03000285103917122\n",
      "iteration 377, loss: 0.033565983176231384\n",
      "iteration 378, loss: 0.03010217472910881\n",
      "iteration 379, loss: 0.03131391480565071\n",
      "iteration 380, loss: 0.03226636350154877\n",
      "iteration 381, loss: 0.03329771012067795\n",
      "iteration 382, loss: 0.0297466441988945\n",
      "iteration 383, loss: 0.030611641705036163\n",
      "iteration 384, loss: 0.030621811747550964\n",
      "iteration 385, loss: 0.03632308915257454\n",
      "iteration 386, loss: 0.03404334560036659\n",
      "iteration 387, loss: 0.030122030526399612\n",
      "iteration 388, loss: 0.029600515961647034\n",
      "iteration 389, loss: 0.031181547790765762\n",
      "iteration 390, loss: 0.034541867673397064\n",
      "iteration 391, loss: 0.03183245658874512\n",
      "iteration 392, loss: 0.02929730713367462\n",
      "iteration 393, loss: 0.027819756418466568\n",
      "iteration 394, loss: 0.027804583311080933\n",
      "iteration 395, loss: 0.0323224812746048\n",
      "iteration 396, loss: 0.028969012200832367\n",
      "iteration 397, loss: 0.03347456455230713\n",
      "iteration 398, loss: 0.03171706944704056\n",
      "iteration 399, loss: 0.032102763652801514\n",
      "iteration 400, loss: 0.034381426870822906\n",
      "iteration 401, loss: 0.03283793479204178\n",
      "iteration 402, loss: 0.032429151237010956\n",
      "iteration 403, loss: 0.03238885849714279\n",
      "iteration 404, loss: 0.029654450714588165\n",
      "iteration 405, loss: 0.03268415480852127\n",
      "iteration 406, loss: 0.03133268654346466\n",
      "iteration 407, loss: 0.030325042083859444\n",
      "iteration 408, loss: 0.03052983246743679\n",
      "iteration 409, loss: 0.030764766037464142\n",
      "iteration 410, loss: 0.029006099328398705\n",
      "iteration 411, loss: 0.03308422490954399\n",
      "iteration 412, loss: 0.029934119433164597\n",
      "iteration 413, loss: 0.030384808778762817\n",
      "iteration 414, loss: 0.03081558458507061\n",
      "iteration 415, loss: 0.030992213636636734\n",
      "iteration 416, loss: 0.032248642295598984\n",
      "iteration 417, loss: 0.03328799456357956\n",
      "iteration 418, loss: 0.02938341721892357\n",
      "iteration 419, loss: 0.03136778250336647\n",
      "iteration 420, loss: 0.027061082422733307\n",
      "iteration 421, loss: 0.03226659446954727\n",
      "iteration 422, loss: 0.030448436737060547\n",
      "iteration 423, loss: 0.03090669773519039\n",
      "iteration 424, loss: 0.028150763362646103\n",
      "iteration 425, loss: 0.029621854424476624\n",
      "iteration 426, loss: 0.030692745000123978\n",
      "iteration 427, loss: 0.028947487473487854\n",
      "iteration 428, loss: 0.03236585855484009\n",
      "iteration 429, loss: 0.02805580198764801\n",
      "iteration 430, loss: 0.02915174886584282\n",
      "iteration 431, loss: 0.029863644391298294\n",
      "iteration 432, loss: 0.032412152737379074\n",
      "iteration 433, loss: 0.033818311989307404\n",
      "iteration 434, loss: 0.03214284032583237\n",
      "iteration 435, loss: 0.030769214034080505\n",
      "iteration 436, loss: 0.030647438019514084\n",
      "iteration 437, loss: 0.03103329800069332\n",
      "iteration 438, loss: 0.032120391726493835\n",
      "iteration 439, loss: 0.0308822151273489\n",
      "iteration 440, loss: 0.03234484791755676\n",
      "iteration 441, loss: 0.028915926814079285\n",
      "iteration 442, loss: 0.03188486397266388\n",
      "iteration 443, loss: 0.031239323318004608\n",
      "iteration 444, loss: 0.032583560794591904\n",
      "iteration 445, loss: 0.02998272515833378\n",
      "iteration 446, loss: 0.031104309484362602\n",
      "iteration 447, loss: 0.031599029898643494\n",
      "iteration 448, loss: 0.026721656322479248\n",
      "iteration 449, loss: 0.026047982275485992\n",
      "iteration 450, loss: 0.0291580930352211\n",
      "iteration 451, loss: 0.030354905873537064\n",
      "iteration 452, loss: 0.03063894808292389\n",
      "iteration 453, loss: 0.02894364297389984\n",
      "iteration 454, loss: 0.030397333204746246\n",
      "iteration 455, loss: 0.02810356393456459\n",
      "iteration 456, loss: 0.028888152912259102\n",
      "iteration 457, loss: 0.028516478836536407\n",
      "iteration 458, loss: 0.029847370460629463\n",
      "iteration 459, loss: 0.032237112522125244\n",
      "iteration 460, loss: 0.03050164505839348\n",
      "iteration 461, loss: 0.03018210455775261\n",
      "iteration 462, loss: 0.029073746874928474\n",
      "iteration 463, loss: 0.029394907876849174\n",
      "iteration 464, loss: 0.026001788675785065\n",
      "iteration 465, loss: 0.028032131493091583\n",
      "iteration 466, loss: 0.028554528951644897\n",
      "iteration 467, loss: 0.029721643775701523\n",
      "iteration 468, loss: 0.028411414474248886\n",
      "iteration 469, loss: 0.029912395402789116\n",
      "iteration 470, loss: 0.02990042045712471\n",
      "iteration 471, loss: 0.02821926213800907\n",
      "iteration 472, loss: 0.029831858351826668\n",
      "iteration 473, loss: 0.02363782934844494\n",
      "iteration 474, loss: 0.030314791947603226\n",
      "iteration 475, loss: 0.029411058872938156\n",
      "iteration 476, loss: 0.027759036049246788\n",
      "iteration 477, loss: 0.027142349630594254\n",
      "iteration 478, loss: 0.03273492306470871\n",
      "iteration 479, loss: 0.028531063348054886\n",
      "iteration 480, loss: 0.033223818987607956\n",
      "iteration 481, loss: 0.02903132699429989\n",
      "iteration 482, loss: 0.02963928133249283\n",
      "iteration 483, loss: 0.032991524785757065\n",
      "iteration 484, loss: 0.02998516336083412\n",
      "iteration 485, loss: 0.0312531404197216\n",
      "iteration 486, loss: 0.02968670055270195\n",
      "iteration 487, loss: 0.02981024608016014\n",
      "iteration 488, loss: 0.02998300828039646\n",
      "iteration 489, loss: 0.029945407062768936\n",
      "iteration 490, loss: 0.030438758432865143\n",
      "iteration 491, loss: 0.027520185336470604\n",
      "iteration 492, loss: 0.030274247750639915\n",
      "iteration 493, loss: 0.03014765866100788\n",
      "iteration 494, loss: 0.03316125273704529\n",
      "iteration 495, loss: 0.02737364172935486\n",
      "iteration 496, loss: 0.0288902185857296\n",
      "iteration 497, loss: 0.03127574175596237\n",
      "iteration 498, loss: 0.026217948645353317\n",
      "iteration 499, loss: 0.027607006952166557\n",
      "iteration 500, loss: 0.02648502215743065\n",
      "iteration 501, loss: 0.03158651292324066\n",
      "iteration 502, loss: 0.0305144302546978\n",
      "iteration 503, loss: 0.02881641685962677\n",
      "iteration 504, loss: 0.027080535888671875\n",
      "iteration 505, loss: 0.028986770659685135\n",
      "iteration 506, loss: 0.02917809784412384\n",
      "iteration 507, loss: 0.03239704295992851\n",
      "iteration 508, loss: 0.030909433960914612\n",
      "iteration 509, loss: 0.030454158782958984\n",
      "iteration 510, loss: 0.03042665123939514\n",
      "iteration 511, loss: 0.028527773916721344\n",
      "iteration 512, loss: 0.027616921812295914\n",
      "iteration 513, loss: 0.02934880182147026\n",
      "iteration 514, loss: 0.030892264097929\n",
      "iteration 515, loss: 0.03159032762050629\n",
      "iteration 516, loss: 0.02792574092745781\n",
      "iteration 517, loss: 0.029326291754841805\n",
      "iteration 518, loss: 0.02942010760307312\n",
      "iteration 519, loss: 0.03068429045379162\n",
      "iteration 520, loss: 0.029843278229236603\n",
      "iteration 521, loss: 0.028931140899658203\n",
      "iteration 522, loss: 0.02933758869767189\n",
      "iteration 523, loss: 0.029856478795409203\n",
      "iteration 524, loss: 0.029682736843824387\n",
      "iteration 525, loss: 0.027231045067310333\n",
      "iteration 526, loss: 0.028163690119981766\n",
      "iteration 527, loss: 0.03222264349460602\n",
      "iteration 528, loss: 0.02985018491744995\n",
      "iteration 529, loss: 0.028696320950984955\n",
      "iteration 530, loss: 0.028952505439519882\n",
      "iteration 531, loss: 0.02944795973598957\n",
      "iteration 532, loss: 0.029707731679081917\n",
      "iteration 533, loss: 0.03042203187942505\n",
      "iteration 534, loss: 0.029253777116537094\n",
      "iteration 535, loss: 0.029460739344358444\n",
      "iteration 536, loss: 0.030565831810235977\n",
      "iteration 537, loss: 0.029985493049025536\n",
      "iteration 538, loss: 0.02832864411175251\n",
      "iteration 539, loss: 0.029590290039777756\n",
      "iteration 540, loss: 0.029951773583889008\n",
      "iteration 541, loss: 0.030443737283349037\n",
      "iteration 542, loss: 0.029045771807432175\n",
      "iteration 543, loss: 0.028452005237340927\n",
      "iteration 544, loss: 0.02906767465174198\n",
      "iteration 545, loss: 0.03032897785305977\n",
      "iteration 546, loss: 0.027147790417075157\n",
      "iteration 547, loss: 0.027285931631922722\n",
      "iteration 548, loss: 0.027852831408381462\n",
      "iteration 549, loss: 0.027529722079634666\n",
      "iteration 550, loss: 0.024638615548610687\n",
      "iteration 551, loss: 0.02795633301138878\n",
      "iteration 552, loss: 0.028046244755387306\n",
      "iteration 553, loss: 0.026771437376737595\n",
      "iteration 554, loss: 0.026710808277130127\n",
      "iteration 555, loss: 0.025535831227898598\n",
      "iteration 556, loss: 0.026978181675076485\n",
      "iteration 557, loss: 0.028020555153489113\n",
      "iteration 558, loss: 0.029207732528448105\n",
      "iteration 559, loss: 0.029191650450229645\n",
      "iteration 560, loss: 0.026354249566793442\n",
      "iteration 561, loss: 0.030118100345134735\n",
      "iteration 562, loss: 0.027129918336868286\n",
      "iteration 563, loss: 0.029974315315485\n",
      "iteration 564, loss: 0.02591875195503235\n",
      "iteration 565, loss: 0.02852894365787506\n",
      "iteration 566, loss: 0.030274957418441772\n",
      "iteration 567, loss: 0.026074837893247604\n",
      "iteration 568, loss: 0.030907664448022842\n",
      "iteration 569, loss: 0.02772638387978077\n",
      "iteration 570, loss: 0.028533633798360825\n",
      "iteration 571, loss: 0.027366209775209427\n",
      "iteration 572, loss: 0.027191830798983574\n",
      "iteration 573, loss: 0.029000159353017807\n",
      "iteration 574, loss: 0.028648147359490395\n",
      "iteration 575, loss: 0.029742958024144173\n",
      "iteration 576, loss: 0.02873251587152481\n",
      "iteration 577, loss: 0.03125208616256714\n",
      "iteration 578, loss: 0.02838551253080368\n",
      "iteration 579, loss: 0.030465615913271904\n",
      "iteration 580, loss: 0.031115224584937096\n",
      "iteration 581, loss: 0.030402114614844322\n",
      "iteration 582, loss: 0.029402675107121468\n",
      "iteration 583, loss: 0.029043342918157578\n",
      "iteration 584, loss: 0.025888852775096893\n",
      "iteration 585, loss: 0.026893598958849907\n",
      "iteration 586, loss: 0.030372869223356247\n",
      "iteration 587, loss: 0.026013057678937912\n",
      "iteration 588, loss: 0.02617032825946808\n",
      "iteration 589, loss: 0.030882734805345535\n",
      "iteration 590, loss: 0.02619064599275589\n",
      "iteration 591, loss: 0.027003921568393707\n",
      "iteration 592, loss: 0.02845143899321556\n",
      "iteration 593, loss: 0.028672225773334503\n",
      "iteration 594, loss: 0.025937121361494064\n",
      "iteration 595, loss: 0.032113347202539444\n",
      "iteration 596, loss: 0.02776484563946724\n",
      "iteration 597, loss: 0.028819598257541656\n",
      "iteration 598, loss: 0.029138656333088875\n",
      "iteration 599, loss: 0.02710091695189476\n",
      "iteration 600, loss: 0.029073916375637054\n",
      "iteration 601, loss: 0.028815127909183502\n",
      "iteration 602, loss: 0.02793385460972786\n",
      "iteration 603, loss: 0.02790830284357071\n",
      "iteration 604, loss: 0.02828521840274334\n",
      "iteration 605, loss: 0.026083435863256454\n",
      "iteration 606, loss: 0.027705170214176178\n",
      "iteration 607, loss: 0.026132751256227493\n",
      "iteration 608, loss: 0.026239506900310516\n",
      "iteration 609, loss: 0.02498868852853775\n",
      "iteration 610, loss: 0.028914347290992737\n",
      "iteration 611, loss: 0.027016187086701393\n",
      "iteration 612, loss: 0.02538904920220375\n",
      "iteration 613, loss: 0.028086025267839432\n",
      "iteration 614, loss: 0.02968340739607811\n",
      "iteration 615, loss: 0.02666989155113697\n",
      "iteration 616, loss: 0.028819601982831955\n",
      "iteration 617, loss: 0.024396978318691254\n",
      "iteration 618, loss: 0.027368295937776566\n",
      "iteration 619, loss: 0.023033684119582176\n",
      "iteration 620, loss: 0.026482151821255684\n",
      "iteration 621, loss: 0.02888084203004837\n",
      "iteration 622, loss: 0.023194648325443268\n",
      "iteration 623, loss: 0.02693762630224228\n",
      "iteration 624, loss: 0.027603745460510254\n",
      "iteration 625, loss: 0.02594536542892456\n",
      "iteration 626, loss: 0.026669185608625412\n",
      "iteration 627, loss: 0.02755269780755043\n",
      "iteration 628, loss: 0.027292953804135323\n",
      "iteration 629, loss: 0.02783101052045822\n",
      "iteration 630, loss: 0.028505370020866394\n",
      "iteration 631, loss: 0.027405740693211555\n",
      "iteration 632, loss: 0.028322428464889526\n",
      "iteration 633, loss: 0.027052605524659157\n",
      "iteration 634, loss: 0.029478395357728004\n",
      "iteration 635, loss: 0.0280921570956707\n",
      "iteration 636, loss: 0.028462229296565056\n",
      "iteration 637, loss: 0.027150683104991913\n",
      "iteration 638, loss: 0.02673393115401268\n",
      "iteration 639, loss: 0.027107104659080505\n",
      "iteration 640, loss: 0.02852734550833702\n",
      "iteration 641, loss: 0.028353555127978325\n",
      "iteration 642, loss: 0.0249624140560627\n",
      "iteration 643, loss: 0.028665367513895035\n",
      "iteration 644, loss: 0.02720780111849308\n",
      "iteration 645, loss: 0.028237782418727875\n",
      "iteration 646, loss: 0.029228392988443375\n",
      "iteration 647, loss: 0.028895460069179535\n",
      "iteration 648, loss: 0.02804415673017502\n",
      "iteration 649, loss: 0.028394833207130432\n",
      "iteration 650, loss: 0.026791580021381378\n",
      "iteration 651, loss: 0.02612849324941635\n",
      "iteration 652, loss: 0.025709163397550583\n",
      "iteration 653, loss: 0.026000794023275375\n",
      "iteration 654, loss: 0.026953257620334625\n",
      "iteration 655, loss: 0.028068680316209793\n",
      "iteration 656, loss: 0.02906302735209465\n",
      "iteration 657, loss: 0.02744104340672493\n",
      "iteration 658, loss: 0.02659580484032631\n",
      "iteration 659, loss: 0.026499144732952118\n",
      "iteration 660, loss: 0.026677794754505157\n",
      "iteration 661, loss: 0.027988895773887634\n",
      "iteration 662, loss: 0.029745101928710938\n",
      "iteration 663, loss: 0.027042685076594353\n",
      "iteration 664, loss: 0.027900230139493942\n",
      "iteration 665, loss: 0.024480344727635384\n",
      "iteration 666, loss: 0.028836039826273918\n",
      "iteration 667, loss: 0.024631205946207047\n",
      "iteration 668, loss: 0.025583479553461075\n",
      "iteration 669, loss: 0.025994595140218735\n",
      "iteration 670, loss: 0.028146542608737946\n",
      "iteration 671, loss: 0.02534654550254345\n",
      "iteration 672, loss: 0.028890974819660187\n",
      "iteration 673, loss: 0.02809215523302555\n",
      "iteration 674, loss: 0.026812752708792686\n",
      "iteration 675, loss: 0.02701525017619133\n",
      "iteration 676, loss: 0.03045591339468956\n",
      "iteration 677, loss: 0.02601136825978756\n",
      "iteration 678, loss: 0.028524501249194145\n",
      "iteration 679, loss: 0.030687619000673294\n",
      "iteration 680, loss: 0.025319963693618774\n",
      "iteration 681, loss: 0.027474667876958847\n",
      "iteration 682, loss: 0.02436045929789543\n",
      "iteration 683, loss: 0.026108792051672935\n",
      "iteration 684, loss: 0.027080189436674118\n",
      "iteration 685, loss: 0.02450595423579216\n",
      "iteration 686, loss: 0.030883830040693283\n",
      "iteration 687, loss: 0.028188765048980713\n",
      "iteration 688, loss: 0.02786112204194069\n",
      "iteration 689, loss: 0.028285669162869453\n",
      "iteration 690, loss: 0.030797291547060013\n",
      "iteration 691, loss: 0.028866034001111984\n",
      "iteration 692, loss: 0.02721383236348629\n",
      "iteration 693, loss: 0.02792098931968212\n",
      "iteration 694, loss: 0.026739148423075676\n",
      "iteration 695, loss: 0.02613900601863861\n",
      "iteration 696, loss: 0.02865324541926384\n",
      "iteration 697, loss: 0.032488979399204254\n",
      "iteration 698, loss: 0.024600986391305923\n",
      "iteration 699, loss: 0.028089238330721855\n",
      "iteration 700, loss: 0.027327651157975197\n",
      "iteration 701, loss: 0.03029683232307434\n",
      "iteration 702, loss: 0.025944553315639496\n",
      "iteration 703, loss: 0.02486315183341503\n",
      "iteration 704, loss: 0.027825426310300827\n",
      "iteration 705, loss: 0.026974402368068695\n",
      "iteration 706, loss: 0.028726283460855484\n",
      "iteration 707, loss: 0.023707792162895203\n",
      "iteration 708, loss: 0.026964783668518066\n",
      "iteration 709, loss: 0.028400111943483353\n",
      "iteration 710, loss: 0.028815604746341705\n",
      "iteration 711, loss: 0.026107851415872574\n",
      "iteration 712, loss: 0.02637883648276329\n",
      "iteration 713, loss: 0.02917487919330597\n",
      "iteration 714, loss: 0.02455132082104683\n",
      "iteration 715, loss: 0.025491472333669662\n",
      "iteration 716, loss: 0.025761589407920837\n",
      "iteration 717, loss: 0.02947734110057354\n",
      "iteration 718, loss: 0.025689881294965744\n",
      "iteration 719, loss: 0.02589321881532669\n",
      "iteration 720, loss: 0.025356454774737358\n",
      "iteration 721, loss: 0.02887013368308544\n",
      "iteration 722, loss: 0.02415507286787033\n",
      "iteration 723, loss: 0.02683868259191513\n",
      "iteration 724, loss: 0.02812032401561737\n",
      "iteration 725, loss: 0.02719411998987198\n",
      "iteration 726, loss: 0.02752702310681343\n",
      "iteration 727, loss: 0.02765817940235138\n",
      "iteration 728, loss: 0.026311423629522324\n",
      "iteration 729, loss: 0.026845179498195648\n",
      "iteration 730, loss: 0.025470299646258354\n",
      "iteration 731, loss: 0.027607876807451248\n",
      "iteration 732, loss: 0.027265217155218124\n",
      "iteration 733, loss: 0.025142952799797058\n",
      "iteration 734, loss: 0.02791241556406021\n",
      "iteration 735, loss: 0.024805374443531036\n",
      "iteration 736, loss: 0.028670024126768112\n",
      "iteration 737, loss: 0.024388384073972702\n",
      "iteration 738, loss: 0.025636229664087296\n",
      "iteration 739, loss: 0.02484608255326748\n",
      "iteration 740, loss: 0.029633961617946625\n",
      "iteration 741, loss: 0.026970453560352325\n",
      "iteration 742, loss: 0.02708861604332924\n",
      "iteration 743, loss: 0.02455771714448929\n",
      "iteration 744, loss: 0.029256610199809074\n",
      "iteration 745, loss: 0.025708768516778946\n",
      "iteration 746, loss: 0.02767714112997055\n",
      "iteration 747, loss: 0.025646794587373734\n",
      "iteration 748, loss: 0.02569781057536602\n",
      "iteration 749, loss: 0.027244385331869125\n",
      "iteration 750, loss: 0.02661178633570671\n",
      "iteration 751, loss: 0.025135356932878494\n",
      "iteration 752, loss: 0.027246616780757904\n",
      "iteration 753, loss: 0.026510097086429596\n",
      "iteration 754, loss: 0.024867065250873566\n",
      "iteration 755, loss: 0.02494022622704506\n",
      "iteration 756, loss: 0.027144767343997955\n",
      "iteration 757, loss: 0.025267045944929123\n",
      "iteration 758, loss: 0.029060617089271545\n",
      "iteration 759, loss: 0.03128701448440552\n",
      "iteration 760, loss: 0.027086397632956505\n",
      "iteration 761, loss: 0.025143727660179138\n",
      "iteration 762, loss: 0.02824820578098297\n",
      "iteration 763, loss: 0.024868521839380264\n",
      "iteration 764, loss: 0.026457330211997032\n",
      "iteration 765, loss: 0.028925640508532524\n",
      "iteration 766, loss: 0.02492203563451767\n",
      "iteration 767, loss: 0.02439061738550663\n",
      "iteration 768, loss: 0.02775905467569828\n",
      "iteration 769, loss: 0.027608424425125122\n",
      "iteration 770, loss: 0.025046242401003838\n",
      "iteration 771, loss: 0.027109215036034584\n",
      "iteration 772, loss: 0.026982109993696213\n",
      "iteration 773, loss: 0.025833088904619217\n",
      "iteration 774, loss: 0.022897059097886086\n",
      "iteration 775, loss: 0.02618190459907055\n",
      "iteration 776, loss: 0.023301832377910614\n",
      "iteration 777, loss: 0.02691328153014183\n",
      "iteration 778, loss: 0.025122126564383507\n",
      "iteration 779, loss: 0.026332765817642212\n",
      "iteration 780, loss: 0.026497187092900276\n",
      "iteration 781, loss: 0.027523381635546684\n",
      "iteration 782, loss: 0.027640338987112045\n",
      "iteration 783, loss: 0.02611851505935192\n",
      "iteration 784, loss: 0.025145575404167175\n",
      "iteration 785, loss: 0.027682526037096977\n",
      "iteration 786, loss: 0.027181580662727356\n",
      "iteration 787, loss: 0.02262958139181137\n",
      "iteration 788, loss: 0.027603495866060257\n",
      "iteration 789, loss: 0.026965197175741196\n",
      "iteration 790, loss: 0.028452515602111816\n",
      "iteration 791, loss: 0.026085516437888145\n",
      "iteration 792, loss: 0.02728937193751335\n",
      "iteration 793, loss: 0.02808172069489956\n",
      "iteration 794, loss: 0.029075365513563156\n",
      "iteration 795, loss: 0.02715730294585228\n",
      "iteration 796, loss: 0.025187334045767784\n",
      "iteration 797, loss: 0.026110423728823662\n",
      "iteration 798, loss: 0.029669031500816345\n",
      "iteration 799, loss: 0.028943192213773727\n",
      "iteration 800, loss: 0.027089249342679977\n",
      "iteration 801, loss: 0.02650977298617363\n",
      "iteration 802, loss: 0.0254830289632082\n",
      "iteration 803, loss: 0.02862854115664959\n",
      "iteration 804, loss: 0.025611426681280136\n",
      "iteration 805, loss: 0.024944251403212547\n",
      "iteration 806, loss: 0.022475067526102066\n",
      "iteration 807, loss: 0.030363943427801132\n",
      "iteration 808, loss: 0.02671969309449196\n",
      "iteration 809, loss: 0.02615353837609291\n",
      "iteration 810, loss: 0.027865547686815262\n",
      "iteration 811, loss: 0.025195065885782242\n",
      "iteration 812, loss: 0.024882066994905472\n",
      "iteration 813, loss: 0.025840502232313156\n",
      "iteration 814, loss: 0.027383742853999138\n",
      "iteration 815, loss: 0.028129326179623604\n",
      "iteration 816, loss: 0.02615656703710556\n",
      "iteration 817, loss: 0.02757434733211994\n",
      "iteration 818, loss: 0.02721373736858368\n",
      "iteration 819, loss: 0.025490790605545044\n",
      "iteration 820, loss: 0.02793683297932148\n",
      "iteration 821, loss: 0.023984510451555252\n",
      "iteration 822, loss: 0.022906651720404625\n",
      "iteration 823, loss: 0.026670411229133606\n",
      "iteration 824, loss: 0.023568926379084587\n",
      "iteration 825, loss: 0.02574574202299118\n",
      "iteration 826, loss: 0.02825562097132206\n",
      "iteration 827, loss: 0.027409717440605164\n",
      "iteration 828, loss: 0.02909853309392929\n",
      "iteration 829, loss: 0.028746139258146286\n",
      "iteration 830, loss: 0.022931942716240883\n",
      "iteration 831, loss: 0.02646752819418907\n",
      "iteration 832, loss: 0.02808968909084797\n",
      "iteration 833, loss: 0.02166883647441864\n",
      "iteration 834, loss: 0.0272701233625412\n",
      "iteration 835, loss: 0.024889182299375534\n",
      "iteration 836, loss: 0.026420235633850098\n",
      "iteration 837, loss: 0.029092568904161453\n",
      "iteration 838, loss: 0.024986041709780693\n",
      "iteration 839, loss: 0.02734331786632538\n",
      "iteration 840, loss: 0.026414696127176285\n",
      "iteration 841, loss: 0.025135550647974014\n",
      "iteration 842, loss: 0.026346299797296524\n",
      "iteration 843, loss: 0.0269492045044899\n",
      "iteration 844, loss: 0.023160915821790695\n",
      "iteration 845, loss: 0.02238534949719906\n",
      "iteration 846, loss: 0.025341495871543884\n",
      "iteration 847, loss: 0.02191247045993805\n",
      "iteration 848, loss: 0.027852024883031845\n",
      "iteration 849, loss: 0.027674399316310883\n",
      "iteration 850, loss: 0.02820124849677086\n",
      "iteration 851, loss: 0.020812109112739563\n",
      "iteration 852, loss: 0.023279912769794464\n",
      "iteration 853, loss: 0.02354281395673752\n",
      "iteration 854, loss: 0.02907736599445343\n",
      "iteration 855, loss: 0.02972729690372944\n",
      "iteration 856, loss: 0.028774918988347054\n",
      "iteration 857, loss: 0.02618471160531044\n",
      "iteration 858, loss: 0.023615650832653046\n",
      "iteration 859, loss: 0.023101868107914925\n",
      "iteration 860, loss: 0.025333603844046593\n",
      "iteration 861, loss: 0.024906225502490997\n",
      "iteration 862, loss: 0.02718367613852024\n",
      "iteration 863, loss: 0.025405433028936386\n",
      "iteration 864, loss: 0.027418676763772964\n",
      "iteration 865, loss: 0.02647623047232628\n",
      "iteration 866, loss: 0.023693803697824478\n",
      "iteration 867, loss: 0.025466252118349075\n",
      "iteration 868, loss: 0.025211753323674202\n",
      "iteration 869, loss: 0.02468695119023323\n",
      "iteration 870, loss: 0.02731253206729889\n",
      "iteration 871, loss: 0.0249517522752285\n",
      "iteration 872, loss: 0.02618248760700226\n",
      "iteration 873, loss: 0.023937739431858063\n",
      "iteration 874, loss: 0.02453972026705742\n",
      "iteration 875, loss: 0.026777807623147964\n",
      "iteration 876, loss: 0.027802065014839172\n",
      "iteration 877, loss: 0.024723075330257416\n",
      "iteration 878, loss: 0.02663579024374485\n",
      "iteration 879, loss: 0.024934343993663788\n",
      "iteration 880, loss: 0.02796037867665291\n",
      "iteration 881, loss: 0.029844164848327637\n",
      "iteration 882, loss: 0.026036042720079422\n",
      "iteration 883, loss: 0.025585491210222244\n",
      "iteration 884, loss: 0.02526715397834778\n",
      "iteration 885, loss: 0.030245114117860794\n",
      "iteration 886, loss: 0.026107951998710632\n",
      "iteration 887, loss: 0.028246507048606873\n",
      "iteration 888, loss: 0.026495592668652534\n",
      "iteration 889, loss: 0.02753414213657379\n",
      "iteration 890, loss: 0.028932999819517136\n",
      "iteration 891, loss: 0.02805314213037491\n",
      "iteration 892, loss: 0.029173266142606735\n",
      "iteration 893, loss: 0.026034537702798843\n",
      "iteration 894, loss: 0.025335606187582016\n",
      "iteration 895, loss: 0.028104513883590698\n",
      "iteration 896, loss: 0.023766962811350822\n",
      "iteration 897, loss: 0.025755200535058975\n",
      "iteration 898, loss: 0.026227619498968124\n",
      "iteration 899, loss: 0.027768349274992943\n",
      "iteration 900, loss: 0.025357022881507874\n",
      "iteration 901, loss: 0.02860957384109497\n",
      "iteration 902, loss: 0.026207784190773964\n",
      "iteration 903, loss: 0.02747713029384613\n",
      "iteration 904, loss: 0.02667398378252983\n",
      "iteration 905, loss: 0.025887465104460716\n",
      "iteration 906, loss: 0.023944461718201637\n",
      "iteration 907, loss: 0.025003086775541306\n",
      "iteration 908, loss: 0.02970151975750923\n",
      "iteration 909, loss: 0.02599707990884781\n",
      "iteration 910, loss: 0.029408587142825127\n",
      "iteration 911, loss: 0.02309897169470787\n",
      "iteration 912, loss: 0.026184003800153732\n",
      "iteration 913, loss: 0.02635624259710312\n",
      "iteration 914, loss: 0.02714388445019722\n",
      "iteration 915, loss: 0.025923646986484528\n",
      "iteration 916, loss: 0.025727201253175735\n",
      "iteration 917, loss: 0.024902936071157455\n",
      "iteration 918, loss: 0.029192805290222168\n",
      "iteration 919, loss: 0.02842911332845688\n",
      "iteration 920, loss: 0.0259939506649971\n",
      "iteration 921, loss: 0.026889003813266754\n",
      "iteration 922, loss: 0.026883233338594437\n",
      "iteration 923, loss: 0.025036964565515518\n",
      "iteration 924, loss: 0.02645745873451233\n",
      "iteration 925, loss: 0.027229705825448036\n",
      "iteration 926, loss: 0.023741066455841064\n",
      "iteration 927, loss: 0.024091098457574844\n",
      "iteration 928, loss: 0.028644053265452385\n",
      "iteration 929, loss: 0.027788400650024414\n",
      "iteration 930, loss: 0.02671472728252411\n",
      "iteration 931, loss: 0.026046397164463997\n",
      "iteration 932, loss: 0.026551594957709312\n",
      "iteration 933, loss: 0.028393864631652832\n",
      "iteration 934, loss: 0.02720596455037594\n",
      "iteration 935, loss: 0.023637667298316956\n",
      "iteration 936, loss: 0.02559812366962433\n",
      "iteration 937, loss: 0.0262895617634058\n",
      "iteration 938, loss: 0.023568471893668175\n",
      "iteration 939, loss: 0.026503603905439377\n",
      "iteration 940, loss: 0.023289255797863007\n",
      "iteration 941, loss: 0.02483608014881611\n",
      "iteration 942, loss: 0.02367692068219185\n",
      "iteration 943, loss: 0.02925339713692665\n",
      "iteration 944, loss: 0.025684397667646408\n",
      "iteration 945, loss: 0.028805360198020935\n",
      "iteration 946, loss: 0.024513844400644302\n",
      "iteration 947, loss: 0.02691059373319149\n",
      "iteration 948, loss: 0.02695155143737793\n",
      "iteration 949, loss: 0.025859301909804344\n",
      "iteration 950, loss: 0.0239730104804039\n",
      "iteration 951, loss: 0.022236015647649765\n",
      "iteration 952, loss: 0.023371774703264236\n",
      "iteration 953, loss: 0.02666146494448185\n",
      "iteration 954, loss: 0.0277725700289011\n",
      "iteration 955, loss: 0.024603597819805145\n",
      "iteration 956, loss: 0.02442418597638607\n",
      "iteration 957, loss: 0.026044050231575966\n",
      "iteration 958, loss: 0.022947998717427254\n",
      "iteration 959, loss: 0.02764638513326645\n",
      "iteration 960, loss: 0.022599751129746437\n",
      "iteration 961, loss: 0.027741653844714165\n",
      "iteration 962, loss: 0.028098715469241142\n",
      "iteration 963, loss: 0.025646749883890152\n",
      "iteration 964, loss: 0.025533553212881088\n",
      "iteration 965, loss: 0.025910761207342148\n",
      "iteration 966, loss: 0.02691432647407055\n",
      "iteration 967, loss: 0.02465106174349785\n",
      "iteration 968, loss: 0.024618225172162056\n",
      "iteration 969, loss: 0.02217886783182621\n",
      "iteration 970, loss: 0.025539755821228027\n",
      "iteration 971, loss: 0.02863093838095665\n",
      "iteration 972, loss: 0.026954278349876404\n",
      "iteration 973, loss: 0.02709369547665119\n",
      "iteration 974, loss: 0.022907540202140808\n",
      "iteration 975, loss: 0.026451438665390015\n",
      "iteration 976, loss: 0.02820427156984806\n",
      "iteration 977, loss: 0.02645663358271122\n",
      "iteration 978, loss: 0.022723421454429626\n",
      "iteration 979, loss: 0.021922532469034195\n",
      "iteration 980, loss: 0.02446366287767887\n",
      "iteration 981, loss: 0.026221811771392822\n",
      "iteration 982, loss: 0.024448204785585403\n",
      "iteration 983, loss: 0.02564518339931965\n",
      "iteration 984, loss: 0.024297602474689484\n",
      "iteration 985, loss: 0.02643793821334839\n",
      "iteration 986, loss: 0.025216858834028244\n",
      "iteration 987, loss: 0.024632124230265617\n",
      "iteration 988, loss: 0.024824537336826324\n",
      "iteration 989, loss: 0.025756724178791046\n",
      "iteration 990, loss: 0.026774859055876732\n",
      "iteration 991, loss: 0.026603201404213905\n",
      "iteration 992, loss: 0.02448684349656105\n",
      "iteration 993, loss: 0.026610322296619415\n",
      "iteration 994, loss: 0.02576664835214615\n",
      "iteration 995, loss: 0.026756010949611664\n",
      "iteration 996, loss: 0.023298131301999092\n",
      "iteration 997, loss: 0.027880286797881126\n",
      "iteration 998, loss: 0.02605244144797325\n",
      "iteration 999, loss: 0.0248753409832716\n",
      "iteration 1000, loss: 0.023983526974916458\n",
      "iteration 1001, loss: 0.026804793626070023\n",
      "iteration 1002, loss: 0.025215277448296547\n",
      "iteration 1003, loss: 0.02485734038054943\n",
      "iteration 1004, loss: 0.022161973640322685\n",
      "iteration 1005, loss: 0.023928366601467133\n",
      "iteration 1006, loss: 0.026425277814269066\n",
      "iteration 1007, loss: 0.023140104487538338\n",
      "iteration 1008, loss: 0.024109091609716415\n",
      "iteration 1009, loss: 0.0253774281591177\n",
      "iteration 1010, loss: 0.027147453278303146\n",
      "iteration 1011, loss: 0.02331007644534111\n",
      "iteration 1012, loss: 0.02648867294192314\n",
      "iteration 1013, loss: 0.023566514253616333\n",
      "iteration 1014, loss: 0.02659904584288597\n",
      "iteration 1015, loss: 0.023580007255077362\n",
      "iteration 1016, loss: 0.026692863553762436\n",
      "iteration 1017, loss: 0.02523595467209816\n",
      "iteration 1018, loss: 0.023976635187864304\n",
      "iteration 1019, loss: 0.025373902171850204\n",
      "iteration 1020, loss: 0.022603407502174377\n",
      "iteration 1021, loss: 0.027890468016266823\n",
      "iteration 1022, loss: 0.025245968252420425\n",
      "iteration 1023, loss: 0.025691868737339973\n",
      "iteration 1024, loss: 0.02350211888551712\n",
      "iteration 1025, loss: 0.02682669088244438\n",
      "iteration 1026, loss: 0.026333432644605637\n",
      "iteration 1027, loss: 0.023543329909443855\n",
      "iteration 1028, loss: 0.026221217587590218\n",
      "iteration 1029, loss: 0.02264244109392166\n",
      "iteration 1030, loss: 0.022299272939562798\n",
      "iteration 1031, loss: 0.024634530767798424\n",
      "iteration 1032, loss: 0.024563509970903397\n",
      "iteration 1033, loss: 0.0251901987940073\n",
      "iteration 1034, loss: 0.027706727385520935\n",
      "iteration 1035, loss: 0.025659166276454926\n",
      "iteration 1036, loss: 0.024935804307460785\n",
      "iteration 1037, loss: 0.023753013461828232\n",
      "iteration 1038, loss: 0.02519001066684723\n",
      "iteration 1039, loss: 0.022608041763305664\n",
      "iteration 1040, loss: 0.025552209466695786\n",
      "iteration 1041, loss: 0.022932525724172592\n",
      "iteration 1042, loss: 0.02473948523402214\n",
      "iteration 1043, loss: 0.022702954709529877\n",
      "iteration 1044, loss: 0.024079885333776474\n",
      "iteration 1045, loss: 0.026217017322778702\n",
      "iteration 1046, loss: 0.02465592697262764\n",
      "iteration 1047, loss: 0.027577554807066917\n",
      "iteration 1048, loss: 0.0227125845849514\n",
      "iteration 1049, loss: 0.027922872453927994\n",
      "iteration 1050, loss: 0.02511002868413925\n",
      "iteration 1051, loss: 0.025280028581619263\n",
      "iteration 1052, loss: 0.02511901594698429\n",
      "iteration 1053, loss: 0.025806764140725136\n",
      "iteration 1054, loss: 0.02618730627000332\n",
      "iteration 1055, loss: 0.025969192385673523\n",
      "iteration 1056, loss: 0.02640303038060665\n",
      "iteration 1057, loss: 0.025957509875297546\n",
      "iteration 1058, loss: 0.02270963415503502\n",
      "iteration 1059, loss: 0.02849126234650612\n",
      "iteration 1060, loss: 0.023031100630760193\n",
      "iteration 1061, loss: 0.02826538495719433\n",
      "iteration 1062, loss: 0.023435533046722412\n",
      "iteration 1063, loss: 0.025988902896642685\n",
      "iteration 1064, loss: 0.025468170642852783\n",
      "iteration 1065, loss: 0.02346625365316868\n",
      "iteration 1066, loss: 0.026012297719717026\n",
      "iteration 1067, loss: 0.0248602032661438\n",
      "iteration 1068, loss: 0.026568196713924408\n",
      "iteration 1069, loss: 0.029469391331076622\n",
      "iteration 1070, loss: 0.02799067460000515\n",
      "iteration 1071, loss: 0.024992845952510834\n",
      "iteration 1072, loss: 0.02456062287092209\n",
      "iteration 1073, loss: 0.027637798339128494\n",
      "iteration 1074, loss: 0.026617828756570816\n",
      "iteration 1075, loss: 0.026737354695796967\n",
      "iteration 1076, loss: 0.02501264587044716\n",
      "iteration 1077, loss: 0.025757677853107452\n",
      "iteration 1078, loss: 0.025915969163179398\n",
      "iteration 1079, loss: 0.026792706921696663\n",
      "iteration 1080, loss: 0.028541654348373413\n",
      "iteration 1081, loss: 0.025002768263220787\n",
      "iteration 1082, loss: 0.028246205300092697\n",
      "iteration 1083, loss: 0.024760305881500244\n",
      "iteration 1084, loss: 0.026369847357273102\n",
      "iteration 1085, loss: 0.027642089873552322\n",
      "iteration 1086, loss: 0.024273764342069626\n",
      "iteration 1087, loss: 0.024332448840141296\n",
      "iteration 1088, loss: 0.02416389435529709\n",
      "iteration 1089, loss: 0.026523340493440628\n",
      "iteration 1090, loss: 0.026059690862894058\n",
      "iteration 1091, loss: 0.023674234747886658\n",
      "iteration 1092, loss: 0.02754155546426773\n",
      "iteration 1093, loss: 0.023798849433660507\n",
      "iteration 1094, loss: 0.02564825490117073\n",
      "iteration 1095, loss: 0.026365213096141815\n",
      "iteration 1096, loss: 0.02512790821492672\n",
      "iteration 1097, loss: 0.024465683847665787\n",
      "iteration 1098, loss: 0.028535466641187668\n",
      "iteration 1099, loss: 0.025487080216407776\n",
      "iteration 1100, loss: 0.025153178721666336\n",
      "iteration 1101, loss: 0.025331934913992882\n",
      "iteration 1102, loss: 0.027042806148529053\n",
      "iteration 1103, loss: 0.025756534188985825\n",
      "iteration 1104, loss: 0.023209435865283012\n",
      "iteration 1105, loss: 0.02684665098786354\n",
      "iteration 1106, loss: 0.026473376899957657\n",
      "iteration 1107, loss: 0.025151263922452927\n",
      "iteration 1108, loss: 0.02654752880334854\n",
      "iteration 1109, loss: 0.023468784987926483\n",
      "iteration 1110, loss: 0.026239218190312386\n",
      "iteration 1111, loss: 0.024409335106611252\n",
      "iteration 1112, loss: 0.02843133732676506\n",
      "iteration 1113, loss: 0.025644659996032715\n",
      "iteration 1114, loss: 0.026571005582809448\n",
      "iteration 1115, loss: 0.027628019452095032\n",
      "iteration 1116, loss: 0.024430256336927414\n",
      "iteration 1117, loss: 0.022571854293346405\n",
      "iteration 1118, loss: 0.023697461932897568\n",
      "iteration 1119, loss: 0.026582174003124237\n",
      "iteration 1120, loss: 0.023856880143284798\n",
      "iteration 1121, loss: 0.024221334606409073\n",
      "iteration 1122, loss: 0.023993225768208504\n",
      "iteration 1123, loss: 0.026429813355207443\n",
      "iteration 1124, loss: 0.027856850996613503\n",
      "iteration 1125, loss: 0.029465019702911377\n",
      "iteration 1126, loss: 0.024911455810070038\n",
      "iteration 1127, loss: 0.025063641369342804\n",
      "iteration 1128, loss: 0.023881347849965096\n",
      "iteration 1129, loss: 0.026024511083960533\n",
      "iteration 1130, loss: 0.023334328085184097\n",
      "iteration 1131, loss: 0.028087761253118515\n",
      "iteration 1132, loss: 0.024720480665564537\n",
      "iteration 1133, loss: 0.025391193106770515\n",
      "iteration 1134, loss: 0.02278602309525013\n",
      "iteration 1135, loss: 0.022605083882808685\n",
      "iteration 1136, loss: 0.023982923477888107\n",
      "iteration 1137, loss: 0.026598602533340454\n",
      "iteration 1138, loss: 0.023482440039515495\n",
      "iteration 1139, loss: 0.02531387098133564\n",
      "iteration 1140, loss: 0.02531181089580059\n",
      "iteration 1141, loss: 0.02584501914680004\n",
      "iteration 1142, loss: 0.026108726859092712\n",
      "iteration 1143, loss: 0.02656860463321209\n",
      "iteration 1144, loss: 0.022702191025018692\n",
      "iteration 1145, loss: 0.02694973535835743\n",
      "iteration 1146, loss: 0.024067893624305725\n",
      "iteration 1147, loss: 0.021672561764717102\n",
      "iteration 1148, loss: 0.026696018874645233\n",
      "iteration 1149, loss: 0.024692272767424583\n",
      "iteration 1150, loss: 0.024615397676825523\n",
      "iteration 1151, loss: 0.023496992886066437\n",
      "iteration 1152, loss: 0.02648710086941719\n",
      "iteration 1153, loss: 0.025516215711832047\n",
      "iteration 1154, loss: 0.02240053191781044\n",
      "iteration 1155, loss: 0.02658005803823471\n",
      "iteration 1156, loss: 0.024092629551887512\n",
      "iteration 1157, loss: 0.022976882755756378\n",
      "iteration 1158, loss: 0.024489225819706917\n",
      "iteration 1159, loss: 0.026168353855609894\n",
      "iteration 1160, loss: 0.026198698207736015\n",
      "iteration 1161, loss: 0.024689186364412308\n",
      "iteration 1162, loss: 0.0238315649330616\n",
      "iteration 1163, loss: 0.026068832725286484\n",
      "iteration 1164, loss: 0.024932701140642166\n",
      "iteration 1165, loss: 0.024594012647867203\n",
      "iteration 1166, loss: 0.02481655962765217\n",
      "iteration 1167, loss: 0.028120551258325577\n",
      "iteration 1168, loss: 0.02455027401447296\n",
      "iteration 1169, loss: 0.023604175075888634\n",
      "iteration 1170, loss: 0.026209890842437744\n",
      "iteration 1171, loss: 0.025511063635349274\n",
      "iteration 1172, loss: 0.023854415863752365\n",
      "iteration 1173, loss: 0.023768845945596695\n",
      "iteration 1174, loss: 0.02440028078854084\n",
      "iteration 1175, loss: 0.02716842107474804\n",
      "iteration 1176, loss: 0.027718447148799896\n",
      "iteration 1177, loss: 0.02711177058517933\n",
      "iteration 1178, loss: 0.023235410451889038\n",
      "iteration 1179, loss: 0.025004297494888306\n",
      "iteration 1180, loss: 0.025907952338457108\n",
      "iteration 1181, loss: 0.02642665058374405\n",
      "iteration 1182, loss: 0.023478465154767036\n",
      "iteration 1183, loss: 0.02397986873984337\n",
      "iteration 1184, loss: 0.025522366166114807\n",
      "iteration 1185, loss: 0.02406143955886364\n",
      "iteration 1186, loss: 0.021622035652399063\n",
      "iteration 1187, loss: 0.027093613520264626\n",
      "iteration 1188, loss: 0.024365205317735672\n",
      "iteration 1189, loss: 0.025789204984903336\n",
      "iteration 1190, loss: 0.02349514327943325\n",
      "iteration 1191, loss: 0.02442649006843567\n",
      "iteration 1192, loss: 0.022729430347681046\n",
      "iteration 1193, loss: 0.023864764720201492\n",
      "iteration 1194, loss: 0.02230975031852722\n",
      "iteration 1195, loss: 0.027784811332821846\n",
      "iteration 1196, loss: 0.023312151432037354\n",
      "iteration 1197, loss: 0.025630498304963112\n",
      "iteration 1198, loss: 0.026393432170152664\n",
      "iteration 1199, loss: 0.025173194706439972\n",
      "iteration 1200, loss: 0.02694196254014969\n",
      "iteration 1201, loss: 0.023797176778316498\n",
      "iteration 1202, loss: 0.027821891009807587\n",
      "iteration 1203, loss: 0.025083795189857483\n",
      "iteration 1204, loss: 0.024047110229730606\n",
      "iteration 1205, loss: 0.02495664730668068\n",
      "iteration 1206, loss: 0.02588675171136856\n",
      "iteration 1207, loss: 0.02751421369612217\n",
      "iteration 1208, loss: 0.023738091811537743\n",
      "iteration 1209, loss: 0.025022367015480995\n",
      "iteration 1210, loss: 0.02618733048439026\n",
      "iteration 1211, loss: 0.02454562857747078\n",
      "iteration 1212, loss: 0.023547500371932983\n",
      "iteration 1213, loss: 0.02526833489537239\n",
      "iteration 1214, loss: 0.025720488280057907\n",
      "iteration 1215, loss: 0.025887684896588326\n",
      "iteration 1216, loss: 0.024831604212522507\n",
      "iteration 1217, loss: 0.02356201782822609\n",
      "iteration 1218, loss: 0.025664687156677246\n",
      "iteration 1219, loss: 0.026282811537384987\n",
      "iteration 1220, loss: 0.024446114897727966\n",
      "iteration 1221, loss: 0.02308056503534317\n",
      "iteration 1222, loss: 0.023278340697288513\n",
      "iteration 1223, loss: 0.026800476014614105\n",
      "iteration 1224, loss: 0.023935340344905853\n",
      "iteration 1225, loss: 0.02418750524520874\n",
      "iteration 1226, loss: 0.02894347533583641\n",
      "iteration 1227, loss: 0.029279675334692\n",
      "iteration 1228, loss: 0.02277938649058342\n",
      "iteration 1229, loss: 0.025148732587695122\n",
      "iteration 1230, loss: 0.02388915978372097\n",
      "iteration 1231, loss: 0.024523239582777023\n",
      "iteration 1232, loss: 0.025288373231887817\n",
      "iteration 1233, loss: 0.024606339633464813\n",
      "iteration 1234, loss: 0.026795629411935806\n",
      "iteration 1235, loss: 0.02303859032690525\n",
      "iteration 1236, loss: 0.025280501693487167\n",
      "iteration 1237, loss: 0.023176275193691254\n",
      "iteration 1238, loss: 0.022633183747529984\n",
      "iteration 1239, loss: 0.025854971259832382\n",
      "iteration 1240, loss: 0.022591911256313324\n",
      "iteration 1241, loss: 0.025291431695222855\n",
      "iteration 1242, loss: 0.025788435712456703\n",
      "iteration 1243, loss: 0.025096088647842407\n",
      "iteration 1244, loss: 0.021678948774933815\n",
      "iteration 1245, loss: 0.023697523400187492\n",
      "iteration 1246, loss: 0.02322634682059288\n",
      "iteration 1247, loss: 0.025210248306393623\n",
      "iteration 1248, loss: 0.025329886004328728\n",
      "iteration 1249, loss: 0.024088742211461067\n",
      "iteration 1250, loss: 0.025312092155218124\n",
      "iteration 1251, loss: 0.02346339076757431\n",
      "iteration 1252, loss: 0.025769993662834167\n",
      "iteration 1253, loss: 0.022444546222686768\n",
      "iteration 1254, loss: 0.02595081552863121\n",
      "iteration 1255, loss: 0.0209500752389431\n",
      "iteration 1256, loss: 0.023442775011062622\n",
      "iteration 1257, loss: 0.02374754659831524\n",
      "iteration 1258, loss: 0.022233055904507637\n",
      "iteration 1259, loss: 0.02507757768034935\n",
      "iteration 1260, loss: 0.024263203144073486\n",
      "iteration 1261, loss: 0.02525341883301735\n",
      "iteration 1262, loss: 0.023404907435178757\n",
      "iteration 1263, loss: 0.023253392428159714\n",
      "iteration 1264, loss: 0.025537265464663506\n",
      "iteration 1265, loss: 0.025546692311763763\n",
      "iteration 1266, loss: 0.023962965235114098\n",
      "iteration 1267, loss: 0.02279004268348217\n",
      "iteration 1268, loss: 0.025033194571733475\n",
      "iteration 1269, loss: 0.027056463062763214\n",
      "iteration 1270, loss: 0.023517772555351257\n",
      "iteration 1271, loss: 0.024687601253390312\n",
      "iteration 1272, loss: 0.024067066609859467\n",
      "iteration 1273, loss: 0.02383037842810154\n",
      "iteration 1274, loss: 0.024040473625063896\n",
      "iteration 1275, loss: 0.021070275455713272\n",
      "iteration 1276, loss: 0.024333372712135315\n",
      "iteration 1277, loss: 0.023843850940465927\n",
      "iteration 1278, loss: 0.023485727608203888\n",
      "iteration 1279, loss: 0.02445325255393982\n",
      "iteration 1280, loss: 0.024386102333664894\n",
      "iteration 1281, loss: 0.022914104163646698\n",
      "iteration 1282, loss: 0.026208117604255676\n",
      "iteration 1283, loss: 0.022258181124925613\n",
      "iteration 1284, loss: 0.024920552968978882\n",
      "iteration 1285, loss: 0.024868130683898926\n",
      "iteration 1286, loss: 0.024060172960162163\n",
      "iteration 1287, loss: 0.023475835099816322\n",
      "iteration 1288, loss: 0.023808034136891365\n",
      "iteration 1289, loss: 0.024470478296279907\n",
      "iteration 1290, loss: 0.02783646062016487\n",
      "iteration 1291, loss: 0.028220124542713165\n",
      "iteration 1292, loss: 0.024373486638069153\n",
      "iteration 1293, loss: 0.02654891088604927\n",
      "iteration 1294, loss: 0.026168957352638245\n",
      "iteration 1295, loss: 0.025289466604590416\n",
      "iteration 1296, loss: 0.024448813870549202\n",
      "iteration 1297, loss: 0.023579003289341927\n",
      "iteration 1298, loss: 0.025942020118236542\n",
      "iteration 1299, loss: 0.025083506479859352\n",
      "iteration 1300, loss: 0.025219902396202087\n",
      "iteration 1301, loss: 0.022951507940888405\n",
      "iteration 1302, loss: 0.022587187588214874\n",
      "iteration 1303, loss: 0.025290463119745255\n",
      "iteration 1304, loss: 0.02468029409646988\n",
      "iteration 1305, loss: 0.025596927851438522\n",
      "iteration 1306, loss: 0.027518626302480698\n",
      "iteration 1307, loss: 0.023439135402441025\n",
      "iteration 1308, loss: 0.022604504600167274\n",
      "iteration 1309, loss: 0.024448417127132416\n",
      "iteration 1310, loss: 0.024528412148356438\n",
      "iteration 1311, loss: 0.023354290053248405\n",
      "iteration 1312, loss: 0.02314291149377823\n",
      "iteration 1313, loss: 0.02352297492325306\n",
      "iteration 1314, loss: 0.02260098233819008\n",
      "iteration 1315, loss: 0.025721721351146698\n",
      "iteration 1316, loss: 0.02405659109354019\n",
      "iteration 1317, loss: 0.024464312940835953\n",
      "iteration 1318, loss: 0.02219226583838463\n",
      "iteration 1319, loss: 0.02503783628344536\n",
      "iteration 1320, loss: 0.022439241409301758\n",
      "iteration 1321, loss: 0.024981122463941574\n",
      "iteration 1322, loss: 0.023564735427498817\n",
      "iteration 1323, loss: 0.023593444377183914\n",
      "iteration 1324, loss: 0.022975433617830276\n",
      "iteration 1325, loss: 0.02440681681036949\n",
      "iteration 1326, loss: 0.024994846433401108\n",
      "iteration 1327, loss: 0.022098358720541\n",
      "iteration 1328, loss: 0.02471175789833069\n",
      "iteration 1329, loss: 0.022396935150027275\n",
      "iteration 1330, loss: 0.024193869903683662\n",
      "iteration 1331, loss: 0.02240931987762451\n",
      "iteration 1332, loss: 0.020007163286209106\n",
      "iteration 1333, loss: 0.023100165650248528\n",
      "iteration 1334, loss: 0.0217380803078413\n",
      "iteration 1335, loss: 0.024171050637960434\n",
      "iteration 1336, loss: 0.022523527964949608\n",
      "iteration 1337, loss: 0.02445177361369133\n",
      "iteration 1338, loss: 0.023250307887792587\n",
      "iteration 1339, loss: 0.022126637399196625\n",
      "iteration 1340, loss: 0.024366380646824837\n",
      "iteration 1341, loss: 0.021556369960308075\n",
      "iteration 1342, loss: 0.023048384115099907\n",
      "iteration 1343, loss: 0.023528026416897774\n",
      "iteration 1344, loss: 0.02590847760438919\n",
      "iteration 1345, loss: 0.02421717904508114\n",
      "iteration 1346, loss: 0.02150024101138115\n",
      "iteration 1347, loss: 0.023756999522447586\n",
      "iteration 1348, loss: 0.027039427310228348\n",
      "iteration 1349, loss: 0.023469921201467514\n",
      "iteration 1350, loss: 0.021649491041898727\n",
      "iteration 1351, loss: 0.02528281882405281\n",
      "iteration 1352, loss: 0.020867299288511276\n",
      "iteration 1353, loss: 0.022651929408311844\n",
      "iteration 1354, loss: 0.025265298783779144\n",
      "iteration 1355, loss: 0.02320723980665207\n",
      "iteration 1356, loss: 0.026065275073051453\n",
      "iteration 1357, loss: 0.02409246936440468\n",
      "iteration 1358, loss: 0.02122136950492859\n",
      "iteration 1359, loss: 0.022535990923643112\n",
      "iteration 1360, loss: 0.023176632821559906\n",
      "iteration 1361, loss: 0.024607278406620026\n",
      "iteration 1362, loss: 0.02329094149172306\n",
      "iteration 1363, loss: 0.02425258234143257\n",
      "iteration 1364, loss: 0.02309982106089592\n",
      "iteration 1365, loss: 0.02388828434050083\n",
      "iteration 1366, loss: 0.021990539506077766\n",
      "iteration 1367, loss: 0.022316783666610718\n",
      "iteration 1368, loss: 0.022710885852575302\n",
      "iteration 1369, loss: 0.023110711947083473\n",
      "iteration 1370, loss: 0.024006089195609093\n",
      "iteration 1371, loss: 0.022032752633094788\n",
      "iteration 1372, loss: 0.023031283169984818\n",
      "iteration 1373, loss: 0.02230123057961464\n",
      "iteration 1374, loss: 0.025193793699145317\n",
      "iteration 1375, loss: 0.02379920706152916\n",
      "iteration 1376, loss: 0.02313981205224991\n",
      "iteration 1377, loss: 0.023211168125271797\n",
      "iteration 1378, loss: 0.022284023463726044\n",
      "iteration 1379, loss: 0.02136980928480625\n",
      "iteration 1380, loss: 0.02302289754152298\n",
      "iteration 1381, loss: 0.02313346043229103\n",
      "iteration 1382, loss: 0.02201727218925953\n",
      "iteration 1383, loss: 0.023330841213464737\n",
      "iteration 1384, loss: 0.02350248396396637\n",
      "iteration 1385, loss: 0.023907655850052834\n",
      "iteration 1386, loss: 0.025224611163139343\n",
      "iteration 1387, loss: 0.02177807316184044\n",
      "iteration 1388, loss: 0.022865861654281616\n",
      "iteration 1389, loss: 0.025061804801225662\n",
      "iteration 1390, loss: 0.022418182343244553\n",
      "iteration 1391, loss: 0.0230034738779068\n",
      "iteration 1392, loss: 0.026042191311717033\n",
      "iteration 1393, loss: 0.021132497116923332\n",
      "iteration 1394, loss: 0.024501211941242218\n",
      "iteration 1395, loss: 0.020716343075037003\n",
      "iteration 1396, loss: 0.02403072826564312\n",
      "iteration 1397, loss: 0.023455843329429626\n",
      "iteration 1398, loss: 0.021903809159994125\n",
      "iteration 1399, loss: 0.0228867270052433\n",
      "iteration 1400, loss: 0.02192792110145092\n",
      "iteration 1401, loss: 0.02464216575026512\n",
      "iteration 1402, loss: 0.02323179319500923\n",
      "iteration 1403, loss: 0.021598517894744873\n",
      "iteration 1404, loss: 0.02100745588541031\n",
      "iteration 1405, loss: 0.022756338119506836\n",
      "iteration 1406, loss: 0.023079900071024895\n",
      "iteration 1407, loss: 0.024063706398010254\n",
      "iteration 1408, loss: 0.026495497673749924\n",
      "iteration 1409, loss: 0.024376817047595978\n",
      "iteration 1410, loss: 0.023182179778814316\n",
      "iteration 1411, loss: 0.020242929458618164\n",
      "iteration 1412, loss: 0.0214675460010767\n",
      "iteration 1413, loss: 0.023107420653104782\n",
      "iteration 1414, loss: 0.023394891992211342\n",
      "iteration 1415, loss: 0.022440297529101372\n",
      "iteration 1416, loss: 0.0211208276450634\n",
      "iteration 1417, loss: 0.02246321551501751\n",
      "iteration 1418, loss: 0.022967368364334106\n",
      "iteration 1419, loss: 0.02659698575735092\n",
      "iteration 1420, loss: 0.02429058402776718\n",
      "iteration 1421, loss: 0.022640325129032135\n",
      "iteration 1422, loss: 0.021808156743645668\n",
      "iteration 1423, loss: 0.023129358887672424\n",
      "iteration 1424, loss: 0.024467941373586655\n",
      "iteration 1425, loss: 0.023687254637479782\n",
      "iteration 1426, loss: 0.024526353925466537\n",
      "iteration 1427, loss: 0.023928023874759674\n",
      "iteration 1428, loss: 0.022769737988710403\n",
      "iteration 1429, loss: 0.021183118224143982\n",
      "iteration 1430, loss: 0.023188818246126175\n",
      "iteration 1431, loss: 0.02143697999417782\n",
      "iteration 1432, loss: 0.022055692970752716\n",
      "iteration 1433, loss: 0.02352038398385048\n",
      "iteration 1434, loss: 0.022844839841127396\n",
      "iteration 1435, loss: 0.0220466461032629\n",
      "iteration 1436, loss: 0.022260919213294983\n",
      "iteration 1437, loss: 0.021377604454755783\n",
      "iteration 1438, loss: 0.024527207016944885\n",
      "iteration 1439, loss: 0.022723957896232605\n",
      "iteration 1440, loss: 0.02118365466594696\n",
      "iteration 1441, loss: 0.023236723616719246\n",
      "iteration 1442, loss: 0.02076728083193302\n",
      "iteration 1443, loss: 0.023319188505411148\n",
      "iteration 1444, loss: 0.017173610627651215\n",
      "iteration 1445, loss: 0.023630879819393158\n",
      "iteration 1446, loss: 0.02133096754550934\n",
      "iteration 1447, loss: 0.020410172641277313\n",
      "iteration 1448, loss: 0.021384382620453835\n",
      "iteration 1449, loss: 0.02125537395477295\n",
      "iteration 1450, loss: 0.021443922072649002\n",
      "iteration 1451, loss: 0.020450033247470856\n",
      "iteration 1452, loss: 0.022826604545116425\n",
      "iteration 1453, loss: 0.025498932227492332\n",
      "iteration 1454, loss: 0.0221803467720747\n",
      "iteration 1455, loss: 0.024367576465010643\n",
      "iteration 1456, loss: 0.023297252133488655\n",
      "iteration 1457, loss: 0.022881459444761276\n",
      "iteration 1458, loss: 0.02187317982316017\n",
      "iteration 1459, loss: 0.022938767448067665\n",
      "iteration 1460, loss: 0.022486742585897446\n",
      "iteration 1461, loss: 0.025332223623991013\n",
      "iteration 1462, loss: 0.021608158946037292\n",
      "iteration 1463, loss: 0.023364011198282242\n",
      "iteration 1464, loss: 0.022310400381684303\n",
      "iteration 1465, loss: 0.02125616744160652\n",
      "iteration 1466, loss: 0.022326264530420303\n",
      "iteration 1467, loss: 0.02312173694372177\n",
      "iteration 1468, loss: 0.02330290526151657\n",
      "iteration 1469, loss: 0.021092236042022705\n",
      "iteration 1470, loss: 0.022265875712037086\n",
      "iteration 1471, loss: 0.021425869315862656\n",
      "iteration 1472, loss: 0.02175747975707054\n",
      "iteration 1473, loss: 0.021409066393971443\n",
      "iteration 1474, loss: 0.020333431661128998\n",
      "iteration 1475, loss: 0.021601689979434013\n",
      "iteration 1476, loss: 0.02093529887497425\n",
      "iteration 1477, loss: 0.02261485904455185\n",
      "iteration 1478, loss: 0.019837377592921257\n",
      "iteration 1479, loss: 0.022310059517621994\n",
      "iteration 1480, loss: 0.022017553448677063\n",
      "iteration 1481, loss: 0.020460618659853935\n",
      "iteration 1482, loss: 0.024283261969685555\n",
      "iteration 1483, loss: 0.019852809607982635\n",
      "iteration 1484, loss: 0.024258285760879517\n",
      "iteration 1485, loss: 0.02654077485203743\n",
      "iteration 1486, loss: 0.025504959747195244\n",
      "iteration 1487, loss: 0.021566741168498993\n",
      "iteration 1488, loss: 0.02206229232251644\n",
      "iteration 1489, loss: 0.020186368376016617\n",
      "iteration 1490, loss: 0.022008609026670456\n",
      "iteration 1491, loss: 0.022334786131978035\n",
      "iteration 1492, loss: 0.024259643629193306\n",
      "iteration 1493, loss: 0.021660206839442253\n",
      "iteration 1494, loss: 0.021015150472521782\n",
      "iteration 1495, loss: 0.02125798538327217\n",
      "iteration 1496, loss: 0.022329922765493393\n",
      "iteration 1497, loss: 0.020660588517785072\n",
      "iteration 1498, loss: 0.01957479491829872\n",
      "iteration 1499, loss: 0.02098747529089451\n",
      "iteration 1500, loss: 0.019543930888175964\n",
      "iteration 1501, loss: 0.023443955928087234\n",
      "iteration 1502, loss: 0.020924532786011696\n",
      "iteration 1503, loss: 0.02229347452521324\n",
      "iteration 1504, loss: 0.021799124777317047\n",
      "iteration 1505, loss: 0.020479511469602585\n",
      "iteration 1506, loss: 0.02162179723381996\n",
      "iteration 1507, loss: 0.023371662944555283\n",
      "iteration 1508, loss: 0.019482677802443504\n",
      "iteration 1509, loss: 0.024283982813358307\n",
      "iteration 1510, loss: 0.022107472643256187\n",
      "iteration 1511, loss: 0.02312767133116722\n",
      "iteration 1512, loss: 0.022678595036268234\n",
      "iteration 1513, loss: 0.02143646590411663\n",
      "iteration 1514, loss: 0.01982612907886505\n",
      "iteration 1515, loss: 0.02069186605513096\n",
      "iteration 1516, loss: 0.01972537487745285\n",
      "iteration 1517, loss: 0.024902306497097015\n",
      "iteration 1518, loss: 0.023544399067759514\n",
      "iteration 1519, loss: 0.023903008550405502\n",
      "iteration 1520, loss: 0.024003157392144203\n",
      "iteration 1521, loss: 0.022302601486444473\n",
      "iteration 1522, loss: 0.01939418911933899\n",
      "iteration 1523, loss: 0.020534809678792953\n",
      "iteration 1524, loss: 0.02348741516470909\n",
      "iteration 1525, loss: 0.021044177934527397\n",
      "iteration 1526, loss: 0.021081404760479927\n",
      "iteration 1527, loss: 0.01982514187693596\n",
      "iteration 1528, loss: 0.019369544461369514\n",
      "iteration 1529, loss: 0.023150378838181496\n",
      "iteration 1530, loss: 0.019027410075068474\n",
      "iteration 1531, loss: 0.022084034979343414\n",
      "iteration 1532, loss: 0.020275507122278214\n",
      "iteration 1533, loss: 0.020831478759646416\n",
      "iteration 1534, loss: 0.021365558728575706\n",
      "iteration 1535, loss: 0.0228770449757576\n",
      "iteration 1536, loss: 0.02454490028321743\n",
      "iteration 1537, loss: 0.023354927077889442\n",
      "iteration 1538, loss: 0.022131502628326416\n",
      "iteration 1539, loss: 0.019758956506848335\n",
      "iteration 1540, loss: 0.021506549790501595\n",
      "iteration 1541, loss: 0.0192618016153574\n",
      "iteration 1542, loss: 0.023978669196367264\n",
      "iteration 1543, loss: 0.021659057587385178\n",
      "iteration 1544, loss: 0.02163584902882576\n",
      "iteration 1545, loss: 0.019789788872003555\n",
      "iteration 1546, loss: 0.020089201629161835\n",
      "iteration 1547, loss: 0.022153714671730995\n",
      "iteration 1548, loss: 0.021347176283597946\n",
      "iteration 1549, loss: 0.019934598356485367\n",
      "iteration 1550, loss: 0.022339953109622\n",
      "iteration 1551, loss: 0.020824505016207695\n",
      "iteration 1552, loss: 0.020519502460956573\n",
      "iteration 1553, loss: 0.021522408351302147\n",
      "iteration 1554, loss: 0.020593959838151932\n",
      "iteration 1555, loss: 0.020236313343048096\n",
      "iteration 1556, loss: 0.02408219873905182\n",
      "iteration 1557, loss: 0.020305965095758438\n",
      "iteration 1558, loss: 0.02033812552690506\n",
      "iteration 1559, loss: 0.020832588896155357\n",
      "iteration 1560, loss: 0.019908178597688675\n",
      "iteration 1561, loss: 0.021294642239809036\n",
      "iteration 1562, loss: 0.01959308050572872\n",
      "iteration 1563, loss: 0.01923448033630848\n",
      "iteration 1564, loss: 0.019764788448810577\n",
      "iteration 1565, loss: 0.022162217646837234\n",
      "iteration 1566, loss: 0.02001248486340046\n",
      "iteration 1567, loss: 0.019990496337413788\n",
      "iteration 1568, loss: 0.020989662036299706\n",
      "iteration 1569, loss: 0.020677436143159866\n",
      "iteration 1570, loss: 0.02141668274998665\n",
      "iteration 1571, loss: 0.02228681743144989\n",
      "iteration 1572, loss: 0.021688655018806458\n",
      "iteration 1573, loss: 0.021057292819023132\n",
      "iteration 1574, loss: 0.023370202630758286\n",
      "iteration 1575, loss: 0.017976263538002968\n",
      "iteration 1576, loss: 0.02196919545531273\n",
      "iteration 1577, loss: 0.020311962813138962\n",
      "iteration 1578, loss: 0.0203150175511837\n",
      "iteration 1579, loss: 0.01872783899307251\n",
      "iteration 1580, loss: 0.020416472107172012\n",
      "iteration 1581, loss: 0.021070286631584167\n",
      "iteration 1582, loss: 0.02173503115773201\n",
      "iteration 1583, loss: 0.02184583619236946\n",
      "iteration 1584, loss: 0.022839874029159546\n",
      "iteration 1585, loss: 0.019827861338853836\n",
      "iteration 1586, loss: 0.022564824670553207\n",
      "iteration 1587, loss: 0.022305166348814964\n",
      "iteration 1588, loss: 0.022454068064689636\n",
      "iteration 1589, loss: 0.01854042522609234\n",
      "iteration 1590, loss: 0.021087994799017906\n",
      "iteration 1591, loss: 0.017350297421216965\n",
      "iteration 1592, loss: 0.023135924711823463\n",
      "iteration 1593, loss: 0.021122679114341736\n",
      "iteration 1594, loss: 0.02308475226163864\n",
      "iteration 1595, loss: 0.020314179360866547\n",
      "iteration 1596, loss: 0.02153785526752472\n",
      "iteration 1597, loss: 0.01952742040157318\n",
      "iteration 1598, loss: 0.021572694182395935\n",
      "iteration 1599, loss: 0.02095668390393257\n",
      "iteration 1600, loss: 0.019354496151208878\n",
      "iteration 1601, loss: 0.02077360637485981\n",
      "iteration 1602, loss: 0.020778095349669456\n",
      "iteration 1603, loss: 0.01938110962510109\n",
      "iteration 1604, loss: 0.019764386117458344\n",
      "iteration 1605, loss: 0.02257084846496582\n",
      "iteration 1606, loss: 0.022180907428264618\n",
      "iteration 1607, loss: 0.018909746780991554\n",
      "iteration 1608, loss: 0.018569879233837128\n",
      "iteration 1609, loss: 0.021944057196378708\n",
      "iteration 1610, loss: 0.020663442090153694\n",
      "iteration 1611, loss: 0.0210113525390625\n",
      "iteration 1612, loss: 0.020196769386529922\n",
      "iteration 1613, loss: 0.01899281144142151\n",
      "iteration 1614, loss: 0.02079581469297409\n",
      "iteration 1615, loss: 0.019891630858182907\n",
      "iteration 1616, loss: 0.019088324159383774\n",
      "iteration 1617, loss: 0.018987638875842094\n",
      "iteration 1618, loss: 0.022167198359966278\n",
      "iteration 1619, loss: 0.024520136415958405\n",
      "iteration 1620, loss: 0.021182436496019363\n",
      "iteration 1621, loss: 0.022653130814433098\n",
      "iteration 1622, loss: 0.01992214471101761\n",
      "iteration 1623, loss: 0.020126931369304657\n",
      "iteration 1624, loss: 0.0218801349401474\n",
      "iteration 1625, loss: 0.020793136209249496\n",
      "iteration 1626, loss: 0.021985430270433426\n",
      "iteration 1627, loss: 0.021898796781897545\n",
      "iteration 1628, loss: 0.022155476734042168\n",
      "iteration 1629, loss: 0.017178867012262344\n",
      "iteration 1630, loss: 0.018099766224622726\n",
      "iteration 1631, loss: 0.018811624497175217\n",
      "iteration 1632, loss: 0.01856178417801857\n",
      "iteration 1633, loss: 0.01902349665760994\n",
      "iteration 1634, loss: 0.020698612555861473\n",
      "iteration 1635, loss: 0.019020991399884224\n",
      "iteration 1636, loss: 0.018337396904826164\n",
      "iteration 1637, loss: 0.02117670513689518\n",
      "iteration 1638, loss: 0.023848315700888634\n",
      "iteration 1639, loss: 0.020625537261366844\n",
      "iteration 1640, loss: 0.02201666682958603\n",
      "iteration 1641, loss: 0.022687003016471863\n",
      "iteration 1642, loss: 0.019921962171792984\n",
      "iteration 1643, loss: 0.02058005891740322\n",
      "iteration 1644, loss: 0.019849726930260658\n",
      "iteration 1645, loss: 0.019240373745560646\n",
      "iteration 1646, loss: 0.022365182638168335\n",
      "iteration 1647, loss: 0.02016530930995941\n",
      "iteration 1648, loss: 0.020515792071819305\n",
      "iteration 1649, loss: 0.0199603158980608\n",
      "iteration 1650, loss: 0.020205697044730186\n",
      "iteration 1651, loss: 0.021388955414295197\n",
      "iteration 1652, loss: 0.019501803442835808\n",
      "iteration 1653, loss: 0.02059275470674038\n",
      "iteration 1654, loss: 0.01987781748175621\n",
      "iteration 1655, loss: 0.019541475921869278\n",
      "iteration 1656, loss: 0.01940283551812172\n",
      "iteration 1657, loss: 0.024352308362722397\n",
      "iteration 1658, loss: 0.02397354505956173\n",
      "iteration 1659, loss: 0.019182994961738586\n",
      "iteration 1660, loss: 0.02100149355828762\n",
      "iteration 1661, loss: 0.019816013053059578\n",
      "iteration 1662, loss: 0.021345291286706924\n",
      "iteration 1663, loss: 0.019099470227956772\n",
      "iteration 1664, loss: 0.01991896890103817\n",
      "iteration 1665, loss: 0.018356017768383026\n",
      "iteration 1666, loss: 0.019697705283761024\n",
      "iteration 1667, loss: 0.020432598888874054\n",
      "iteration 1668, loss: 0.02020823024213314\n",
      "iteration 1669, loss: 0.01965920254588127\n",
      "iteration 1670, loss: 0.020153168588876724\n",
      "iteration 1671, loss: 0.02058066613972187\n",
      "iteration 1672, loss: 0.02339084818959236\n",
      "iteration 1673, loss: 0.02054615318775177\n",
      "iteration 1674, loss: 0.01961740478873253\n",
      "iteration 1675, loss: 0.024222277104854584\n",
      "iteration 1676, loss: 0.020497284829616547\n",
      "iteration 1677, loss: 0.01818035915493965\n",
      "iteration 1678, loss: 0.020029328763484955\n",
      "iteration 1679, loss: 0.024406708776950836\n",
      "iteration 1680, loss: 0.019911857321858406\n",
      "iteration 1681, loss: 0.0192109327763319\n",
      "iteration 1682, loss: 0.01887170411646366\n",
      "iteration 1683, loss: 0.01811416819691658\n",
      "iteration 1684, loss: 0.019964836537837982\n",
      "iteration 1685, loss: 0.020788025110960007\n",
      "iteration 1686, loss: 0.02048332616686821\n",
      "iteration 1687, loss: 0.019002579152584076\n",
      "iteration 1688, loss: 0.02080208621919155\n",
      "iteration 1689, loss: 0.019903605803847313\n",
      "iteration 1690, loss: 0.019824784249067307\n",
      "iteration 1691, loss: 0.020567718893289566\n",
      "iteration 1692, loss: 0.01991082727909088\n",
      "iteration 1693, loss: 0.0201103538274765\n",
      "iteration 1694, loss: 0.0239386186003685\n",
      "iteration 1695, loss: 0.021458735689520836\n",
      "iteration 1696, loss: 0.02035313844680786\n",
      "iteration 1697, loss: 0.02028096467256546\n",
      "iteration 1698, loss: 0.018683256581425667\n",
      "iteration 1699, loss: 0.019389700144529343\n",
      "iteration 1700, loss: 0.022752663120627403\n",
      "iteration 1701, loss: 0.01984294131398201\n",
      "iteration 1702, loss: 0.017386462539434433\n",
      "iteration 1703, loss: 0.020280340686440468\n",
      "iteration 1704, loss: 0.019914129748940468\n",
      "iteration 1705, loss: 0.018958112224936485\n",
      "iteration 1706, loss: 0.02095683105289936\n",
      "iteration 1707, loss: 0.020857786759734154\n",
      "iteration 1708, loss: 0.01986869052052498\n",
      "iteration 1709, loss: 0.021066997200250626\n",
      "iteration 1710, loss: 0.021190188825130463\n",
      "iteration 1711, loss: 0.016522979363799095\n",
      "iteration 1712, loss: 0.020900094881653786\n",
      "iteration 1713, loss: 0.021295271813869476\n",
      "iteration 1714, loss: 0.018635030835866928\n",
      "iteration 1715, loss: 0.01921536773443222\n",
      "iteration 1716, loss: 0.02027643471956253\n",
      "iteration 1717, loss: 0.018283026292920113\n",
      "iteration 1718, loss: 0.018644176423549652\n",
      "iteration 1719, loss: 0.01975082978606224\n",
      "iteration 1720, loss: 0.019153106957674026\n",
      "iteration 1721, loss: 0.020320838317275047\n",
      "iteration 1722, loss: 0.021426059305667877\n",
      "iteration 1723, loss: 0.018113017082214355\n",
      "iteration 1724, loss: 0.020357154309749603\n",
      "iteration 1725, loss: 0.019611552357673645\n",
      "iteration 1726, loss: 0.01944749802350998\n",
      "iteration 1727, loss: 0.018328461796045303\n",
      "iteration 1728, loss: 0.018829258158802986\n",
      "iteration 1729, loss: 0.021112110465765\n",
      "iteration 1730, loss: 0.02094706892967224\n",
      "iteration 1731, loss: 0.020503563806414604\n",
      "iteration 1732, loss: 0.01831105165183544\n",
      "iteration 1733, loss: 0.018997250124812126\n",
      "iteration 1734, loss: 0.019465241581201553\n",
      "iteration 1735, loss: 0.021490249782800674\n",
      "iteration 1736, loss: 0.021784180775284767\n",
      "iteration 1737, loss: 0.023788457736372948\n",
      "iteration 1738, loss: 0.020024873316287994\n",
      "iteration 1739, loss: 0.020221121609210968\n",
      "iteration 1740, loss: 0.019129719585180283\n",
      "iteration 1741, loss: 0.019067663699388504\n",
      "iteration 1742, loss: 0.019764065742492676\n",
      "iteration 1743, loss: 0.020508619025349617\n",
      "iteration 1744, loss: 0.02224581688642502\n",
      "iteration 1745, loss: 0.01852322742342949\n",
      "iteration 1746, loss: 0.02194196730852127\n",
      "iteration 1747, loss: 0.017861397936940193\n",
      "iteration 1748, loss: 0.021003037691116333\n",
      "iteration 1749, loss: 0.020702674984931946\n",
      "iteration 1750, loss: 0.019584864377975464\n",
      "iteration 1751, loss: 0.018311038613319397\n",
      "iteration 1752, loss: 0.022142518311738968\n",
      "iteration 1753, loss: 0.017774084582924843\n",
      "iteration 1754, loss: 0.02218901365995407\n",
      "iteration 1755, loss: 0.02060537226498127\n",
      "iteration 1756, loss: 0.020825404673814774\n",
      "iteration 1757, loss: 0.020502189174294472\n",
      "iteration 1758, loss: 0.020137794315814972\n",
      "iteration 1759, loss: 0.018424995243549347\n",
      "iteration 1760, loss: 0.019159404560923576\n",
      "iteration 1761, loss: 0.019314538687467575\n",
      "iteration 1762, loss: 0.018894009292125702\n",
      "iteration 1763, loss: 0.01906416565179825\n",
      "iteration 1764, loss: 0.021830154582858086\n",
      "iteration 1765, loss: 0.018386492505669594\n",
      "iteration 1766, loss: 0.021268729120492935\n",
      "iteration 1767, loss: 0.020400777459144592\n",
      "iteration 1768, loss: 0.019880585372447968\n",
      "iteration 1769, loss: 0.018636386841535568\n",
      "iteration 1770, loss: 0.018012788146734238\n",
      "iteration 1771, loss: 0.021732818335294724\n",
      "iteration 1772, loss: 0.017615698277950287\n",
      "iteration 1773, loss: 0.019208062440156937\n",
      "iteration 1774, loss: 0.017038891091942787\n",
      "iteration 1775, loss: 0.01757097616791725\n",
      "iteration 1776, loss: 0.02267376333475113\n",
      "iteration 1777, loss: 0.020293831825256348\n",
      "iteration 1778, loss: 0.020099159330129623\n",
      "iteration 1779, loss: 0.01940140128135681\n",
      "iteration 1780, loss: 0.018331123515963554\n",
      "iteration 1781, loss: 0.021463826298713684\n",
      "iteration 1782, loss: 0.01782785728573799\n",
      "iteration 1783, loss: 0.018233411014080048\n",
      "iteration 1784, loss: 0.02252446673810482\n",
      "iteration 1785, loss: 0.021043280139565468\n",
      "iteration 1786, loss: 0.018335040658712387\n",
      "iteration 1787, loss: 0.01913607306778431\n",
      "iteration 1788, loss: 0.02005293034017086\n",
      "iteration 1789, loss: 0.01992833986878395\n",
      "iteration 1790, loss: 0.020653298124670982\n",
      "iteration 1791, loss: 0.022199321538209915\n",
      "iteration 1792, loss: 0.02100321277976036\n",
      "iteration 1793, loss: 0.0193310659378767\n",
      "iteration 1794, loss: 0.021299447864294052\n",
      "iteration 1795, loss: 0.019170785322785378\n",
      "iteration 1796, loss: 0.019984818994998932\n",
      "iteration 1797, loss: 0.019389431923627853\n",
      "iteration 1798, loss: 0.01958460360765457\n",
      "iteration 1799, loss: 0.019795624539256096\n",
      "iteration 1800, loss: 0.020194046199321747\n",
      "iteration 1801, loss: 0.01772412285208702\n",
      "iteration 1802, loss: 0.020918434485793114\n",
      "iteration 1803, loss: 0.01895834505558014\n",
      "iteration 1804, loss: 0.020201213657855988\n",
      "iteration 1805, loss: 0.018177974969148636\n",
      "iteration 1806, loss: 0.018653105944395065\n",
      "iteration 1807, loss: 0.019742850214242935\n",
      "iteration 1808, loss: 0.018974795937538147\n",
      "iteration 1809, loss: 0.018988775089383125\n",
      "iteration 1810, loss: 0.018030760809779167\n",
      "iteration 1811, loss: 0.018990978598594666\n",
      "iteration 1812, loss: 0.01986665278673172\n",
      "iteration 1813, loss: 0.021572332829236984\n",
      "iteration 1814, loss: 0.018847253173589706\n",
      "iteration 1815, loss: 0.021626772359013557\n",
      "iteration 1816, loss: 0.019949378445744514\n",
      "iteration 1817, loss: 0.016052190214395523\n",
      "iteration 1818, loss: 0.01903669908642769\n",
      "iteration 1819, loss: 0.020771734416484833\n",
      "iteration 1820, loss: 0.02045832946896553\n",
      "iteration 1821, loss: 0.021749082952737808\n",
      "iteration 1822, loss: 0.021920856088399887\n",
      "iteration 1823, loss: 0.020791754126548767\n",
      "iteration 1824, loss: 0.021921031177043915\n",
      "iteration 1825, loss: 0.019975710660219193\n",
      "iteration 1826, loss: 0.02153145708143711\n",
      "iteration 1827, loss: 0.016428541392087936\n",
      "iteration 1828, loss: 0.019520461559295654\n",
      "iteration 1829, loss: 0.02021612599492073\n",
      "iteration 1830, loss: 0.019185788929462433\n",
      "iteration 1831, loss: 0.01912727952003479\n",
      "iteration 1832, loss: 0.018586836755275726\n",
      "iteration 1833, loss: 0.018215548247098923\n",
      "iteration 1834, loss: 0.018361661583185196\n",
      "iteration 1835, loss: 0.016568854451179504\n",
      "iteration 1836, loss: 0.01817365363240242\n",
      "iteration 1837, loss: 0.02076893113553524\n",
      "iteration 1838, loss: 0.017704930156469345\n",
      "iteration 1839, loss: 0.018983986228704453\n",
      "iteration 1840, loss: 0.017467904835939407\n",
      "iteration 1841, loss: 0.02012256532907486\n",
      "iteration 1842, loss: 0.01681065745651722\n",
      "iteration 1843, loss: 0.017409546300768852\n",
      "iteration 1844, loss: 0.018289387226104736\n",
      "iteration 1845, loss: 0.017755411565303802\n",
      "iteration 1846, loss: 0.01761404424905777\n",
      "iteration 1847, loss: 0.01998300850391388\n",
      "iteration 1848, loss: 0.019266027957201004\n",
      "iteration 1849, loss: 0.02105284109711647\n",
      "iteration 1850, loss: 0.01837497018277645\n",
      "iteration 1851, loss: 0.019499674439430237\n",
      "iteration 1852, loss: 0.019242461770772934\n",
      "iteration 1853, loss: 0.020256968215107918\n",
      "iteration 1854, loss: 0.018427211791276932\n",
      "iteration 1855, loss: 0.01783997379243374\n",
      "iteration 1856, loss: 0.016142286360263824\n",
      "iteration 1857, loss: 0.018906431272625923\n",
      "iteration 1858, loss: 0.01602187380194664\n",
      "iteration 1859, loss: 0.019334621727466583\n",
      "iteration 1860, loss: 0.019154204055666924\n",
      "iteration 1861, loss: 0.019202005118131638\n",
      "iteration 1862, loss: 0.019773004576563835\n",
      "iteration 1863, loss: 0.0192482378333807\n",
      "iteration 1864, loss: 0.019252026453614235\n",
      "iteration 1865, loss: 0.01775551214814186\n",
      "iteration 1866, loss: 0.02048482373356819\n",
      "iteration 1867, loss: 0.017960939556360245\n",
      "iteration 1868, loss: 0.01756473071873188\n",
      "iteration 1869, loss: 0.02004235051572323\n",
      "iteration 1870, loss: 0.01907859370112419\n",
      "iteration 1871, loss: 0.017942674458026886\n",
      "iteration 1872, loss: 0.018409419804811478\n",
      "iteration 1873, loss: 0.01796518638730049\n",
      "iteration 1874, loss: 0.01941211149096489\n",
      "iteration 1875, loss: 0.01908155530691147\n",
      "iteration 1876, loss: 0.017536666244268417\n",
      "iteration 1877, loss: 0.02128451317548752\n",
      "iteration 1878, loss: 0.017376234754920006\n",
      "iteration 1879, loss: 0.01959734782576561\n",
      "iteration 1880, loss: 0.02024885267019272\n",
      "iteration 1881, loss: 0.02157658338546753\n",
      "iteration 1882, loss: 0.018738651648163795\n",
      "iteration 1883, loss: 0.017444543540477753\n",
      "iteration 1884, loss: 0.016810625791549683\n",
      "iteration 1885, loss: 0.01889527216553688\n",
      "iteration 1886, loss: 0.01836310513317585\n",
      "iteration 1887, loss: 0.01929227076470852\n",
      "iteration 1888, loss: 0.017633972689509392\n",
      "iteration 1889, loss: 0.01893986016511917\n",
      "iteration 1890, loss: 0.019044872373342514\n",
      "iteration 1891, loss: 0.017393790185451508\n",
      "iteration 1892, loss: 0.01816904917359352\n",
      "iteration 1893, loss: 0.019501671195030212\n",
      "iteration 1894, loss: 0.01703854650259018\n",
      "iteration 1895, loss: 0.019971618428826332\n",
      "iteration 1896, loss: 0.019065408036112785\n",
      "iteration 1897, loss: 0.018692266196012497\n",
      "iteration 1898, loss: 0.01732397824525833\n",
      "iteration 1899, loss: 0.01722242310643196\n",
      "iteration 1900, loss: 0.018215589225292206\n",
      "iteration 1901, loss: 0.01880243979394436\n",
      "iteration 1902, loss: 0.020426256582140923\n",
      "iteration 1903, loss: 0.019033219665288925\n",
      "iteration 1904, loss: 0.02001221850514412\n",
      "iteration 1905, loss: 0.017205778509378433\n",
      "iteration 1906, loss: 0.017452241852879524\n",
      "iteration 1907, loss: 0.020175926387310028\n",
      "iteration 1908, loss: 0.018254537135362625\n",
      "iteration 1909, loss: 0.019216932356357574\n",
      "iteration 1910, loss: 0.017970051616430283\n",
      "iteration 1911, loss: 0.019960027188062668\n",
      "iteration 1912, loss: 0.017320087179541588\n",
      "iteration 1913, loss: 0.018728481605648994\n",
      "iteration 1914, loss: 0.017688322812318802\n",
      "iteration 1915, loss: 0.019182011485099792\n",
      "iteration 1916, loss: 0.018174506723880768\n",
      "iteration 1917, loss: 0.021464094519615173\n",
      "iteration 1918, loss: 0.017771774902939796\n",
      "iteration 1919, loss: 0.016667846590280533\n",
      "iteration 1920, loss: 0.019769836217164993\n",
      "iteration 1921, loss: 0.02106824889779091\n",
      "iteration 1922, loss: 0.016191579401493073\n",
      "iteration 1923, loss: 0.018034061416983604\n",
      "iteration 1924, loss: 0.017976615577936172\n",
      "iteration 1925, loss: 0.017966987565159798\n",
      "iteration 1926, loss: 0.01837606355547905\n",
      "iteration 1927, loss: 0.020406242460012436\n",
      "iteration 1928, loss: 0.01772041991353035\n",
      "iteration 1929, loss: 0.019157420843839645\n",
      "iteration 1930, loss: 0.017417877912521362\n",
      "iteration 1931, loss: 0.018963657319545746\n",
      "iteration 1932, loss: 0.01657397672533989\n",
      "iteration 1933, loss: 0.01864464394748211\n",
      "iteration 1934, loss: 0.01580715924501419\n",
      "iteration 1935, loss: 0.017576996237039566\n",
      "iteration 1936, loss: 0.018673337996006012\n",
      "iteration 1937, loss: 0.01717185601592064\n",
      "iteration 1938, loss: 0.015860628336668015\n",
      "iteration 1939, loss: 0.016340695321559906\n",
      "iteration 1940, loss: 0.018079977482557297\n",
      "iteration 1941, loss: 0.01799839735031128\n",
      "iteration 1942, loss: 0.015805616974830627\n",
      "iteration 1943, loss: 0.018494633957743645\n",
      "iteration 1944, loss: 0.017920464277267456\n",
      "iteration 1945, loss: 0.016871657222509384\n",
      "iteration 1946, loss: 0.016359921544790268\n",
      "iteration 1947, loss: 0.0160058680921793\n",
      "iteration 1948, loss: 0.01947430521249771\n",
      "iteration 1949, loss: 0.01854124292731285\n",
      "iteration 1950, loss: 0.01741461455821991\n",
      "iteration 1951, loss: 0.017908107489347458\n",
      "iteration 1952, loss: 0.017650345340371132\n",
      "iteration 1953, loss: 0.01742865890264511\n",
      "iteration 1954, loss: 0.016731370240449905\n",
      "iteration 1955, loss: 0.017108801752328873\n",
      "iteration 1956, loss: 0.017154233530163765\n",
      "iteration 1957, loss: 0.01927460916340351\n",
      "iteration 1958, loss: 0.018048163503408432\n",
      "iteration 1959, loss: 0.018337946385145187\n",
      "iteration 1960, loss: 0.01670064404606819\n",
      "iteration 1961, loss: 0.016563106328248978\n",
      "iteration 1962, loss: 0.01820165291428566\n",
      "iteration 1963, loss: 0.014429502189159393\n",
      "iteration 1964, loss: 0.01924186386168003\n",
      "iteration 1965, loss: 0.018811779096722603\n",
      "iteration 1966, loss: 0.01781412959098816\n",
      "iteration 1967, loss: 0.016111653298139572\n",
      "iteration 1968, loss: 0.01787281595170498\n",
      "iteration 1969, loss: 0.015711341053247452\n",
      "iteration 1970, loss: 0.01910913735628128\n",
      "iteration 1971, loss: 0.018701860681176186\n",
      "iteration 1972, loss: 0.020970746874809265\n",
      "iteration 1973, loss: 0.01589067652821541\n",
      "iteration 1974, loss: 0.017116079106926918\n",
      "iteration 1975, loss: 0.019217712804675102\n",
      "iteration 1976, loss: 0.016904423013329506\n",
      "iteration 1977, loss: 0.01690404862165451\n",
      "iteration 1978, loss: 0.016025759279727936\n",
      "iteration 1979, loss: 0.016748331487178802\n",
      "iteration 1980, loss: 0.016584645956754684\n",
      "iteration 1981, loss: 0.017639122903347015\n",
      "iteration 1982, loss: 0.017387723550200462\n",
      "iteration 1983, loss: 0.017210830003023148\n",
      "iteration 1984, loss: 0.01745004579424858\n",
      "iteration 1985, loss: 0.020207293331623077\n",
      "iteration 1986, loss: 0.018810592591762543\n",
      "iteration 1987, loss: 0.01559537649154663\n",
      "iteration 1988, loss: 0.0178050734102726\n",
      "iteration 1989, loss: 0.017627812922000885\n",
      "iteration 1990, loss: 0.018250657245516777\n",
      "iteration 1991, loss: 0.017179124057292938\n",
      "iteration 1992, loss: 0.019734922796487808\n",
      "iteration 1993, loss: 0.01862528547644615\n",
      "iteration 1994, loss: 0.01924252137541771\n",
      "iteration 1995, loss: 0.019137516617774963\n",
      "iteration 1996, loss: 0.0172128826379776\n",
      "iteration 1997, loss: 0.016941353678703308\n",
      "iteration 1998, loss: 0.01592753827571869\n",
      "iteration 1999, loss: 0.017420820891857147\n",
      "iteration 2000, loss: 0.01882278546690941\n",
      "iteration 2001, loss: 0.016244053840637207\n",
      "iteration 2002, loss: 0.016482319682836533\n",
      "iteration 2003, loss: 0.017978273332118988\n",
      "iteration 2004, loss: 0.019858965650200844\n",
      "iteration 2005, loss: 0.017655937001109123\n",
      "iteration 2006, loss: 0.018078917637467384\n",
      "iteration 2007, loss: 0.018550047650933266\n",
      "iteration 2008, loss: 0.01625562086701393\n",
      "iteration 2009, loss: 0.016865413635969162\n",
      "iteration 2010, loss: 0.020790494978427887\n",
      "iteration 2011, loss: 0.01857168786227703\n",
      "iteration 2012, loss: 0.01696426421403885\n",
      "iteration 2013, loss: 0.017745567485690117\n",
      "iteration 2014, loss: 0.020112361758947372\n",
      "iteration 2015, loss: 0.022432580590248108\n",
      "iteration 2016, loss: 0.019138764590024948\n",
      "iteration 2017, loss: 0.018725808709859848\n",
      "iteration 2018, loss: 0.020491302013397217\n",
      "iteration 2019, loss: 0.019445501267910004\n",
      "iteration 2020, loss: 0.017907369881868362\n",
      "iteration 2021, loss: 0.018195543438196182\n",
      "iteration 2022, loss: 0.019734865054488182\n",
      "iteration 2023, loss: 0.016051294282078743\n",
      "iteration 2024, loss: 0.016327042132616043\n",
      "iteration 2025, loss: 0.01799364760518074\n",
      "iteration 2026, loss: 0.018846137449145317\n",
      "iteration 2027, loss: 0.016954168677330017\n",
      "iteration 2028, loss: 0.018409568816423416\n",
      "iteration 2029, loss: 0.01657688245177269\n",
      "iteration 2030, loss: 0.016744237393140793\n",
      "iteration 2031, loss: 0.019540458917617798\n",
      "iteration 2032, loss: 0.017974361777305603\n",
      "iteration 2033, loss: 0.018101483583450317\n",
      "iteration 2034, loss: 0.018886573612689972\n",
      "iteration 2035, loss: 0.017342422157526016\n",
      "iteration 2036, loss: 0.0171881802380085\n",
      "iteration 2037, loss: 0.017728425562381744\n",
      "iteration 2038, loss: 0.014154734089970589\n",
      "iteration 2039, loss: 0.015352794900536537\n",
      "iteration 2040, loss: 0.017192721366882324\n",
      "iteration 2041, loss: 0.0191548690199852\n",
      "iteration 2042, loss: 0.017319712787866592\n",
      "iteration 2043, loss: 0.016796329990029335\n",
      "iteration 2044, loss: 0.016722779721021652\n",
      "iteration 2045, loss: 0.01922968029975891\n",
      "iteration 2046, loss: 0.01585141196846962\n",
      "iteration 2047, loss: 0.020096328109502792\n",
      "iteration 2048, loss: 0.01841929368674755\n",
      "iteration 2049, loss: 0.017969904467463493\n",
      "iteration 2050, loss: 0.017024222761392593\n",
      "iteration 2051, loss: 0.01834433153271675\n",
      "iteration 2052, loss: 0.01697150617837906\n",
      "iteration 2053, loss: 0.01787719503045082\n",
      "iteration 2054, loss: 0.01689968630671501\n",
      "iteration 2055, loss: 0.020420053973793983\n",
      "iteration 2056, loss: 0.01781497150659561\n",
      "iteration 2057, loss: 0.019241832196712494\n",
      "iteration 2058, loss: 0.017628557980060577\n",
      "iteration 2059, loss: 0.018640760332345963\n",
      "iteration 2060, loss: 0.016863126307725906\n",
      "iteration 2061, loss: 0.017647383734583855\n",
      "iteration 2062, loss: 0.01780049502849579\n",
      "iteration 2063, loss: 0.0163891538977623\n",
      "iteration 2064, loss: 0.018131719902157784\n",
      "iteration 2065, loss: 0.01564629375934601\n",
      "iteration 2066, loss: 0.019422372803092003\n",
      "iteration 2067, loss: 0.01707504875957966\n",
      "iteration 2068, loss: 0.017031986266374588\n",
      "iteration 2069, loss: 0.01694628596305847\n",
      "iteration 2070, loss: 0.017260022461414337\n",
      "iteration 2071, loss: 0.018580470234155655\n",
      "iteration 2072, loss: 0.01721060276031494\n",
      "iteration 2073, loss: 0.015361692756414413\n",
      "iteration 2074, loss: 0.015431092120707035\n",
      "iteration 2075, loss: 0.01663355529308319\n",
      "iteration 2076, loss: 0.01595885679125786\n",
      "iteration 2077, loss: 0.018222570419311523\n",
      "iteration 2078, loss: 0.019754543900489807\n",
      "iteration 2079, loss: 0.01825595460832119\n",
      "iteration 2080, loss: 0.016140207648277283\n",
      "iteration 2081, loss: 0.01681385561823845\n",
      "iteration 2082, loss: 0.01528842095285654\n",
      "iteration 2083, loss: 0.01772236078977585\n",
      "iteration 2084, loss: 0.01731358841061592\n",
      "iteration 2085, loss: 0.014037469401955605\n",
      "iteration 2086, loss: 0.018327660858631134\n",
      "iteration 2087, loss: 0.01651744171977043\n",
      "iteration 2088, loss: 0.01529605221003294\n",
      "iteration 2089, loss: 0.0173838809132576\n",
      "iteration 2090, loss: 0.01848694682121277\n",
      "iteration 2091, loss: 0.01583496853709221\n",
      "iteration 2092, loss: 0.01660226099193096\n",
      "iteration 2093, loss: 0.01745595782995224\n",
      "iteration 2094, loss: 0.01683105155825615\n",
      "iteration 2095, loss: 0.016107957810163498\n",
      "iteration 2096, loss: 0.016134291887283325\n",
      "iteration 2097, loss: 0.019491026178002357\n",
      "iteration 2098, loss: 0.016207031905651093\n",
      "iteration 2099, loss: 0.019476788118481636\n",
      "iteration 2100, loss: 0.01620844379067421\n",
      "iteration 2101, loss: 0.017832430079579353\n",
      "iteration 2102, loss: 0.01786229759454727\n",
      "iteration 2103, loss: 0.016101710498332977\n",
      "iteration 2104, loss: 0.016069186851382256\n",
      "iteration 2105, loss: 0.01876399666070938\n",
      "iteration 2106, loss: 0.019241908565163612\n",
      "iteration 2107, loss: 0.017518624663352966\n",
      "iteration 2108, loss: 0.01837657392024994\n",
      "iteration 2109, loss: 0.018738973885774612\n",
      "iteration 2110, loss: 0.018787944689393044\n",
      "iteration 2111, loss: 0.017524421215057373\n",
      "iteration 2112, loss: 0.018417440354824066\n",
      "iteration 2113, loss: 0.014833316206932068\n",
      "iteration 2114, loss: 0.018359830603003502\n",
      "iteration 2115, loss: 0.017674308270215988\n",
      "iteration 2116, loss: 0.018838338553905487\n",
      "iteration 2117, loss: 0.018797950819134712\n",
      "iteration 2118, loss: 0.017552029341459274\n",
      "iteration 2119, loss: 0.013680439442396164\n",
      "iteration 2120, loss: 0.014486026018857956\n",
      "iteration 2121, loss: 0.01706245169043541\n",
      "iteration 2122, loss: 0.01795618236064911\n",
      "iteration 2123, loss: 0.01754717156291008\n",
      "iteration 2124, loss: 0.016266483813524246\n",
      "iteration 2125, loss: 0.01716981828212738\n",
      "iteration 2126, loss: 0.015568193979561329\n",
      "iteration 2127, loss: 0.019456442445516586\n",
      "iteration 2128, loss: 0.01692837104201317\n",
      "iteration 2129, loss: 0.015532293356955051\n",
      "iteration 2130, loss: 0.01669193059206009\n",
      "iteration 2131, loss: 0.017314903438091278\n",
      "iteration 2132, loss: 0.018436117097735405\n",
      "iteration 2133, loss: 0.01659775897860527\n",
      "iteration 2134, loss: 0.019579418003559113\n",
      "iteration 2135, loss: 0.017639923840761185\n",
      "iteration 2136, loss: 0.0165494903922081\n",
      "iteration 2137, loss: 0.018321678042411804\n",
      "iteration 2138, loss: 0.01650974340736866\n",
      "iteration 2139, loss: 0.01840912364423275\n",
      "iteration 2140, loss: 0.01873099058866501\n",
      "iteration 2141, loss: 0.015016257762908936\n",
      "iteration 2142, loss: 0.016739575192332268\n",
      "iteration 2143, loss: 0.015511590987443924\n",
      "iteration 2144, loss: 0.01587614044547081\n",
      "iteration 2145, loss: 0.01711682602763176\n",
      "iteration 2146, loss: 0.016364458948373795\n",
      "iteration 2147, loss: 0.01725614070892334\n",
      "iteration 2148, loss: 0.016879986971616745\n",
      "iteration 2149, loss: 0.016172606498003006\n",
      "iteration 2150, loss: 0.016374535858631134\n",
      "iteration 2151, loss: 0.018156588077545166\n",
      "iteration 2152, loss: 0.015935467556118965\n",
      "iteration 2153, loss: 0.015551499091088772\n",
      "iteration 2154, loss: 0.01582663133740425\n",
      "iteration 2155, loss: 0.014487650245428085\n",
      "iteration 2156, loss: 0.015779217705130577\n",
      "iteration 2157, loss: 0.016523852944374084\n",
      "iteration 2158, loss: 0.01614922471344471\n",
      "iteration 2159, loss: 0.016987595707178116\n",
      "iteration 2160, loss: 0.016303133219480515\n",
      "iteration 2161, loss: 0.014851201325654984\n",
      "iteration 2162, loss: 0.01548236794769764\n",
      "iteration 2163, loss: 0.014977790415287018\n",
      "iteration 2164, loss: 0.01693210005760193\n",
      "iteration 2165, loss: 0.016727112233638763\n",
      "iteration 2166, loss: 0.018551483750343323\n",
      "iteration 2167, loss: 0.0161491297185421\n",
      "iteration 2168, loss: 0.015879325568675995\n",
      "iteration 2169, loss: 0.01879163645207882\n",
      "iteration 2170, loss: 0.015562020242214203\n",
      "iteration 2171, loss: 0.019691577181220055\n",
      "iteration 2172, loss: 0.015216088853776455\n",
      "iteration 2173, loss: 0.018035031855106354\n",
      "iteration 2174, loss: 0.01766989938914776\n",
      "iteration 2175, loss: 0.017321787774562836\n",
      "iteration 2176, loss: 0.017145540565252304\n",
      "iteration 2177, loss: 0.015807537361979485\n",
      "iteration 2178, loss: 0.01682193949818611\n",
      "iteration 2179, loss: 0.015370074659585953\n",
      "iteration 2180, loss: 0.0171625018119812\n",
      "iteration 2181, loss: 0.015363159589469433\n",
      "iteration 2182, loss: 0.014642279595136642\n",
      "iteration 2183, loss: 0.016886048018932343\n",
      "iteration 2184, loss: 0.016242321580648422\n",
      "iteration 2185, loss: 0.017362911254167557\n",
      "iteration 2186, loss: 0.01717301830649376\n",
      "iteration 2187, loss: 0.01609880104660988\n",
      "iteration 2188, loss: 0.01875295862555504\n",
      "iteration 2189, loss: 0.016265900805592537\n",
      "iteration 2190, loss: 0.01459948904812336\n",
      "iteration 2191, loss: 0.014192976988852024\n",
      "iteration 2192, loss: 0.015961507335305214\n",
      "iteration 2193, loss: 0.017701946198940277\n",
      "iteration 2194, loss: 0.016931600868701935\n",
      "iteration 2195, loss: 0.015261376276612282\n",
      "iteration 2196, loss: 0.01638122648000717\n",
      "iteration 2197, loss: 0.01618874818086624\n",
      "iteration 2198, loss: 0.016315406188368797\n",
      "iteration 2199, loss: 0.015428281389176846\n",
      "iteration 2200, loss: 0.016770893707871437\n",
      "iteration 2201, loss: 0.015592141076922417\n",
      "iteration 2202, loss: 0.01603619195520878\n",
      "iteration 2203, loss: 0.015272623859345913\n",
      "iteration 2204, loss: 0.01538925152271986\n",
      "iteration 2205, loss: 0.01653244160115719\n",
      "iteration 2206, loss: 0.01684652455151081\n",
      "iteration 2207, loss: 0.016954921185970306\n",
      "iteration 2208, loss: 0.015208406373858452\n",
      "iteration 2209, loss: 0.014313377439975739\n",
      "iteration 2210, loss: 0.01614915393292904\n",
      "iteration 2211, loss: 0.015514619648456573\n",
      "iteration 2212, loss: 0.014301341027021408\n",
      "iteration 2213, loss: 0.01673285663127899\n",
      "iteration 2214, loss: 0.014858225360512733\n",
      "iteration 2215, loss: 0.015648946166038513\n",
      "iteration 2216, loss: 0.013751063495874405\n",
      "iteration 2217, loss: 0.014330278150737286\n",
      "iteration 2218, loss: 0.015644509345293045\n",
      "iteration 2219, loss: 0.016570216044783592\n",
      "iteration 2220, loss: 0.016078582033514977\n",
      "iteration 2221, loss: 0.015453344210982323\n",
      "iteration 2222, loss: 0.016751497983932495\n",
      "iteration 2223, loss: 0.014601832255721092\n",
      "iteration 2224, loss: 0.015645163133740425\n",
      "iteration 2225, loss: 0.014050396159291267\n",
      "iteration 2226, loss: 0.015629682689905167\n",
      "iteration 2227, loss: 0.015136866830289364\n",
      "iteration 2228, loss: 0.01604984700679779\n",
      "iteration 2229, loss: 0.015696696937084198\n",
      "iteration 2230, loss: 0.015777315944433212\n",
      "iteration 2231, loss: 0.016890745609998703\n",
      "iteration 2232, loss: 0.016284581273794174\n",
      "iteration 2233, loss: 0.016198890283703804\n",
      "iteration 2234, loss: 0.01772408001124859\n",
      "iteration 2235, loss: 0.01631488837301731\n",
      "iteration 2236, loss: 0.014796311967074871\n",
      "iteration 2237, loss: 0.017936106771230698\n",
      "iteration 2238, loss: 0.015799634158611298\n",
      "iteration 2239, loss: 0.014606427401304245\n",
      "iteration 2240, loss: 0.014310146681964397\n",
      "iteration 2241, loss: 0.015780050307512283\n",
      "iteration 2242, loss: 0.015083788894116879\n",
      "iteration 2243, loss: 0.015646696090698242\n",
      "iteration 2244, loss: 0.013415511697530746\n",
      "iteration 2245, loss: 0.014106906950473785\n",
      "iteration 2246, loss: 0.01426522247493267\n",
      "iteration 2247, loss: 0.01708146184682846\n",
      "iteration 2248, loss: 0.015601380728185177\n",
      "iteration 2249, loss: 0.01617920584976673\n",
      "iteration 2250, loss: 0.01613527163863182\n",
      "iteration 2251, loss: 0.015842612832784653\n",
      "iteration 2252, loss: 0.01585775800049305\n",
      "iteration 2253, loss: 0.014666156843304634\n",
      "iteration 2254, loss: 0.016367048025131226\n",
      "iteration 2255, loss: 0.014936474151909351\n",
      "iteration 2256, loss: 0.016488760709762573\n",
      "iteration 2257, loss: 0.015269502066075802\n",
      "iteration 2258, loss: 0.015013018622994423\n",
      "iteration 2259, loss: 0.015937384217977524\n",
      "iteration 2260, loss: 0.01611725613474846\n",
      "iteration 2261, loss: 0.014995330944657326\n",
      "iteration 2262, loss: 0.016310978680849075\n",
      "iteration 2263, loss: 0.0143665736541152\n",
      "iteration 2264, loss: 0.016458511352539062\n",
      "iteration 2265, loss: 0.015352819114923477\n",
      "iteration 2266, loss: 0.016171809285879135\n",
      "iteration 2267, loss: 0.015148074366152287\n",
      "iteration 2268, loss: 0.014223851263523102\n",
      "iteration 2269, loss: 0.017219506204128265\n",
      "iteration 2270, loss: 0.0160839706659317\n",
      "iteration 2271, loss: 0.016307346522808075\n",
      "iteration 2272, loss: 0.01547750923782587\n",
      "iteration 2273, loss: 0.016127604991197586\n",
      "iteration 2274, loss: 0.01663351058959961\n",
      "iteration 2275, loss: 0.015915103256702423\n",
      "iteration 2276, loss: 0.013668242841959\n",
      "iteration 2277, loss: 0.01670221984386444\n",
      "iteration 2278, loss: 0.016041526570916176\n",
      "iteration 2279, loss: 0.0158559400588274\n",
      "iteration 2280, loss: 0.016583170741796494\n",
      "iteration 2281, loss: 0.016240451484918594\n",
      "iteration 2282, loss: 0.0165548175573349\n",
      "iteration 2283, loss: 0.017036782577633858\n",
      "iteration 2284, loss: 0.017692076042294502\n",
      "iteration 2285, loss: 0.015237254090607166\n",
      "iteration 2286, loss: 0.016104163601994514\n",
      "iteration 2287, loss: 0.016251418739557266\n",
      "iteration 2288, loss: 0.01348899770528078\n",
      "iteration 2289, loss: 0.01539548672735691\n",
      "iteration 2290, loss: 0.016431834548711777\n",
      "iteration 2291, loss: 0.014974744990468025\n",
      "iteration 2292, loss: 0.016154196113348007\n",
      "iteration 2293, loss: 0.016831956803798676\n",
      "iteration 2294, loss: 0.014546563848853111\n",
      "iteration 2295, loss: 0.018050625920295715\n",
      "iteration 2296, loss: 0.01631884276866913\n",
      "iteration 2297, loss: 0.014286000281572342\n",
      "iteration 2298, loss: 0.016468852758407593\n",
      "iteration 2299, loss: 0.016420681029558182\n",
      "iteration 2300, loss: 0.014770003966987133\n",
      "iteration 2301, loss: 0.01661326177418232\n",
      "iteration 2302, loss: 0.017487091943621635\n",
      "iteration 2303, loss: 0.01721160113811493\n",
      "iteration 2304, loss: 0.015393956564366817\n",
      "iteration 2305, loss: 0.01558369118720293\n",
      "iteration 2306, loss: 0.014241970144212246\n",
      "iteration 2307, loss: 0.013312789611518383\n",
      "iteration 2308, loss: 0.01516285352408886\n",
      "iteration 2309, loss: 0.015879221260547638\n",
      "iteration 2310, loss: 0.016853056848049164\n",
      "iteration 2311, loss: 0.016047252342104912\n",
      "iteration 2312, loss: 0.014074252918362617\n",
      "iteration 2313, loss: 0.013327346183359623\n",
      "iteration 2314, loss: 0.015271324664354324\n",
      "iteration 2315, loss: 0.014285767450928688\n",
      "iteration 2316, loss: 0.0156841017305851\n",
      "iteration 2317, loss: 0.015418518334627151\n",
      "iteration 2318, loss: 0.014793271198868752\n",
      "iteration 2319, loss: 0.013920038938522339\n",
      "iteration 2320, loss: 0.016434837132692337\n",
      "iteration 2321, loss: 0.015939468517899513\n",
      "iteration 2322, loss: 0.01657666452229023\n",
      "iteration 2323, loss: 0.01551328506320715\n",
      "iteration 2324, loss: 0.014835718087852001\n",
      "iteration 2325, loss: 0.01410387922078371\n",
      "iteration 2326, loss: 0.015458177775144577\n",
      "iteration 2327, loss: 0.014110613614320755\n",
      "iteration 2328, loss: 0.015406915917992592\n",
      "iteration 2329, loss: 0.014410605654120445\n",
      "iteration 2330, loss: 0.01607241854071617\n",
      "iteration 2331, loss: 0.014482910744845867\n",
      "iteration 2332, loss: 0.015091047622263432\n",
      "iteration 2333, loss: 0.015788167715072632\n",
      "iteration 2334, loss: 0.018330488353967667\n",
      "iteration 2335, loss: 0.016307799145579338\n",
      "iteration 2336, loss: 0.015004750341176987\n",
      "iteration 2337, loss: 0.016222340986132622\n",
      "iteration 2338, loss: 0.01842377707362175\n",
      "iteration 2339, loss: 0.014524145051836967\n",
      "iteration 2340, loss: 0.015850000083446503\n",
      "iteration 2341, loss: 0.014844694174826145\n",
      "iteration 2342, loss: 0.017376665025949478\n",
      "iteration 2343, loss: 0.014456138014793396\n",
      "iteration 2344, loss: 0.01520548015832901\n",
      "iteration 2345, loss: 0.013571112416684628\n",
      "iteration 2346, loss: 0.014483845792710781\n",
      "iteration 2347, loss: 0.016004454344511032\n",
      "iteration 2348, loss: 0.01356750912964344\n",
      "iteration 2349, loss: 0.01456504873931408\n",
      "iteration 2350, loss: 0.013721466064453125\n",
      "iteration 2351, loss: 0.016171198338270187\n",
      "iteration 2352, loss: 0.013479743152856827\n",
      "iteration 2353, loss: 0.015352046117186546\n",
      "iteration 2354, loss: 0.017428208142518997\n",
      "iteration 2355, loss: 0.016673550009727478\n",
      "iteration 2356, loss: 0.015791326761245728\n",
      "iteration 2357, loss: 0.01646796241402626\n",
      "iteration 2358, loss: 0.014840275049209595\n",
      "iteration 2359, loss: 0.014424142427742481\n",
      "iteration 2360, loss: 0.015789007768034935\n",
      "iteration 2361, loss: 0.017811333760619164\n",
      "iteration 2362, loss: 0.01467055082321167\n",
      "iteration 2363, loss: 0.014981383457779884\n",
      "iteration 2364, loss: 0.014087861403822899\n",
      "iteration 2365, loss: 0.017141427844762802\n",
      "iteration 2366, loss: 0.012680166400969028\n",
      "iteration 2367, loss: 0.013838659971952438\n",
      "iteration 2368, loss: 0.015547937713563442\n",
      "iteration 2369, loss: 0.015446831472218037\n",
      "iteration 2370, loss: 0.012670976109802723\n",
      "iteration 2371, loss: 0.013906147330999374\n",
      "iteration 2372, loss: 0.01375911757349968\n",
      "iteration 2373, loss: 0.01696525514125824\n",
      "iteration 2374, loss: 0.015062209218740463\n",
      "iteration 2375, loss: 0.016223322600126266\n",
      "iteration 2376, loss: 0.01533816009759903\n",
      "iteration 2377, loss: 0.013860920444130898\n",
      "iteration 2378, loss: 0.0157407708466053\n",
      "iteration 2379, loss: 0.013276202604174614\n",
      "iteration 2380, loss: 0.015217062085866928\n",
      "iteration 2381, loss: 0.01344858668744564\n",
      "iteration 2382, loss: 0.014529208652675152\n",
      "iteration 2383, loss: 0.014619563706219196\n",
      "iteration 2384, loss: 0.016072802245616913\n",
      "iteration 2385, loss: 0.012624109163880348\n",
      "iteration 2386, loss: 0.014578187838196754\n",
      "iteration 2387, loss: 0.01545384805649519\n",
      "iteration 2388, loss: 0.016234468668699265\n",
      "iteration 2389, loss: 0.015107017941772938\n",
      "iteration 2390, loss: 0.013909904286265373\n",
      "iteration 2391, loss: 0.01391659490764141\n",
      "iteration 2392, loss: 0.014507165178656578\n",
      "iteration 2393, loss: 0.015470541082322598\n",
      "iteration 2394, loss: 0.013533472083508968\n",
      "iteration 2395, loss: 0.017538903281092644\n",
      "iteration 2396, loss: 0.014310711994767189\n",
      "iteration 2397, loss: 0.015385252423584461\n",
      "iteration 2398, loss: 0.01608709618449211\n",
      "iteration 2399, loss: 0.01525870431214571\n",
      "iteration 2400, loss: 0.015462081879377365\n",
      "iteration 2401, loss: 0.014283309690654278\n",
      "iteration 2402, loss: 0.015279926359653473\n",
      "iteration 2403, loss: 0.014764683321118355\n",
      "iteration 2404, loss: 0.015334591269493103\n",
      "iteration 2405, loss: 0.016296280547976494\n",
      "iteration 2406, loss: 0.015046671032905579\n",
      "iteration 2407, loss: 0.014136632904410362\n",
      "iteration 2408, loss: 0.015585050918161869\n",
      "iteration 2409, loss: 0.015454858541488647\n",
      "iteration 2410, loss: 0.01624576561152935\n",
      "iteration 2411, loss: 0.013042940758168697\n",
      "iteration 2412, loss: 0.01605749875307083\n",
      "iteration 2413, loss: 0.015809565782546997\n",
      "iteration 2414, loss: 0.01515595056116581\n",
      "iteration 2415, loss: 0.0163602065294981\n",
      "iteration 2416, loss: 0.0167971421033144\n",
      "iteration 2417, loss: 0.014232015237212181\n",
      "iteration 2418, loss: 0.01361088640987873\n",
      "iteration 2419, loss: 0.01668979972600937\n",
      "iteration 2420, loss: 0.0142619414255023\n",
      "iteration 2421, loss: 0.017644280567765236\n",
      "iteration 2422, loss: 0.015878895297646523\n",
      "iteration 2423, loss: 0.014901083894073963\n",
      "iteration 2424, loss: 0.013221165165305138\n",
      "iteration 2425, loss: 0.016083311289548874\n",
      "iteration 2426, loss: 0.015172102488577366\n",
      "iteration 2427, loss: 0.015438612550497055\n",
      "iteration 2428, loss: 0.014202635735273361\n",
      "iteration 2429, loss: 0.016163703054189682\n",
      "iteration 2430, loss: 0.015305031090974808\n",
      "iteration 2431, loss: 0.015043435618281364\n",
      "iteration 2432, loss: 0.015866292640566826\n",
      "iteration 2433, loss: 0.016237815842032433\n",
      "iteration 2434, loss: 0.01573692448437214\n",
      "iteration 2435, loss: 0.01589575782418251\n",
      "iteration 2436, loss: 0.01374150812625885\n",
      "iteration 2437, loss: 0.01454242691397667\n",
      "iteration 2438, loss: 0.0143842538818717\n",
      "iteration 2439, loss: 0.016458826139569283\n",
      "iteration 2440, loss: 0.01707267016172409\n",
      "iteration 2441, loss: 0.015528503805398941\n",
      "iteration 2442, loss: 0.01438107155263424\n",
      "iteration 2443, loss: 0.013404765166342258\n",
      "iteration 2444, loss: 0.017036262899637222\n",
      "iteration 2445, loss: 0.015042496845126152\n",
      "iteration 2446, loss: 0.014292897656559944\n",
      "iteration 2447, loss: 0.014201472513377666\n",
      "iteration 2448, loss: 0.0154273621737957\n",
      "iteration 2449, loss: 0.01642262563109398\n",
      "iteration 2450, loss: 0.016030648723244667\n",
      "iteration 2451, loss: 0.014227521605789661\n",
      "iteration 2452, loss: 0.01827210932970047\n",
      "iteration 2453, loss: 0.015414886176586151\n",
      "iteration 2454, loss: 0.01440943032503128\n",
      "iteration 2455, loss: 0.0157557912170887\n",
      "iteration 2456, loss: 0.014955230057239532\n",
      "iteration 2457, loss: 0.013995464891195297\n",
      "iteration 2458, loss: 0.014025780372321606\n",
      "iteration 2459, loss: 0.014887874945998192\n",
      "iteration 2460, loss: 0.014378358609974384\n",
      "iteration 2461, loss: 0.017340118065476418\n",
      "iteration 2462, loss: 0.015133509412407875\n",
      "iteration 2463, loss: 0.013944115489721298\n",
      "iteration 2464, loss: 0.01570703834295273\n",
      "iteration 2465, loss: 0.01509547233581543\n",
      "iteration 2466, loss: 0.014800477772951126\n",
      "iteration 2467, loss: 0.01511689368635416\n",
      "iteration 2468, loss: 0.016329314559698105\n",
      "iteration 2469, loss: 0.014089928939938545\n",
      "iteration 2470, loss: 0.014006266370415688\n",
      "iteration 2471, loss: 0.013372565619647503\n",
      "iteration 2472, loss: 0.013965731486678123\n",
      "iteration 2473, loss: 0.015164202079176903\n",
      "iteration 2474, loss: 0.013733137398958206\n",
      "iteration 2475, loss: 0.01421252079308033\n",
      "iteration 2476, loss: 0.01475499663501978\n",
      "iteration 2477, loss: 0.014939699321985245\n",
      "iteration 2478, loss: 0.013679428957402706\n",
      "iteration 2479, loss: 0.01644495502114296\n",
      "iteration 2480, loss: 0.013892723247408867\n",
      "iteration 2481, loss: 0.014691532589495182\n",
      "iteration 2482, loss: 0.014813106507062912\n",
      "iteration 2483, loss: 0.015804463997483253\n",
      "iteration 2484, loss: 0.01632584072649479\n",
      "iteration 2485, loss: 0.013585202395915985\n",
      "iteration 2486, loss: 0.015053920447826385\n",
      "iteration 2487, loss: 0.013024444691836834\n",
      "iteration 2488, loss: 0.011460112407803535\n",
      "iteration 2489, loss: 0.014758598059415817\n",
      "iteration 2490, loss: 0.016318680718541145\n",
      "iteration 2491, loss: 0.012283233925700188\n",
      "iteration 2492, loss: 0.013091495260596275\n",
      "iteration 2493, loss: 0.01513654738664627\n",
      "iteration 2494, loss: 0.014653339050710201\n",
      "iteration 2495, loss: 0.015010178089141846\n",
      "iteration 2496, loss: 0.016917986795306206\n",
      "iteration 2497, loss: 0.013201920315623283\n",
      "iteration 2498, loss: 0.01610853523015976\n",
      "iteration 2499, loss: 0.01755715161561966\n",
      "iteration 2500, loss: 0.01460395846515894\n",
      "iteration 2501, loss: 0.0158708393573761\n",
      "iteration 2502, loss: 0.013432549312710762\n",
      "iteration 2503, loss: 0.013140637427568436\n",
      "iteration 2504, loss: 0.014461302198469639\n",
      "iteration 2505, loss: 0.015716206282377243\n",
      "iteration 2506, loss: 0.014789070002734661\n",
      "iteration 2507, loss: 0.011670590378344059\n",
      "iteration 2508, loss: 0.015968946740031242\n",
      "iteration 2509, loss: 0.01366572268307209\n",
      "iteration 2510, loss: 0.014890113845467567\n",
      "iteration 2511, loss: 0.014758949168026447\n",
      "iteration 2512, loss: 0.01527536753565073\n",
      "iteration 2513, loss: 0.01567639783024788\n",
      "iteration 2514, loss: 0.01328903529793024\n",
      "iteration 2515, loss: 0.014700989238917828\n",
      "iteration 2516, loss: 0.014203928411006927\n",
      "iteration 2517, loss: 0.0159462857991457\n",
      "iteration 2518, loss: 0.01529635675251484\n",
      "iteration 2519, loss: 0.013305818662047386\n",
      "iteration 2520, loss: 0.014858783222734928\n",
      "iteration 2521, loss: 0.012747390195727348\n",
      "iteration 2522, loss: 0.014786271378397942\n",
      "iteration 2523, loss: 0.014061669819056988\n",
      "iteration 2524, loss: 0.014089325442910194\n",
      "iteration 2525, loss: 0.013566031120717525\n",
      "iteration 2526, loss: 0.01402880996465683\n",
      "iteration 2527, loss: 0.014973924495279789\n",
      "iteration 2528, loss: 0.014295860193669796\n",
      "iteration 2529, loss: 0.013735175132751465\n",
      "iteration 2530, loss: 0.014687933959066868\n",
      "iteration 2531, loss: 0.014217555522918701\n",
      "iteration 2532, loss: 0.01619207113981247\n",
      "iteration 2533, loss: 0.014021527022123337\n",
      "iteration 2534, loss: 0.014421219937503338\n",
      "iteration 2535, loss: 0.016120390966534615\n",
      "iteration 2536, loss: 0.014531668275594711\n",
      "iteration 2537, loss: 0.013968038372695446\n",
      "iteration 2538, loss: 0.014640999026596546\n",
      "iteration 2539, loss: 0.01573621854186058\n",
      "iteration 2540, loss: 0.01658780872821808\n",
      "iteration 2541, loss: 0.012021650560200214\n",
      "iteration 2542, loss: 0.015153768472373486\n",
      "iteration 2543, loss: 0.013903443701565266\n",
      "iteration 2544, loss: 0.014326030388474464\n",
      "iteration 2545, loss: 0.014614850282669067\n",
      "iteration 2546, loss: 0.011352474801242352\n",
      "iteration 2547, loss: 0.01605667546391487\n",
      "iteration 2548, loss: 0.015305278822779655\n",
      "iteration 2549, loss: 0.013966036960482597\n",
      "iteration 2550, loss: 0.015166239812970161\n",
      "iteration 2551, loss: 0.014062260277569294\n",
      "iteration 2552, loss: 0.01292220875620842\n",
      "iteration 2553, loss: 0.014693282544612885\n",
      "iteration 2554, loss: 0.014259569346904755\n",
      "iteration 2555, loss: 0.014032834209501743\n",
      "iteration 2556, loss: 0.014611161313951015\n",
      "iteration 2557, loss: 0.013951441273093224\n",
      "iteration 2558, loss: 0.015709087252616882\n",
      "iteration 2559, loss: 0.012799199670553207\n",
      "iteration 2560, loss: 0.014405488967895508\n",
      "iteration 2561, loss: 0.012454363517463207\n",
      "iteration 2562, loss: 0.014856847934424877\n",
      "iteration 2563, loss: 0.013812622055411339\n",
      "iteration 2564, loss: 0.013268491253256798\n",
      "iteration 2565, loss: 0.014823861420154572\n",
      "iteration 2566, loss: 0.01241319440305233\n",
      "iteration 2567, loss: 0.014083399437367916\n",
      "iteration 2568, loss: 0.014431477524340153\n",
      "iteration 2569, loss: 0.014812815934419632\n",
      "iteration 2570, loss: 0.013229124248027802\n",
      "iteration 2571, loss: 0.012633072212338448\n",
      "iteration 2572, loss: 0.015241206623613834\n",
      "iteration 2573, loss: 0.014442166313529015\n",
      "iteration 2574, loss: 0.015161404386162758\n",
      "iteration 2575, loss: 0.017231237143278122\n",
      "iteration 2576, loss: 0.014151666313409805\n",
      "iteration 2577, loss: 0.015473180450499058\n",
      "iteration 2578, loss: 0.014399870298802853\n",
      "iteration 2579, loss: 0.011192882433533669\n",
      "iteration 2580, loss: 0.013389289379119873\n",
      "iteration 2581, loss: 0.014160615392029285\n",
      "iteration 2582, loss: 0.014512725174427032\n",
      "iteration 2583, loss: 0.013437223620712757\n",
      "iteration 2584, loss: 0.013406038284301758\n",
      "iteration 2585, loss: 0.014288365840911865\n",
      "iteration 2586, loss: 0.013059670105576515\n",
      "iteration 2587, loss: 0.01409057155251503\n",
      "iteration 2588, loss: 0.0165853388607502\n",
      "iteration 2589, loss: 0.014287509024143219\n",
      "iteration 2590, loss: 0.013971243984997272\n",
      "iteration 2591, loss: 0.017291612923145294\n",
      "iteration 2592, loss: 0.013603952713310719\n",
      "iteration 2593, loss: 0.015241664834320545\n",
      "iteration 2594, loss: 0.014242077246308327\n",
      "iteration 2595, loss: 0.013545380905270576\n",
      "iteration 2596, loss: 0.015568102709949017\n",
      "iteration 2597, loss: 0.01520473137497902\n",
      "iteration 2598, loss: 0.015403762459754944\n",
      "iteration 2599, loss: 0.013966917991638184\n",
      "iteration 2600, loss: 0.011628922075033188\n",
      "iteration 2601, loss: 0.013432832434773445\n",
      "iteration 2602, loss: 0.015210738405585289\n",
      "iteration 2603, loss: 0.01400667242705822\n",
      "iteration 2604, loss: 0.014771216548979282\n",
      "iteration 2605, loss: 0.013232776895165443\n",
      "iteration 2606, loss: 0.013581858947873116\n",
      "iteration 2607, loss: 0.01482363324612379\n",
      "iteration 2608, loss: 0.014760059304535389\n",
      "iteration 2609, loss: 0.013468378223478794\n",
      "iteration 2610, loss: 0.016689466312527657\n",
      "iteration 2611, loss: 0.015677353367209435\n",
      "iteration 2612, loss: 0.01556095015257597\n",
      "iteration 2613, loss: 0.015943443402647972\n",
      "iteration 2614, loss: 0.01198551245033741\n",
      "iteration 2615, loss: 0.012972237542271614\n",
      "iteration 2616, loss: 0.013586435467004776\n",
      "iteration 2617, loss: 0.014080207794904709\n",
      "iteration 2618, loss: 0.015783287584781647\n",
      "iteration 2619, loss: 0.013443885371088982\n",
      "iteration 2620, loss: 0.014343303628265858\n",
      "iteration 2621, loss: 0.0146395368501544\n",
      "iteration 2622, loss: 0.013117323629558086\n",
      "iteration 2623, loss: 0.016101814806461334\n",
      "iteration 2624, loss: 0.014708036556839943\n",
      "iteration 2625, loss: 0.01448140200227499\n",
      "iteration 2626, loss: 0.01566455140709877\n",
      "iteration 2627, loss: 0.014168191701173782\n",
      "iteration 2628, loss: 0.013629268854856491\n",
      "iteration 2629, loss: 0.012264050543308258\n",
      "iteration 2630, loss: 0.01402710098773241\n",
      "iteration 2631, loss: 0.01261858455836773\n",
      "iteration 2632, loss: 0.014678539708256721\n",
      "iteration 2633, loss: 0.013282734900712967\n",
      "iteration 2634, loss: 0.01242646761238575\n",
      "iteration 2635, loss: 0.01453247107565403\n",
      "iteration 2636, loss: 0.011552846059203148\n",
      "iteration 2637, loss: 0.013461902737617493\n",
      "iteration 2638, loss: 0.012188799679279327\n",
      "iteration 2639, loss: 0.01322132721543312\n",
      "iteration 2640, loss: 0.013780921697616577\n",
      "iteration 2641, loss: 0.015145521610975266\n",
      "iteration 2642, loss: 0.013988358899950981\n",
      "iteration 2643, loss: 0.013147476129233837\n",
      "iteration 2644, loss: 0.013236950151622295\n",
      "iteration 2645, loss: 0.014183325693011284\n",
      "iteration 2646, loss: 0.014384889975190163\n",
      "iteration 2647, loss: 0.014018295332789421\n",
      "iteration 2648, loss: 0.013232296332716942\n",
      "iteration 2649, loss: 0.01416161097586155\n",
      "iteration 2650, loss: 0.013208620250225067\n",
      "iteration 2651, loss: 0.014901416376233101\n",
      "iteration 2652, loss: 0.013537587597966194\n",
      "iteration 2653, loss: 0.013533209450542927\n",
      "iteration 2654, loss: 0.012319464236497879\n",
      "iteration 2655, loss: 0.013645182363688946\n",
      "iteration 2656, loss: 0.013657892122864723\n",
      "iteration 2657, loss: 0.01384616270661354\n",
      "iteration 2658, loss: 0.014930210076272488\n",
      "iteration 2659, loss: 0.014276172965765\n",
      "iteration 2660, loss: 0.014533201232552528\n",
      "iteration 2661, loss: 0.012209270149469376\n",
      "iteration 2662, loss: 0.011212198995053768\n",
      "iteration 2663, loss: 0.01398388296365738\n",
      "iteration 2664, loss: 0.01361686922609806\n",
      "iteration 2665, loss: 0.01630287803709507\n",
      "iteration 2666, loss: 0.014991220086812973\n",
      "iteration 2667, loss: 0.013362562283873558\n",
      "iteration 2668, loss: 0.012536260299384594\n",
      "iteration 2669, loss: 0.014214432798326015\n",
      "iteration 2670, loss: 0.015251124277710915\n",
      "iteration 2671, loss: 0.016922252252697945\n",
      "iteration 2672, loss: 0.013280495069921017\n",
      "iteration 2673, loss: 0.012559306807816029\n",
      "iteration 2674, loss: 0.013618839904665947\n",
      "iteration 2675, loss: 0.012767542153596878\n",
      "iteration 2676, loss: 0.014260709285736084\n",
      "iteration 2677, loss: 0.014793476089835167\n",
      "iteration 2678, loss: 0.013313015922904015\n",
      "iteration 2679, loss: 0.01598244719207287\n",
      "iteration 2680, loss: 0.014065580442547798\n",
      "iteration 2681, loss: 0.012904438190162182\n",
      "iteration 2682, loss: 0.01212814450263977\n",
      "iteration 2683, loss: 0.01365220732986927\n",
      "iteration 2684, loss: 0.014700914733111858\n",
      "iteration 2685, loss: 0.014627572149038315\n",
      "iteration 2686, loss: 0.015975333750247955\n",
      "iteration 2687, loss: 0.013318769633769989\n",
      "iteration 2688, loss: 0.015323063358664513\n",
      "iteration 2689, loss: 0.01741013675928116\n",
      "iteration 2690, loss: 0.015125320293009281\n",
      "iteration 2691, loss: 0.012388037517666817\n",
      "iteration 2692, loss: 0.015104461461305618\n",
      "iteration 2693, loss: 0.012800417840480804\n",
      "iteration 2694, loss: 0.01454844418913126\n",
      "iteration 2695, loss: 0.012370983138680458\n",
      "iteration 2696, loss: 0.013171613216400146\n",
      "iteration 2697, loss: 0.013564176857471466\n",
      "iteration 2698, loss: 0.01332925260066986\n",
      "iteration 2699, loss: 0.013831015676259995\n",
      "iteration 2700, loss: 0.014474140480160713\n",
      "iteration 2701, loss: 0.01342238299548626\n",
      "iteration 2702, loss: 0.013337945565581322\n",
      "iteration 2703, loss: 0.012880580499768257\n",
      "iteration 2704, loss: 0.014254561625421047\n",
      "iteration 2705, loss: 0.014024514704942703\n",
      "iteration 2706, loss: 0.015466587617993355\n",
      "iteration 2707, loss: 0.01325394120067358\n",
      "iteration 2708, loss: 0.01298312097787857\n",
      "iteration 2709, loss: 0.014835135079920292\n",
      "iteration 2710, loss: 0.013948721811175346\n",
      "iteration 2711, loss: 0.013819620013237\n",
      "iteration 2712, loss: 0.014489894732832909\n",
      "iteration 2713, loss: 0.013083044439554214\n",
      "iteration 2714, loss: 0.01250564306974411\n",
      "iteration 2715, loss: 0.014279221184551716\n",
      "iteration 2716, loss: 0.013932758942246437\n",
      "iteration 2717, loss: 0.015474334359169006\n",
      "iteration 2718, loss: 0.014479092322289944\n",
      "iteration 2719, loss: 0.014428996481001377\n",
      "iteration 2720, loss: 0.01549834106117487\n",
      "iteration 2721, loss: 0.012090859934687614\n",
      "iteration 2722, loss: 0.012001409195363522\n",
      "iteration 2723, loss: 0.013692516833543777\n",
      "iteration 2724, loss: 0.012967739254236221\n",
      "iteration 2725, loss: 0.012815361842513084\n",
      "iteration 2726, loss: 0.014015108346939087\n",
      "iteration 2727, loss: 0.013495653867721558\n",
      "iteration 2728, loss: 0.01613982766866684\n",
      "iteration 2729, loss: 0.014957806095480919\n",
      "iteration 2730, loss: 0.012923909351229668\n",
      "iteration 2731, loss: 0.014931228011846542\n",
      "iteration 2732, loss: 0.014165702275931835\n",
      "iteration 2733, loss: 0.013932416215538979\n",
      "iteration 2734, loss: 0.01306731253862381\n",
      "iteration 2735, loss: 0.013359885662794113\n",
      "iteration 2736, loss: 0.014366621151566505\n",
      "iteration 2737, loss: 0.012770824134349823\n",
      "iteration 2738, loss: 0.015054302290081978\n",
      "iteration 2739, loss: 0.014424191787838936\n",
      "iteration 2740, loss: 0.014274196699261665\n",
      "iteration 2741, loss: 0.01388637162744999\n",
      "iteration 2742, loss: 0.012859834358096123\n",
      "iteration 2743, loss: 0.014714952558279037\n",
      "iteration 2744, loss: 0.013074202463030815\n",
      "iteration 2745, loss: 0.01523447036743164\n",
      "iteration 2746, loss: 0.015320935286581516\n",
      "iteration 2747, loss: 0.014142423868179321\n",
      "iteration 2748, loss: 0.014447205699980259\n",
      "iteration 2749, loss: 0.01280093565583229\n",
      "iteration 2750, loss: 0.012280075810849667\n",
      "iteration 2751, loss: 0.015498308464884758\n",
      "iteration 2752, loss: 0.013707112520933151\n",
      "iteration 2753, loss: 0.01313606183975935\n",
      "iteration 2754, loss: 0.0134639423340559\n",
      "iteration 2755, loss: 0.013940227217972279\n",
      "iteration 2756, loss: 0.011571958661079407\n",
      "iteration 2757, loss: 0.012456395663321018\n",
      "iteration 2758, loss: 0.012920578941702843\n",
      "iteration 2759, loss: 0.013516973704099655\n",
      "iteration 2760, loss: 0.015157017856836319\n",
      "iteration 2761, loss: 0.012652059085667133\n",
      "iteration 2762, loss: 0.01438379567116499\n",
      "iteration 2763, loss: 0.014317158609628677\n",
      "iteration 2764, loss: 0.015508132055401802\n",
      "iteration 2765, loss: 0.013731587678194046\n",
      "iteration 2766, loss: 0.014492494985461235\n",
      "iteration 2767, loss: 0.013167324475944042\n",
      "iteration 2768, loss: 0.012559575960040092\n",
      "iteration 2769, loss: 0.01309913769364357\n",
      "iteration 2770, loss: 0.014144307933747768\n",
      "iteration 2771, loss: 0.014201860874891281\n",
      "iteration 2772, loss: 0.013650313019752502\n",
      "iteration 2773, loss: 0.014367124065756798\n",
      "iteration 2774, loss: 0.012627167627215385\n",
      "iteration 2775, loss: 0.012513769790530205\n",
      "iteration 2776, loss: 0.012694060802459717\n",
      "iteration 2777, loss: 0.015015942044556141\n",
      "iteration 2778, loss: 0.013349482789635658\n",
      "iteration 2779, loss: 0.013566872105002403\n",
      "iteration 2780, loss: 0.014114730060100555\n",
      "iteration 2781, loss: 0.014274410903453827\n",
      "iteration 2782, loss: 0.012626204639673233\n",
      "iteration 2783, loss: 0.012712150812149048\n",
      "iteration 2784, loss: 0.013920367695391178\n",
      "iteration 2785, loss: 0.012701418250799179\n",
      "iteration 2786, loss: 0.01210842002183199\n",
      "iteration 2787, loss: 0.013081667944788933\n",
      "iteration 2788, loss: 0.012545262463390827\n",
      "iteration 2789, loss: 0.01297805830836296\n",
      "iteration 2790, loss: 0.01375495083630085\n",
      "iteration 2791, loss: 0.011380180716514587\n",
      "iteration 2792, loss: 0.015867557376623154\n",
      "iteration 2793, loss: 0.01328190229833126\n",
      "iteration 2794, loss: 0.013880662620067596\n",
      "iteration 2795, loss: 0.014075400307774544\n",
      "iteration 2796, loss: 0.014384899288415909\n",
      "iteration 2797, loss: 0.015030574053525925\n",
      "iteration 2798, loss: 0.013562457635998726\n",
      "iteration 2799, loss: 0.014685800299048424\n",
      "iteration 2800, loss: 0.016383197158575058\n",
      "iteration 2801, loss: 0.014442574232816696\n",
      "iteration 2802, loss: 0.01444865483790636\n",
      "iteration 2803, loss: 0.014792571775615215\n",
      "iteration 2804, loss: 0.015520493499934673\n",
      "iteration 2805, loss: 0.012735940515995026\n",
      "iteration 2806, loss: 0.014899944886565208\n",
      "iteration 2807, loss: 0.0134770842269063\n",
      "iteration 2808, loss: 0.013398224487900734\n",
      "iteration 2809, loss: 0.015444735996425152\n",
      "iteration 2810, loss: 0.012814771384000778\n",
      "iteration 2811, loss: 0.013666853308677673\n",
      "iteration 2812, loss: 0.015011599287390709\n",
      "iteration 2813, loss: 0.01149856485426426\n",
      "iteration 2814, loss: 0.015752483159303665\n",
      "iteration 2815, loss: 0.014271244406700134\n",
      "iteration 2816, loss: 0.014174041338264942\n",
      "iteration 2817, loss: 0.014156864956021309\n",
      "iteration 2818, loss: 0.013042313978075981\n",
      "iteration 2819, loss: 0.01261388510465622\n",
      "iteration 2820, loss: 0.014606768265366554\n",
      "iteration 2821, loss: 0.01190580427646637\n",
      "iteration 2822, loss: 0.012711243703961372\n",
      "iteration 2823, loss: 0.014422796666622162\n",
      "iteration 2824, loss: 0.013710143975913525\n",
      "iteration 2825, loss: 0.012747918255627155\n",
      "iteration 2826, loss: 0.012445095926523209\n",
      "iteration 2827, loss: 0.011756046675145626\n",
      "iteration 2828, loss: 0.011858263984322548\n",
      "iteration 2829, loss: 0.011109830811619759\n",
      "iteration 2830, loss: 0.013797430321574211\n",
      "iteration 2831, loss: 0.011932870373129845\n",
      "iteration 2832, loss: 0.013783971779048443\n",
      "iteration 2833, loss: 0.014077781699597836\n",
      "iteration 2834, loss: 0.012419618666172028\n",
      "iteration 2835, loss: 0.013501178473234177\n",
      "iteration 2836, loss: 0.012933497317135334\n",
      "iteration 2837, loss: 0.012837068177759647\n",
      "iteration 2838, loss: 0.01209123246371746\n",
      "iteration 2839, loss: 0.01374773308634758\n",
      "iteration 2840, loss: 0.011637643910944462\n",
      "iteration 2841, loss: 0.014287617057561874\n",
      "iteration 2842, loss: 0.011693553999066353\n",
      "iteration 2843, loss: 0.011632388457655907\n",
      "iteration 2844, loss: 0.01312110386788845\n",
      "iteration 2845, loss: 0.012093287892639637\n",
      "iteration 2846, loss: 0.012842155992984772\n",
      "iteration 2847, loss: 0.013878226280212402\n",
      "iteration 2848, loss: 0.012324094772338867\n",
      "iteration 2849, loss: 0.012979291379451752\n",
      "iteration 2850, loss: 0.012425431050360203\n",
      "iteration 2851, loss: 0.014628694392740726\n",
      "iteration 2852, loss: 0.011611117981374264\n",
      "iteration 2853, loss: 0.012044988572597504\n",
      "iteration 2854, loss: 0.013107780367136002\n",
      "iteration 2855, loss: 0.012812018394470215\n",
      "iteration 2856, loss: 0.013775601983070374\n",
      "iteration 2857, loss: 0.013653881847858429\n",
      "iteration 2858, loss: 0.010845078155398369\n",
      "iteration 2859, loss: 0.013098862953484058\n",
      "iteration 2860, loss: 0.013036953285336494\n",
      "iteration 2861, loss: 0.013807853683829308\n",
      "iteration 2862, loss: 0.012901073321700096\n",
      "iteration 2863, loss: 0.012746020220220089\n",
      "iteration 2864, loss: 0.011962737888097763\n",
      "iteration 2865, loss: 0.0112433061003685\n",
      "iteration 2866, loss: 0.012395289726555347\n",
      "iteration 2867, loss: 0.01325184665620327\n",
      "iteration 2868, loss: 0.012315908446907997\n",
      "iteration 2869, loss: 0.01282254233956337\n",
      "iteration 2870, loss: 0.012786990962922573\n",
      "iteration 2871, loss: 0.01212989166378975\n",
      "iteration 2872, loss: 0.013351562432944775\n",
      "iteration 2873, loss: 0.012434098869562149\n",
      "iteration 2874, loss: 0.014398060739040375\n",
      "iteration 2875, loss: 0.01140864472836256\n",
      "iteration 2876, loss: 0.014281428419053555\n",
      "iteration 2877, loss: 0.012512844055891037\n",
      "iteration 2878, loss: 0.012180091813206673\n",
      "iteration 2879, loss: 0.014225540682673454\n",
      "iteration 2880, loss: 0.014338801614940166\n",
      "iteration 2881, loss: 0.011504815891385078\n",
      "iteration 2882, loss: 0.011030239053070545\n",
      "iteration 2883, loss: 0.015906507149338722\n",
      "iteration 2884, loss: 0.01232517696917057\n",
      "iteration 2885, loss: 0.014109350740909576\n",
      "iteration 2886, loss: 0.012536141090095043\n",
      "iteration 2887, loss: 0.012780405580997467\n",
      "iteration 2888, loss: 0.013455015607178211\n",
      "iteration 2889, loss: 0.011088727973401546\n",
      "iteration 2890, loss: 0.01407651323825121\n",
      "iteration 2891, loss: 0.013172883540391922\n",
      "iteration 2892, loss: 0.01303810067474842\n",
      "iteration 2893, loss: 0.01239481195807457\n",
      "iteration 2894, loss: 0.012226246297359467\n",
      "iteration 2895, loss: 0.014890278689563274\n",
      "iteration 2896, loss: 0.01021647173911333\n",
      "iteration 2897, loss: 0.013635387644171715\n",
      "iteration 2898, loss: 0.015127105638384819\n",
      "iteration 2899, loss: 0.012564030475914478\n",
      "iteration 2900, loss: 0.013548826798796654\n",
      "iteration 2901, loss: 0.013005081564188004\n",
      "iteration 2902, loss: 0.01468714326620102\n",
      "iteration 2903, loss: 0.01336803287267685\n",
      "iteration 2904, loss: 0.011599087156355381\n",
      "iteration 2905, loss: 0.01316293515264988\n",
      "iteration 2906, loss: 0.015081645920872688\n",
      "iteration 2907, loss: 0.012584908865392208\n",
      "iteration 2908, loss: 0.01385716162621975\n",
      "iteration 2909, loss: 0.012291820719838142\n",
      "iteration 2910, loss: 0.014450390823185444\n",
      "iteration 2911, loss: 0.01321513019502163\n",
      "iteration 2912, loss: 0.012864158488810062\n",
      "iteration 2913, loss: 0.013670304790139198\n",
      "iteration 2914, loss: 0.013813687488436699\n",
      "iteration 2915, loss: 0.012624627910554409\n",
      "iteration 2916, loss: 0.014899884350597858\n",
      "iteration 2917, loss: 0.014063358306884766\n",
      "iteration 2918, loss: 0.014124088920652866\n",
      "iteration 2919, loss: 0.012341802008450031\n",
      "iteration 2920, loss: 0.01170423999428749\n",
      "iteration 2921, loss: 0.013924075290560722\n",
      "iteration 2922, loss: 0.01168081909418106\n",
      "iteration 2923, loss: 0.012934784404933453\n",
      "iteration 2924, loss: 0.0118181761354208\n",
      "iteration 2925, loss: 0.012490605935454369\n",
      "iteration 2926, loss: 0.01355477049946785\n",
      "iteration 2927, loss: 0.013386638835072517\n",
      "iteration 2928, loss: 0.01334875077009201\n",
      "iteration 2929, loss: 0.011679467745125294\n",
      "iteration 2930, loss: 0.011603066697716713\n",
      "iteration 2931, loss: 0.010844158940017223\n",
      "iteration 2932, loss: 0.012671329081058502\n",
      "iteration 2933, loss: 0.012845663353800774\n",
      "iteration 2934, loss: 0.014246889390051365\n",
      "iteration 2935, loss: 0.012576768174767494\n",
      "iteration 2936, loss: 0.01266251876950264\n",
      "iteration 2937, loss: 0.015142805874347687\n",
      "iteration 2938, loss: 0.012130809016525745\n",
      "iteration 2939, loss: 0.014045309275388718\n",
      "iteration 2940, loss: 0.012092938646674156\n",
      "iteration 2941, loss: 0.013578415848314762\n",
      "iteration 2942, loss: 0.012761075049638748\n",
      "iteration 2943, loss: 0.013803930021822453\n",
      "iteration 2944, loss: 0.011773032136261463\n",
      "iteration 2945, loss: 0.0113694928586483\n",
      "iteration 2946, loss: 0.012368281371891499\n",
      "iteration 2947, loss: 0.01264915056526661\n",
      "iteration 2948, loss: 0.013536101207137108\n",
      "iteration 2949, loss: 0.012260072864592075\n",
      "iteration 2950, loss: 0.01349688321352005\n",
      "iteration 2951, loss: 0.012993658892810345\n",
      "iteration 2952, loss: 0.013104450888931751\n",
      "iteration 2953, loss: 0.015959158539772034\n",
      "iteration 2954, loss: 0.012649653479456902\n",
      "iteration 2955, loss: 0.012982049956917763\n",
      "iteration 2956, loss: 0.011932075954973698\n",
      "iteration 2957, loss: 0.01381600834429264\n",
      "iteration 2958, loss: 0.013048691675066948\n",
      "iteration 2959, loss: 0.012756905518472195\n",
      "iteration 2960, loss: 0.012245283462107182\n",
      "iteration 2961, loss: 0.014188004657626152\n",
      "iteration 2962, loss: 0.013180464506149292\n",
      "iteration 2963, loss: 0.012698248028755188\n",
      "iteration 2964, loss: 0.012993544340133667\n",
      "iteration 2965, loss: 0.011592581868171692\n",
      "iteration 2966, loss: 0.013407624326646328\n",
      "iteration 2967, loss: 0.012881193310022354\n",
      "iteration 2968, loss: 0.013326087966561317\n",
      "iteration 2969, loss: 0.01392209529876709\n",
      "iteration 2970, loss: 0.014420628547668457\n",
      "iteration 2971, loss: 0.010498189367353916\n",
      "iteration 2972, loss: 0.013930149376392365\n",
      "iteration 2973, loss: 0.011778689920902252\n",
      "iteration 2974, loss: 0.01157081127166748\n",
      "iteration 2975, loss: 0.01396282110363245\n",
      "iteration 2976, loss: 0.013556722551584244\n",
      "iteration 2977, loss: 0.012231716886162758\n",
      "iteration 2978, loss: 0.011770243756473064\n",
      "iteration 2979, loss: 0.015711961314082146\n",
      "iteration 2980, loss: 0.012996400706470013\n",
      "iteration 2981, loss: 0.013305513188242912\n",
      "iteration 2982, loss: 0.011848445981740952\n",
      "iteration 2983, loss: 0.012758512049913406\n",
      "iteration 2984, loss: 0.010529880411922932\n",
      "iteration 2985, loss: 0.01312174927443266\n",
      "iteration 2986, loss: 0.012281326577067375\n",
      "iteration 2987, loss: 0.013810798525810242\n",
      "iteration 2988, loss: 0.012733717449009418\n",
      "iteration 2989, loss: 0.011959383264183998\n",
      "iteration 2990, loss: 0.012866634875535965\n",
      "iteration 2991, loss: 0.01341429352760315\n",
      "iteration 2992, loss: 0.013482853770256042\n",
      "iteration 2993, loss: 0.013095361180603504\n",
      "iteration 2994, loss: 0.010281509719789028\n",
      "iteration 2995, loss: 0.0114653455093503\n",
      "iteration 2996, loss: 0.01351265236735344\n",
      "iteration 2997, loss: 0.01168748177587986\n",
      "iteration 2998, loss: 0.011142648756504059\n",
      "iteration 2999, loss: 0.012200698256492615\n",
      "iteration 3000, loss: 0.01085676159709692\n",
      "iteration 3001, loss: 0.011404883116483688\n",
      "iteration 3002, loss: 0.012029275298118591\n",
      "iteration 3003, loss: 0.01394396647810936\n",
      "iteration 3004, loss: 0.011767594143748283\n",
      "iteration 3005, loss: 0.011612819507718086\n",
      "iteration 3006, loss: 0.012366432696580887\n",
      "iteration 3007, loss: 0.012852981686592102\n",
      "iteration 3008, loss: 0.012728193774819374\n",
      "iteration 3009, loss: 0.011645725928246975\n",
      "iteration 3010, loss: 0.01149735413491726\n",
      "iteration 3011, loss: 0.011872339993715286\n",
      "iteration 3012, loss: 0.013939404860138893\n",
      "iteration 3013, loss: 0.011817585676908493\n",
      "iteration 3014, loss: 0.012988229282200336\n",
      "iteration 3015, loss: 0.013197068125009537\n",
      "iteration 3016, loss: 0.015024405904114246\n",
      "iteration 3017, loss: 0.011793495155870914\n",
      "iteration 3018, loss: 0.012389956042170525\n",
      "iteration 3019, loss: 0.012140944600105286\n",
      "iteration 3020, loss: 0.012749266810715199\n",
      "iteration 3021, loss: 0.011035848408937454\n",
      "iteration 3022, loss: 0.013279831036925316\n",
      "iteration 3023, loss: 0.01406602282077074\n",
      "iteration 3024, loss: 0.012560144998133183\n",
      "iteration 3025, loss: 0.012157706543803215\n",
      "iteration 3026, loss: 0.012638308107852936\n",
      "iteration 3027, loss: 0.011891904287040234\n",
      "iteration 3028, loss: 0.013311098329722881\n",
      "iteration 3029, loss: 0.013134790584445\n",
      "iteration 3030, loss: 0.012993274256587029\n",
      "iteration 3031, loss: 0.012791271321475506\n",
      "iteration 3032, loss: 0.011464281007647514\n",
      "iteration 3033, loss: 0.01133444532752037\n",
      "iteration 3034, loss: 0.014998487196862698\n",
      "iteration 3035, loss: 0.012857113033533096\n",
      "iteration 3036, loss: 0.013236386701464653\n",
      "iteration 3037, loss: 0.012720085680484772\n",
      "iteration 3038, loss: 0.010938894003629684\n",
      "iteration 3039, loss: 0.015092562884092331\n",
      "iteration 3040, loss: 0.012882384471595287\n",
      "iteration 3041, loss: 0.011464111506938934\n",
      "iteration 3042, loss: 0.015607799403369427\n",
      "iteration 3043, loss: 0.012719043530523777\n",
      "iteration 3044, loss: 0.012263165786862373\n",
      "iteration 3045, loss: 0.013278895989060402\n",
      "iteration 3046, loss: 0.013196846470236778\n",
      "iteration 3047, loss: 0.012119034305214882\n",
      "iteration 3048, loss: 0.012412996031343937\n",
      "iteration 3049, loss: 0.013460438698530197\n",
      "iteration 3050, loss: 0.0128787187859416\n",
      "iteration 3051, loss: 0.013509307987987995\n",
      "iteration 3052, loss: 0.010570933111011982\n",
      "iteration 3053, loss: 0.011141697876155376\n",
      "iteration 3054, loss: 0.012783806771039963\n",
      "iteration 3055, loss: 0.01192314550280571\n",
      "iteration 3056, loss: 0.013173161074519157\n",
      "iteration 3057, loss: 0.013724411837756634\n",
      "iteration 3058, loss: 0.01369386911392212\n",
      "iteration 3059, loss: 0.012823110446333885\n",
      "iteration 3060, loss: 0.013014361262321472\n",
      "iteration 3061, loss: 0.012739162892103195\n",
      "iteration 3062, loss: 0.012415394186973572\n",
      "iteration 3063, loss: 0.013948899693787098\n",
      "iteration 3064, loss: 0.010650347918272018\n",
      "iteration 3065, loss: 0.012771077454090118\n",
      "iteration 3066, loss: 0.012278244830667973\n",
      "iteration 3067, loss: 0.011261723935604095\n",
      "iteration 3068, loss: 0.012103661894798279\n",
      "iteration 3069, loss: 0.011946573853492737\n",
      "iteration 3070, loss: 0.012093596160411835\n",
      "iteration 3071, loss: 0.013363940641283989\n",
      "iteration 3072, loss: 0.011940076015889645\n",
      "iteration 3073, loss: 0.013490371406078339\n",
      "iteration 3074, loss: 0.013292280957102776\n",
      "iteration 3075, loss: 0.012235332280397415\n",
      "iteration 3076, loss: 0.010409274138510227\n",
      "iteration 3077, loss: 0.012126591056585312\n",
      "iteration 3078, loss: 0.013404710218310356\n",
      "iteration 3079, loss: 0.011426346376538277\n",
      "iteration 3080, loss: 0.014039568603038788\n",
      "iteration 3081, loss: 0.01254574116319418\n",
      "iteration 3082, loss: 0.012255624867975712\n",
      "iteration 3083, loss: 0.012764918617904186\n",
      "iteration 3084, loss: 0.012939347885549068\n",
      "iteration 3085, loss: 0.013247914612293243\n",
      "iteration 3086, loss: 0.013200677931308746\n",
      "iteration 3087, loss: 0.01325477659702301\n",
      "iteration 3088, loss: 0.011958656832575798\n",
      "iteration 3089, loss: 0.011371325701475143\n",
      "iteration 3090, loss: 0.012073231860995293\n",
      "iteration 3091, loss: 0.012068500742316246\n",
      "iteration 3092, loss: 0.012157646007835865\n",
      "iteration 3093, loss: 0.011420984752476215\n",
      "iteration 3094, loss: 0.012496321462094784\n",
      "iteration 3095, loss: 0.01235220953822136\n",
      "iteration 3096, loss: 0.009959165006875992\n",
      "iteration 3097, loss: 0.011637760326266289\n",
      "iteration 3098, loss: 0.012344921007752419\n",
      "iteration 3099, loss: 0.011152945458889008\n",
      "iteration 3100, loss: 0.012214496731758118\n",
      "iteration 3101, loss: 0.012344429269433022\n",
      "iteration 3102, loss: 0.012651363387703896\n",
      "iteration 3103, loss: 0.012910930439829826\n",
      "iteration 3104, loss: 0.012578854337334633\n",
      "iteration 3105, loss: 0.01286713033914566\n",
      "iteration 3106, loss: 0.011829221621155739\n",
      "iteration 3107, loss: 0.012156864628195763\n",
      "iteration 3108, loss: 0.01275862380862236\n",
      "iteration 3109, loss: 0.013409709557890892\n",
      "iteration 3110, loss: 0.014283508062362671\n",
      "iteration 3111, loss: 0.01131337508559227\n",
      "iteration 3112, loss: 0.011540818959474564\n",
      "iteration 3113, loss: 0.013229102827608585\n",
      "iteration 3114, loss: 0.014043178409337997\n",
      "iteration 3115, loss: 0.011827262118458748\n",
      "iteration 3116, loss: 0.011741643771529198\n",
      "iteration 3117, loss: 0.01295473799109459\n",
      "iteration 3118, loss: 0.011301813647150993\n",
      "iteration 3119, loss: 0.01246023178100586\n",
      "iteration 3120, loss: 0.010217921808362007\n",
      "iteration 3121, loss: 0.011794066056609154\n",
      "iteration 3122, loss: 0.01280638761818409\n",
      "iteration 3123, loss: 0.013050669804215431\n",
      "iteration 3124, loss: 0.011675162240862846\n",
      "iteration 3125, loss: 0.011547770351171494\n",
      "iteration 3126, loss: 0.011374483816325665\n",
      "iteration 3127, loss: 0.011441471055150032\n",
      "iteration 3128, loss: 0.011043043807148933\n",
      "iteration 3129, loss: 0.010917911306023598\n",
      "iteration 3130, loss: 0.011057956144213676\n",
      "iteration 3131, loss: 0.010800345800817013\n",
      "iteration 3132, loss: 0.0115390345454216\n",
      "iteration 3133, loss: 0.011564565822482109\n",
      "iteration 3134, loss: 0.012929492630064487\n",
      "iteration 3135, loss: 0.011815578676760197\n",
      "iteration 3136, loss: 0.01151750236749649\n",
      "iteration 3137, loss: 0.011684807017445564\n",
      "iteration 3138, loss: 0.011549100279808044\n",
      "iteration 3139, loss: 0.011605454608798027\n",
      "iteration 3140, loss: 0.011758961714804173\n",
      "iteration 3141, loss: 0.01171572133898735\n",
      "iteration 3142, loss: 0.013721397146582603\n",
      "iteration 3143, loss: 0.011956373229622841\n",
      "iteration 3144, loss: 0.012949313037097454\n",
      "iteration 3145, loss: 0.014467233791947365\n",
      "iteration 3146, loss: 0.011249641887843609\n",
      "iteration 3147, loss: 0.012861490249633789\n",
      "iteration 3148, loss: 0.01058704312890768\n",
      "iteration 3149, loss: 0.012432988733053207\n",
      "iteration 3150, loss: 0.013265690766274929\n",
      "iteration 3151, loss: 0.01280038058757782\n",
      "iteration 3152, loss: 0.012091143988072872\n",
      "iteration 3153, loss: 0.013410743325948715\n",
      "iteration 3154, loss: 0.01288266945630312\n",
      "iteration 3155, loss: 0.014033762738108635\n",
      "iteration 3156, loss: 0.013697834685444832\n",
      "iteration 3157, loss: 0.011138463392853737\n",
      "iteration 3158, loss: 0.01403418742120266\n",
      "iteration 3159, loss: 0.012746749445796013\n",
      "iteration 3160, loss: 0.011405709199607372\n",
      "iteration 3161, loss: 0.01275729387998581\n",
      "iteration 3162, loss: 0.013906901702284813\n",
      "iteration 3163, loss: 0.010359609499573708\n",
      "iteration 3164, loss: 0.011240962892770767\n",
      "iteration 3165, loss: 0.013227429240942001\n",
      "iteration 3166, loss: 0.01170443370938301\n",
      "iteration 3167, loss: 0.011564698070287704\n",
      "iteration 3168, loss: 0.012065854854881763\n",
      "iteration 3169, loss: 0.011788875795900822\n",
      "iteration 3170, loss: 0.013316133990883827\n",
      "iteration 3171, loss: 0.011244811117649078\n",
      "iteration 3172, loss: 0.012376556172966957\n",
      "iteration 3173, loss: 0.011443990282714367\n",
      "iteration 3174, loss: 0.010769953951239586\n",
      "iteration 3175, loss: 0.012793973088264465\n",
      "iteration 3176, loss: 0.012944987043738365\n",
      "iteration 3177, loss: 0.01018020324409008\n",
      "iteration 3178, loss: 0.014083999209105968\n",
      "iteration 3179, loss: 0.012692730873823166\n",
      "iteration 3180, loss: 0.010736788623034954\n",
      "iteration 3181, loss: 0.011380795389413834\n",
      "iteration 3182, loss: 0.01201177854090929\n",
      "iteration 3183, loss: 0.012306095100939274\n",
      "iteration 3184, loss: 0.014014998450875282\n",
      "iteration 3185, loss: 0.01096302829682827\n",
      "iteration 3186, loss: 0.013399602845311165\n",
      "iteration 3187, loss: 0.014003373682498932\n",
      "iteration 3188, loss: 0.011185954324901104\n",
      "iteration 3189, loss: 0.012590443715453148\n",
      "iteration 3190, loss: 0.012269622646272182\n",
      "iteration 3191, loss: 0.012111268937587738\n",
      "iteration 3192, loss: 0.011388823390007019\n",
      "iteration 3193, loss: 0.011094888672232628\n",
      "iteration 3194, loss: 0.012605947442352772\n",
      "iteration 3195, loss: 0.01282636821269989\n",
      "iteration 3196, loss: 0.012296538800001144\n",
      "iteration 3197, loss: 0.010883752256631851\n",
      "iteration 3198, loss: 0.010289974510669708\n",
      "iteration 3199, loss: 0.01254275906831026\n",
      "iteration 3200, loss: 0.012333734892308712\n",
      "iteration 3201, loss: 0.01082670595496893\n",
      "iteration 3202, loss: 0.01112053170800209\n",
      "iteration 3203, loss: 0.012491907924413681\n",
      "iteration 3204, loss: 0.012202832847833633\n",
      "iteration 3205, loss: 0.011773167178034782\n",
      "iteration 3206, loss: 0.011032657697796822\n",
      "iteration 3207, loss: 0.010978497564792633\n",
      "iteration 3208, loss: 0.010500149801373482\n",
      "iteration 3209, loss: 0.011211149394512177\n",
      "iteration 3210, loss: 0.011712899431586266\n",
      "iteration 3211, loss: 0.010863132774829865\n",
      "iteration 3212, loss: 0.011183146387338638\n",
      "iteration 3213, loss: 0.011953071691095829\n",
      "iteration 3214, loss: 0.012409649789333344\n",
      "iteration 3215, loss: 0.012597039341926575\n",
      "iteration 3216, loss: 0.012574388645589352\n",
      "iteration 3217, loss: 0.011576454155147076\n",
      "iteration 3218, loss: 0.012238143011927605\n",
      "iteration 3219, loss: 0.012409485876560211\n",
      "iteration 3220, loss: 0.011744127608835697\n",
      "iteration 3221, loss: 0.012149078771471977\n",
      "iteration 3222, loss: 0.011346773244440556\n",
      "iteration 3223, loss: 0.01150057464838028\n",
      "iteration 3224, loss: 0.011777946725487709\n",
      "iteration 3225, loss: 0.011630836874246597\n",
      "iteration 3226, loss: 0.012939129024744034\n",
      "iteration 3227, loss: 0.012316588312387466\n",
      "iteration 3228, loss: 0.010978629812598228\n",
      "iteration 3229, loss: 0.013035302050411701\n",
      "iteration 3230, loss: 0.012284358032047749\n",
      "iteration 3231, loss: 0.013830432668328285\n",
      "iteration 3232, loss: 0.01205999031662941\n",
      "iteration 3233, loss: 0.011071794666349888\n",
      "iteration 3234, loss: 0.011420778930187225\n",
      "iteration 3235, loss: 0.013700579293072224\n",
      "iteration 3236, loss: 0.012465647421777248\n",
      "iteration 3237, loss: 0.011845462024211884\n",
      "iteration 3238, loss: 0.01099743228405714\n",
      "iteration 3239, loss: 0.011685177683830261\n",
      "iteration 3240, loss: 0.011368815787136555\n",
      "iteration 3241, loss: 0.011494845151901245\n",
      "iteration 3242, loss: 0.011018102988600731\n",
      "iteration 3243, loss: 0.010652946308255196\n",
      "iteration 3244, loss: 0.01146023627370596\n",
      "iteration 3245, loss: 0.011162187904119492\n",
      "iteration 3246, loss: 0.011497296392917633\n",
      "iteration 3247, loss: 0.010152741335332394\n",
      "iteration 3248, loss: 0.011015419848263264\n",
      "iteration 3249, loss: 0.012207547202706337\n",
      "iteration 3250, loss: 0.013253822922706604\n",
      "iteration 3251, loss: 0.013169454410672188\n",
      "iteration 3252, loss: 0.01320688333362341\n",
      "iteration 3253, loss: 0.011387663893401623\n",
      "iteration 3254, loss: 0.01226484403014183\n",
      "iteration 3255, loss: 0.01009360421448946\n",
      "iteration 3256, loss: 0.010893391445279121\n",
      "iteration 3257, loss: 0.012691935524344444\n",
      "iteration 3258, loss: 0.011515539139509201\n",
      "iteration 3259, loss: 0.013240092433989048\n",
      "iteration 3260, loss: 0.011259211227297783\n",
      "iteration 3261, loss: 0.011924926191568375\n",
      "iteration 3262, loss: 0.011875605210661888\n",
      "iteration 3263, loss: 0.010918773710727692\n",
      "iteration 3264, loss: 0.010530437342822552\n",
      "iteration 3265, loss: 0.009988646022975445\n",
      "iteration 3266, loss: 0.011949519626796246\n",
      "iteration 3267, loss: 0.012823695316910744\n",
      "iteration 3268, loss: 0.012629995122551918\n",
      "iteration 3269, loss: 0.009728113189339638\n",
      "iteration 3270, loss: 0.010645884089171886\n",
      "iteration 3271, loss: 0.012856965884566307\n",
      "iteration 3272, loss: 0.01084431353956461\n",
      "iteration 3273, loss: 0.01129689160734415\n",
      "iteration 3274, loss: 0.011833878234028816\n",
      "iteration 3275, loss: 0.01200133003294468\n",
      "iteration 3276, loss: 0.010328426025807858\n",
      "iteration 3277, loss: 0.014683853834867477\n",
      "iteration 3278, loss: 0.012453439645469189\n",
      "iteration 3279, loss: 0.01045737974345684\n",
      "iteration 3280, loss: 0.01088574156165123\n",
      "iteration 3281, loss: 0.010806292295455933\n",
      "iteration 3282, loss: 0.012418333441019058\n",
      "iteration 3283, loss: 0.011806027963757515\n",
      "iteration 3284, loss: 0.01219244860112667\n",
      "iteration 3285, loss: 0.011995397508144379\n",
      "iteration 3286, loss: 0.011591584421694279\n",
      "iteration 3287, loss: 0.011124689131975174\n",
      "iteration 3288, loss: 0.010938091203570366\n",
      "iteration 3289, loss: 0.01104914303869009\n",
      "iteration 3290, loss: 0.011798381805419922\n",
      "iteration 3291, loss: 0.01045321300625801\n",
      "iteration 3292, loss: 0.012277086265385151\n",
      "iteration 3293, loss: 0.012108944356441498\n",
      "iteration 3294, loss: 0.012241151183843613\n",
      "iteration 3295, loss: 0.010830086655914783\n",
      "iteration 3296, loss: 0.01178714819252491\n",
      "iteration 3297, loss: 0.011004328727722168\n",
      "iteration 3298, loss: 0.012571790255606174\n",
      "iteration 3299, loss: 0.011470738798379898\n",
      "iteration 3300, loss: 0.013971392065286636\n",
      "iteration 3301, loss: 0.011289273388683796\n",
      "iteration 3302, loss: 0.010159030556678772\n",
      "iteration 3303, loss: 0.011720729991793633\n",
      "iteration 3304, loss: 0.011474058032035828\n",
      "iteration 3305, loss: 0.01121566817164421\n",
      "iteration 3306, loss: 0.011533857323229313\n",
      "iteration 3307, loss: 0.010451078414916992\n",
      "iteration 3308, loss: 0.012017469853162766\n",
      "iteration 3309, loss: 0.010027645155787468\n",
      "iteration 3310, loss: 0.011334963142871857\n",
      "iteration 3311, loss: 0.010004271753132343\n",
      "iteration 3312, loss: 0.01110842451453209\n",
      "iteration 3313, loss: 0.011050201021134853\n",
      "iteration 3314, loss: 0.011252254247665405\n",
      "iteration 3315, loss: 0.00976793747395277\n",
      "iteration 3316, loss: 0.012834320776164532\n",
      "iteration 3317, loss: 0.012423662468791008\n",
      "iteration 3318, loss: 0.009366437792778015\n",
      "iteration 3319, loss: 0.010482006706297398\n",
      "iteration 3320, loss: 0.01028028130531311\n",
      "iteration 3321, loss: 0.010641012340784073\n",
      "iteration 3322, loss: 0.01103982049971819\n",
      "iteration 3323, loss: 0.012052208185195923\n",
      "iteration 3324, loss: 0.012901151552796364\n",
      "iteration 3325, loss: 0.010933076031506062\n",
      "iteration 3326, loss: 0.013177350163459778\n",
      "iteration 3327, loss: 0.012172985821962357\n",
      "iteration 3328, loss: 0.012781592085957527\n",
      "iteration 3329, loss: 0.01292242668569088\n",
      "iteration 3330, loss: 0.011089259758591652\n",
      "iteration 3331, loss: 0.012701120227575302\n",
      "iteration 3332, loss: 0.012988267466425896\n",
      "iteration 3333, loss: 0.01110038161277771\n",
      "iteration 3334, loss: 0.012440800666809082\n",
      "iteration 3335, loss: 0.014696521684527397\n",
      "iteration 3336, loss: 0.01119282841682434\n",
      "iteration 3337, loss: 0.011548953130841255\n",
      "iteration 3338, loss: 0.014102419838309288\n",
      "iteration 3339, loss: 0.012362135574221611\n",
      "iteration 3340, loss: 0.012555373832583427\n",
      "iteration 3341, loss: 0.010231947526335716\n",
      "iteration 3342, loss: 0.012375732883810997\n",
      "iteration 3343, loss: 0.013014290481805801\n",
      "iteration 3344, loss: 0.011326268315315247\n",
      "iteration 3345, loss: 0.010517595335841179\n",
      "iteration 3346, loss: 0.010866941884160042\n",
      "iteration 3347, loss: 0.010616978630423546\n",
      "iteration 3348, loss: 0.011006149463355541\n",
      "iteration 3349, loss: 0.011464040726423264\n",
      "iteration 3350, loss: 0.011084236204624176\n",
      "iteration 3351, loss: 0.011682553216814995\n",
      "iteration 3352, loss: 0.010039301589131355\n",
      "iteration 3353, loss: 0.012793539091944695\n",
      "iteration 3354, loss: 0.013189887627959251\n",
      "iteration 3355, loss: 0.011387459933757782\n",
      "iteration 3356, loss: 0.011488611809909344\n",
      "iteration 3357, loss: 0.013498814776539803\n",
      "iteration 3358, loss: 0.011889114044606686\n",
      "iteration 3359, loss: 0.009303470142185688\n",
      "iteration 3360, loss: 0.010205728933215141\n",
      "iteration 3361, loss: 0.011299150995910168\n",
      "iteration 3362, loss: 0.0116350669413805\n",
      "iteration 3363, loss: 0.00930800661444664\n",
      "iteration 3364, loss: 0.012970270588994026\n",
      "iteration 3365, loss: 0.012934106402099133\n",
      "iteration 3366, loss: 0.011428967118263245\n",
      "iteration 3367, loss: 0.012100429274141788\n",
      "iteration 3368, loss: 0.010999715887010098\n",
      "iteration 3369, loss: 0.011184907518327236\n",
      "iteration 3370, loss: 0.01210583932697773\n",
      "iteration 3371, loss: 0.011610815301537514\n",
      "iteration 3372, loss: 0.01264018565416336\n",
      "iteration 3373, loss: 0.012823224067687988\n",
      "iteration 3374, loss: 0.01050734706223011\n",
      "iteration 3375, loss: 0.011525779962539673\n",
      "iteration 3376, loss: 0.01094021461904049\n",
      "iteration 3377, loss: 0.011311288923025131\n",
      "iteration 3378, loss: 0.01106233149766922\n",
      "iteration 3379, loss: 0.011791804805397987\n",
      "iteration 3380, loss: 0.012650141492486\n",
      "iteration 3381, loss: 0.012215688824653625\n",
      "iteration 3382, loss: 0.0093495212495327\n",
      "iteration 3383, loss: 0.011935903690755367\n",
      "iteration 3384, loss: 0.012808222323656082\n",
      "iteration 3385, loss: 0.011749662458896637\n",
      "iteration 3386, loss: 0.01062827743589878\n",
      "iteration 3387, loss: 0.01018848828971386\n",
      "iteration 3388, loss: 0.009851377457380295\n",
      "iteration 3389, loss: 0.011181916110217571\n",
      "iteration 3390, loss: 0.010477622970938683\n",
      "iteration 3391, loss: 0.012605291791260242\n",
      "iteration 3392, loss: 0.012147299014031887\n",
      "iteration 3393, loss: 0.012692797929048538\n",
      "iteration 3394, loss: 0.010546176694333553\n",
      "iteration 3395, loss: 0.013165449723601341\n",
      "iteration 3396, loss: 0.010518131777644157\n",
      "iteration 3397, loss: 0.010651339776813984\n",
      "iteration 3398, loss: 0.01148436963558197\n",
      "iteration 3399, loss: 0.012500569224357605\n",
      "iteration 3400, loss: 0.01089654304087162\n",
      "iteration 3401, loss: 0.010205013677477837\n",
      "iteration 3402, loss: 0.011991615407168865\n",
      "iteration 3403, loss: 0.013109639286994934\n",
      "iteration 3404, loss: 0.008897705003619194\n",
      "iteration 3405, loss: 0.01226090732961893\n",
      "iteration 3406, loss: 0.011675303801894188\n",
      "iteration 3407, loss: 0.01128428801894188\n",
      "iteration 3408, loss: 0.01287134364247322\n",
      "iteration 3409, loss: 0.010963980108499527\n",
      "iteration 3410, loss: 0.01195572130382061\n",
      "iteration 3411, loss: 0.01070857048034668\n",
      "iteration 3412, loss: 0.010383381508290768\n",
      "iteration 3413, loss: 0.0100446417927742\n",
      "iteration 3414, loss: 0.010762149468064308\n",
      "iteration 3415, loss: 0.011144991032779217\n",
      "iteration 3416, loss: 0.01065135095268488\n",
      "iteration 3417, loss: 0.011042752303183079\n",
      "iteration 3418, loss: 0.011684129945933819\n",
      "iteration 3419, loss: 0.01100572757422924\n",
      "iteration 3420, loss: 0.01067749597132206\n",
      "iteration 3421, loss: 0.010984455235302448\n",
      "iteration 3422, loss: 0.010121587663888931\n",
      "iteration 3423, loss: 0.010990927927196026\n",
      "iteration 3424, loss: 0.012213896960020065\n",
      "iteration 3425, loss: 0.010029824450612068\n",
      "iteration 3426, loss: 0.011808780953288078\n",
      "iteration 3427, loss: 0.011114072054624557\n",
      "iteration 3428, loss: 0.011865783482789993\n",
      "iteration 3429, loss: 0.011382296681404114\n",
      "iteration 3430, loss: 0.010912135243415833\n",
      "iteration 3431, loss: 0.01028494630008936\n",
      "iteration 3432, loss: 0.010205347090959549\n",
      "iteration 3433, loss: 0.011119026690721512\n",
      "iteration 3434, loss: 0.011229371652007103\n",
      "iteration 3435, loss: 0.0106133334338665\n",
      "iteration 3436, loss: 0.010814779438078403\n",
      "iteration 3437, loss: 0.011731214821338654\n",
      "iteration 3438, loss: 0.010762568563222885\n",
      "iteration 3439, loss: 0.009917423129081726\n",
      "iteration 3440, loss: 0.008916408754885197\n",
      "iteration 3441, loss: 0.010663200169801712\n",
      "iteration 3442, loss: 0.012409706600010395\n",
      "iteration 3443, loss: 0.011051958426833153\n",
      "iteration 3444, loss: 0.012255332432687283\n",
      "iteration 3445, loss: 0.011186894029378891\n",
      "iteration 3446, loss: 0.011299872770905495\n",
      "iteration 3447, loss: 0.011849547736346722\n",
      "iteration 3448, loss: 0.011253572069108486\n",
      "iteration 3449, loss: 0.010433402843773365\n",
      "iteration 3450, loss: 0.011667128652334213\n",
      "iteration 3451, loss: 0.01181604154407978\n",
      "iteration 3452, loss: 0.01224706694483757\n",
      "iteration 3453, loss: 0.010569284670054913\n",
      "iteration 3454, loss: 0.011236882768571377\n",
      "iteration 3455, loss: 0.014598681591451168\n",
      "iteration 3456, loss: 0.011809412389993668\n",
      "iteration 3457, loss: 0.009109808132052422\n",
      "iteration 3458, loss: 0.012427947483956814\n",
      "iteration 3459, loss: 0.010449393652379513\n",
      "iteration 3460, loss: 0.01246950589120388\n",
      "iteration 3461, loss: 0.011303511448204517\n",
      "iteration 3462, loss: 0.009933010675013065\n",
      "iteration 3463, loss: 0.014022592455148697\n",
      "iteration 3464, loss: 0.010511545464396477\n",
      "iteration 3465, loss: 0.012133782729506493\n",
      "iteration 3466, loss: 0.013391830027103424\n",
      "iteration 3467, loss: 0.011973592452704906\n",
      "iteration 3468, loss: 0.011352600529789925\n",
      "iteration 3469, loss: 0.01170404814183712\n",
      "iteration 3470, loss: 0.009033748880028725\n",
      "iteration 3471, loss: 0.012406088411808014\n",
      "iteration 3472, loss: 0.011677578091621399\n",
      "iteration 3473, loss: 0.00984400324523449\n",
      "iteration 3474, loss: 0.010062615387141705\n",
      "iteration 3475, loss: 0.01209588535130024\n",
      "iteration 3476, loss: 0.012765402905642986\n",
      "iteration 3477, loss: 0.010624216869473457\n",
      "iteration 3478, loss: 0.010440289974212646\n",
      "iteration 3479, loss: 0.011612223461270332\n",
      "iteration 3480, loss: 0.011774202808737755\n",
      "iteration 3481, loss: 0.009607098065316677\n",
      "iteration 3482, loss: 0.011497221887111664\n",
      "iteration 3483, loss: 0.012057697400450706\n",
      "iteration 3484, loss: 0.010486358776688576\n",
      "iteration 3485, loss: 0.011842122301459312\n",
      "iteration 3486, loss: 0.0116298608481884\n",
      "iteration 3487, loss: 0.011754872277379036\n",
      "iteration 3488, loss: 0.010189809836447239\n",
      "iteration 3489, loss: 0.009984093718230724\n",
      "iteration 3490, loss: 0.009698864072561264\n",
      "iteration 3491, loss: 0.012466257438063622\n",
      "iteration 3492, loss: 0.01243574358522892\n",
      "iteration 3493, loss: 0.010439775884151459\n",
      "iteration 3494, loss: 0.009944148361682892\n",
      "iteration 3495, loss: 0.012305084615945816\n",
      "iteration 3496, loss: 0.009756598621606827\n",
      "iteration 3497, loss: 0.010691718198359013\n",
      "iteration 3498, loss: 0.011392460204660892\n",
      "iteration 3499, loss: 0.012345916591584682\n",
      "iteration 3500, loss: 0.010524706915020943\n",
      "iteration 3501, loss: 0.011412153020501137\n",
      "iteration 3502, loss: 0.010914456099271774\n",
      "iteration 3503, loss: 0.00948404986411333\n",
      "iteration 3504, loss: 0.012549011968076229\n",
      "iteration 3505, loss: 0.011440061964094639\n",
      "iteration 3506, loss: 0.011318748816847801\n",
      "iteration 3507, loss: 0.010304233059287071\n",
      "iteration 3508, loss: 0.010640750639140606\n",
      "iteration 3509, loss: 0.009115861728787422\n",
      "iteration 3510, loss: 0.011789681389927864\n",
      "iteration 3511, loss: 0.011431618593633175\n",
      "iteration 3512, loss: 0.010249493643641472\n",
      "iteration 3513, loss: 0.010326772928237915\n",
      "iteration 3514, loss: 0.010525992140173912\n",
      "iteration 3515, loss: 0.012485314160585403\n",
      "iteration 3516, loss: 0.009645645506680012\n",
      "iteration 3517, loss: 0.011185640469193459\n",
      "iteration 3518, loss: 0.010711188428103924\n",
      "iteration 3519, loss: 0.011902342550456524\n",
      "iteration 3520, loss: 0.012111463584005833\n",
      "iteration 3521, loss: 0.009932655841112137\n",
      "iteration 3522, loss: 0.009278936311602592\n",
      "iteration 3523, loss: 0.011079023592174053\n",
      "iteration 3524, loss: 0.011127171106636524\n",
      "iteration 3525, loss: 0.010605653747916222\n",
      "iteration 3526, loss: 0.012947773560881615\n",
      "iteration 3527, loss: 0.009859953075647354\n",
      "iteration 3528, loss: 0.012262335047125816\n",
      "iteration 3529, loss: 0.010738762095570564\n",
      "iteration 3530, loss: 0.010513093322515488\n",
      "iteration 3531, loss: 0.011619117110967636\n",
      "iteration 3532, loss: 0.01137908548116684\n",
      "iteration 3533, loss: 0.011709545738995075\n",
      "iteration 3534, loss: 0.011831862851977348\n",
      "iteration 3535, loss: 0.010524408891797066\n",
      "iteration 3536, loss: 0.010481433011591434\n",
      "iteration 3537, loss: 0.01073897909373045\n",
      "iteration 3538, loss: 0.011528532952070236\n",
      "iteration 3539, loss: 0.010111058130860329\n",
      "iteration 3540, loss: 0.011580686084926128\n",
      "iteration 3541, loss: 0.010896570980548859\n",
      "iteration 3542, loss: 0.010865254327654839\n",
      "iteration 3543, loss: 0.01034146174788475\n",
      "iteration 3544, loss: 0.009099636226892471\n",
      "iteration 3545, loss: 0.010526599362492561\n",
      "iteration 3546, loss: 0.010578854940831661\n",
      "iteration 3547, loss: 0.008711643517017365\n",
      "iteration 3548, loss: 0.011750653386116028\n",
      "iteration 3549, loss: 0.00895062368363142\n",
      "iteration 3550, loss: 0.010720856487751007\n",
      "iteration 3551, loss: 0.009178301319479942\n",
      "iteration 3552, loss: 0.01148160733282566\n",
      "iteration 3553, loss: 0.010209119878709316\n",
      "iteration 3554, loss: 0.011368035338819027\n",
      "iteration 3555, loss: 0.010482721030712128\n",
      "iteration 3556, loss: 0.010460629127919674\n",
      "iteration 3557, loss: 0.01136280968785286\n",
      "iteration 3558, loss: 0.011988602578639984\n",
      "iteration 3559, loss: 0.008705365471541882\n",
      "iteration 3560, loss: 0.010176289826631546\n",
      "iteration 3561, loss: 0.010112540796399117\n",
      "iteration 3562, loss: 0.011655950918793678\n",
      "iteration 3563, loss: 0.012548767030239105\n",
      "iteration 3564, loss: 0.011186925694346428\n",
      "iteration 3565, loss: 0.010817436501383781\n",
      "iteration 3566, loss: 0.011353112757205963\n",
      "iteration 3567, loss: 0.008911512792110443\n",
      "iteration 3568, loss: 0.009782405570149422\n",
      "iteration 3569, loss: 0.009855663403868675\n",
      "iteration 3570, loss: 0.009603044018149376\n",
      "iteration 3571, loss: 0.010721182450652122\n",
      "iteration 3572, loss: 0.01099445391446352\n",
      "iteration 3573, loss: 0.010913163423538208\n",
      "iteration 3574, loss: 0.009483875706791878\n",
      "iteration 3575, loss: 0.009729345329105854\n",
      "iteration 3576, loss: 0.009825637564063072\n",
      "iteration 3577, loss: 0.011308148503303528\n",
      "iteration 3578, loss: 0.011197146028280258\n",
      "iteration 3579, loss: 0.01030842587351799\n",
      "iteration 3580, loss: 0.011436711996793747\n",
      "iteration 3581, loss: 0.011228518560528755\n",
      "iteration 3582, loss: 0.011130797676742077\n",
      "iteration 3583, loss: 0.00937793217599392\n",
      "iteration 3584, loss: 0.011241016909480095\n",
      "iteration 3585, loss: 0.010341467335820198\n",
      "iteration 3586, loss: 0.009784098714590073\n",
      "iteration 3587, loss: 0.010373398661613464\n",
      "iteration 3588, loss: 0.01134579535573721\n",
      "iteration 3589, loss: 0.011668766848742962\n",
      "iteration 3590, loss: 0.010581770911812782\n",
      "iteration 3591, loss: 0.010992037132382393\n",
      "iteration 3592, loss: 0.0108535410836339\n",
      "iteration 3593, loss: 0.009635372087359428\n",
      "iteration 3594, loss: 0.011574748903512955\n",
      "iteration 3595, loss: 0.012624918483197689\n",
      "iteration 3596, loss: 0.010285643860697746\n",
      "iteration 3597, loss: 0.010338676162064075\n",
      "iteration 3598, loss: 0.010743148624897003\n",
      "iteration 3599, loss: 0.011039583943784237\n",
      "iteration 3600, loss: 0.013602133840322495\n",
      "iteration 3601, loss: 0.010627465322613716\n",
      "iteration 3602, loss: 0.011917933821678162\n",
      "iteration 3603, loss: 0.011599916964769363\n",
      "iteration 3604, loss: 0.011091447435319424\n",
      "iteration 3605, loss: 0.010283643379807472\n",
      "iteration 3606, loss: 0.01076609268784523\n",
      "iteration 3607, loss: 0.008686012588441372\n",
      "iteration 3608, loss: 0.011007794179022312\n",
      "iteration 3609, loss: 0.009644852951169014\n",
      "iteration 3610, loss: 0.01191096194088459\n",
      "iteration 3611, loss: 0.010485446080565453\n",
      "iteration 3612, loss: 0.010482043027877808\n",
      "iteration 3613, loss: 0.010228201746940613\n",
      "iteration 3614, loss: 0.009601232595741749\n",
      "iteration 3615, loss: 0.011169337667524815\n",
      "iteration 3616, loss: 0.011513954028487206\n",
      "iteration 3617, loss: 0.010499997064471245\n",
      "iteration 3618, loss: 0.010815707966685295\n",
      "iteration 3619, loss: 0.01142929121851921\n",
      "iteration 3620, loss: 0.010837964713573456\n",
      "iteration 3621, loss: 0.01183258555829525\n",
      "iteration 3622, loss: 0.010616159997880459\n",
      "iteration 3623, loss: 0.011675003916025162\n",
      "iteration 3624, loss: 0.010657777078449726\n",
      "iteration 3625, loss: 0.013188336975872517\n",
      "iteration 3626, loss: 0.01185869611799717\n",
      "iteration 3627, loss: 0.011840872466564178\n",
      "iteration 3628, loss: 0.012698706239461899\n",
      "iteration 3629, loss: 0.010827871039509773\n",
      "iteration 3630, loss: 0.00877458043396473\n",
      "iteration 3631, loss: 0.009850074537098408\n",
      "iteration 3632, loss: 0.009948348626494408\n",
      "iteration 3633, loss: 0.011347822844982147\n",
      "iteration 3634, loss: 0.009639741852879524\n",
      "iteration 3635, loss: 0.009938787668943405\n",
      "iteration 3636, loss: 0.010045737028121948\n",
      "iteration 3637, loss: 0.011101503856480122\n",
      "iteration 3638, loss: 0.00937670562416315\n",
      "iteration 3639, loss: 0.009873243048787117\n",
      "iteration 3640, loss: 0.010973588563501835\n",
      "iteration 3641, loss: 0.010842975229024887\n",
      "iteration 3642, loss: 0.011045990511775017\n",
      "iteration 3643, loss: 0.011050641536712646\n",
      "iteration 3644, loss: 0.010068727657198906\n",
      "iteration 3645, loss: 0.010293912142515182\n",
      "iteration 3646, loss: 0.008447508327662945\n",
      "iteration 3647, loss: 0.010751650668680668\n",
      "iteration 3648, loss: 0.01063024066388607\n",
      "iteration 3649, loss: 0.010822965763509274\n",
      "iteration 3650, loss: 0.010117325000464916\n",
      "iteration 3651, loss: 0.00935768336057663\n",
      "iteration 3652, loss: 0.010701322928071022\n",
      "iteration 3653, loss: 0.009957762435078621\n",
      "iteration 3654, loss: 0.012054323218762875\n",
      "iteration 3655, loss: 0.011247582733631134\n",
      "iteration 3656, loss: 0.009990192949771881\n",
      "iteration 3657, loss: 0.01092660054564476\n",
      "iteration 3658, loss: 0.009933654218912125\n",
      "iteration 3659, loss: 0.010064879432320595\n",
      "iteration 3660, loss: 0.009670357219874859\n",
      "iteration 3661, loss: 0.010421084240078926\n",
      "iteration 3662, loss: 0.009905221872031689\n",
      "iteration 3663, loss: 0.009764381684362888\n",
      "iteration 3664, loss: 0.01015496626496315\n",
      "iteration 3665, loss: 0.010670343413949013\n",
      "iteration 3666, loss: 0.009988570585846901\n",
      "iteration 3667, loss: 0.010105104185640812\n",
      "iteration 3668, loss: 0.01065741665661335\n",
      "iteration 3669, loss: 0.010111676529049873\n",
      "iteration 3670, loss: 0.01112828217446804\n",
      "iteration 3671, loss: 0.009994394145905972\n",
      "iteration 3672, loss: 0.008180425502359867\n",
      "iteration 3673, loss: 0.00891750305891037\n",
      "iteration 3674, loss: 0.010731571353971958\n",
      "iteration 3675, loss: 0.010737909004092216\n",
      "iteration 3676, loss: 0.010742772370576859\n",
      "iteration 3677, loss: 0.010515918955206871\n",
      "iteration 3678, loss: 0.010505005717277527\n",
      "iteration 3679, loss: 0.01066509634256363\n",
      "iteration 3680, loss: 0.009676208719611168\n",
      "iteration 3681, loss: 0.009993200190365314\n",
      "iteration 3682, loss: 0.009800372645258904\n",
      "iteration 3683, loss: 0.01088461373001337\n",
      "iteration 3684, loss: 0.012170051224529743\n",
      "iteration 3685, loss: 0.01149054430425167\n",
      "iteration 3686, loss: 0.010667320340871811\n",
      "iteration 3687, loss: 0.011271627619862556\n",
      "iteration 3688, loss: 0.010189004242420197\n",
      "iteration 3689, loss: 0.009593350812792778\n",
      "iteration 3690, loss: 0.011859899386763573\n",
      "iteration 3691, loss: 0.012801161035895348\n",
      "iteration 3692, loss: 0.010918686166405678\n",
      "iteration 3693, loss: 0.010609052143990993\n",
      "iteration 3694, loss: 0.013024628162384033\n",
      "iteration 3695, loss: 0.010057833045721054\n",
      "iteration 3696, loss: 0.010189821012318134\n",
      "iteration 3697, loss: 0.010394797660410404\n",
      "iteration 3698, loss: 0.009674280881881714\n",
      "iteration 3699, loss: 0.01109796017408371\n",
      "iteration 3700, loss: 0.010694711469113827\n",
      "iteration 3701, loss: 0.009631820023059845\n",
      "iteration 3702, loss: 0.01012696698307991\n",
      "iteration 3703, loss: 0.010685808956623077\n",
      "iteration 3704, loss: 0.010560069233179092\n",
      "iteration 3705, loss: 0.010549378581345081\n",
      "iteration 3706, loss: 0.010471377521753311\n",
      "iteration 3707, loss: 0.010884039103984833\n",
      "iteration 3708, loss: 0.008942907676100731\n",
      "iteration 3709, loss: 0.011065492406487465\n",
      "iteration 3710, loss: 0.009718049317598343\n",
      "iteration 3711, loss: 0.010219547897577286\n",
      "iteration 3712, loss: 0.01055607944726944\n",
      "iteration 3713, loss: 0.010962003841996193\n",
      "iteration 3714, loss: 0.010248948819935322\n",
      "iteration 3715, loss: 0.009491313248872757\n",
      "iteration 3716, loss: 0.010044691152870655\n",
      "iteration 3717, loss: 0.009351575747132301\n",
      "iteration 3718, loss: 0.011422540061175823\n",
      "iteration 3719, loss: 0.011241676285862923\n",
      "iteration 3720, loss: 0.010578949935734272\n",
      "iteration 3721, loss: 0.009344203397631645\n",
      "iteration 3722, loss: 0.009193957783281803\n",
      "iteration 3723, loss: 0.008936796337366104\n",
      "iteration 3724, loss: 0.011544248089194298\n",
      "iteration 3725, loss: 0.009507568553090096\n",
      "iteration 3726, loss: 0.010553340427577496\n",
      "iteration 3727, loss: 0.009739872068166733\n",
      "iteration 3728, loss: 0.009931406937539577\n",
      "iteration 3729, loss: 0.009953958913683891\n",
      "iteration 3730, loss: 0.00866794865578413\n",
      "iteration 3731, loss: 0.010767132043838501\n",
      "iteration 3732, loss: 0.010344155132770538\n",
      "iteration 3733, loss: 0.012464618310332298\n",
      "iteration 3734, loss: 0.010555039159953594\n",
      "iteration 3735, loss: 0.011082133278250694\n",
      "iteration 3736, loss: 0.011724546551704407\n",
      "iteration 3737, loss: 0.011651268228888512\n",
      "iteration 3738, loss: 0.010101989842951298\n",
      "iteration 3739, loss: 0.010529467836022377\n",
      "iteration 3740, loss: 0.010514385998249054\n",
      "iteration 3741, loss: 0.010195929557085037\n",
      "iteration 3742, loss: 0.010010212659835815\n",
      "iteration 3743, loss: 0.011673716828227043\n",
      "iteration 3744, loss: 0.013213886879384518\n",
      "iteration 3745, loss: 0.011085337959229946\n",
      "iteration 3746, loss: 0.011444468051195145\n",
      "iteration 3747, loss: 0.00959433987736702\n",
      "iteration 3748, loss: 0.010310106910765171\n",
      "iteration 3749, loss: 0.010322973132133484\n",
      "iteration 3750, loss: 0.010879728011786938\n",
      "iteration 3751, loss: 0.01067059300839901\n",
      "iteration 3752, loss: 0.00987303163856268\n",
      "iteration 3753, loss: 0.01085101068019867\n",
      "iteration 3754, loss: 0.01124592311680317\n",
      "iteration 3755, loss: 0.009642143733799458\n",
      "iteration 3756, loss: 0.010391350835561752\n",
      "iteration 3757, loss: 0.009112734347581863\n",
      "iteration 3758, loss: 0.0074011399410665035\n",
      "iteration 3759, loss: 0.010357992723584175\n",
      "iteration 3760, loss: 0.009230528958141804\n",
      "iteration 3761, loss: 0.00992112047970295\n",
      "iteration 3762, loss: 0.01068655215203762\n",
      "iteration 3763, loss: 0.011346986517310143\n",
      "iteration 3764, loss: 0.011684742756187916\n",
      "iteration 3765, loss: 0.00987726915627718\n",
      "iteration 3766, loss: 0.00862901285290718\n",
      "iteration 3767, loss: 0.01010207086801529\n",
      "iteration 3768, loss: 0.009544970467686653\n",
      "iteration 3769, loss: 0.009719088673591614\n",
      "iteration 3770, loss: 0.009651048108935356\n",
      "iteration 3771, loss: 0.010029401630163193\n",
      "iteration 3772, loss: 0.010602012276649475\n",
      "iteration 3773, loss: 0.011186870746314526\n",
      "iteration 3774, loss: 0.009654075838625431\n",
      "iteration 3775, loss: 0.009928002953529358\n",
      "iteration 3776, loss: 0.009772425517439842\n",
      "iteration 3777, loss: 0.010476150549948215\n",
      "iteration 3778, loss: 0.008856769651174545\n",
      "iteration 3779, loss: 0.009609234519302845\n",
      "iteration 3780, loss: 0.011209357529878616\n",
      "iteration 3781, loss: 0.009912450797855854\n",
      "iteration 3782, loss: 0.010804669931530952\n",
      "iteration 3783, loss: 0.00994350016117096\n",
      "iteration 3784, loss: 0.011484256014227867\n",
      "iteration 3785, loss: 0.011446227319538593\n",
      "iteration 3786, loss: 0.011373765766620636\n",
      "iteration 3787, loss: 0.011128842830657959\n",
      "iteration 3788, loss: 0.010574838146567345\n",
      "iteration 3789, loss: 0.009598784148693085\n",
      "iteration 3790, loss: 0.01112265232950449\n",
      "iteration 3791, loss: 0.009418793953955173\n",
      "iteration 3792, loss: 0.010669765062630177\n",
      "iteration 3793, loss: 0.010094520635902882\n",
      "iteration 3794, loss: 0.00945253111422062\n",
      "iteration 3795, loss: 0.01011451706290245\n",
      "iteration 3796, loss: 0.01212723646312952\n",
      "iteration 3797, loss: 0.009166978299617767\n",
      "iteration 3798, loss: 0.011548344045877457\n",
      "iteration 3799, loss: 0.010142872110009193\n",
      "iteration 3800, loss: 0.010305531322956085\n",
      "iteration 3801, loss: 0.009778613224625587\n",
      "iteration 3802, loss: 0.01094922237098217\n",
      "iteration 3803, loss: 0.010537581518292427\n",
      "iteration 3804, loss: 0.01053319126367569\n",
      "iteration 3805, loss: 0.008908683434128761\n",
      "iteration 3806, loss: 0.009225854650139809\n",
      "iteration 3807, loss: 0.009897825308144093\n",
      "iteration 3808, loss: 0.009593033231794834\n",
      "iteration 3809, loss: 0.010334574617445469\n",
      "iteration 3810, loss: 0.00999157503247261\n",
      "iteration 3811, loss: 0.011495139449834824\n",
      "iteration 3812, loss: 0.011652624234557152\n",
      "iteration 3813, loss: 0.009682697243988514\n",
      "iteration 3814, loss: 0.009806480258703232\n",
      "iteration 3815, loss: 0.010804655961692333\n",
      "iteration 3816, loss: 0.009264473803341389\n",
      "iteration 3817, loss: 0.010839918628334999\n",
      "iteration 3818, loss: 0.008917802013456821\n",
      "iteration 3819, loss: 0.010716141201555729\n",
      "iteration 3820, loss: 0.009731785394251347\n",
      "iteration 3821, loss: 0.00946750771254301\n",
      "iteration 3822, loss: 0.010367928072810173\n",
      "iteration 3823, loss: 0.010288191959261894\n",
      "iteration 3824, loss: 0.010786660015583038\n",
      "iteration 3825, loss: 0.009103139862418175\n",
      "iteration 3826, loss: 0.010255874134600163\n",
      "iteration 3827, loss: 0.01079972181469202\n",
      "iteration 3828, loss: 0.010347504168748856\n",
      "iteration 3829, loss: 0.01038195751607418\n",
      "iteration 3830, loss: 0.012177815660834312\n",
      "iteration 3831, loss: 0.008759228512644768\n",
      "iteration 3832, loss: 0.011636851355433464\n",
      "iteration 3833, loss: 0.011050748638808727\n",
      "iteration 3834, loss: 0.010097634047269821\n",
      "iteration 3835, loss: 0.010994594544172287\n",
      "iteration 3836, loss: 0.008917303755879402\n",
      "iteration 3837, loss: 0.01032230444252491\n",
      "iteration 3838, loss: 0.009701695293188095\n",
      "iteration 3839, loss: 0.009959466755390167\n",
      "iteration 3840, loss: 0.010311385616660118\n",
      "iteration 3841, loss: 0.009568421170115471\n",
      "iteration 3842, loss: 0.011370249092578888\n",
      "iteration 3843, loss: 0.009572114795446396\n",
      "iteration 3844, loss: 0.009938381612300873\n",
      "iteration 3845, loss: 0.010397868230938911\n",
      "iteration 3846, loss: 0.010353004559874535\n",
      "iteration 3847, loss: 0.011559748090803623\n",
      "iteration 3848, loss: 0.009394153021275997\n",
      "iteration 3849, loss: 0.00925949215888977\n",
      "iteration 3850, loss: 0.010184841230511665\n",
      "iteration 3851, loss: 0.011451376602053642\n",
      "iteration 3852, loss: 0.010395082645118237\n",
      "iteration 3853, loss: 0.009756922721862793\n",
      "iteration 3854, loss: 0.011066103354096413\n",
      "iteration 3855, loss: 0.00792619213461876\n",
      "iteration 3856, loss: 0.009831315837800503\n",
      "iteration 3857, loss: 0.010554959066212177\n",
      "iteration 3858, loss: 0.008673159405589104\n",
      "iteration 3859, loss: 0.010481411591172218\n",
      "iteration 3860, loss: 0.00786670297384262\n",
      "iteration 3861, loss: 0.008751412853598595\n",
      "iteration 3862, loss: 0.009701771661639214\n",
      "iteration 3863, loss: 0.009199582040309906\n",
      "iteration 3864, loss: 0.008238698355853558\n",
      "iteration 3865, loss: 0.009500494226813316\n",
      "iteration 3866, loss: 0.012098437175154686\n",
      "iteration 3867, loss: 0.00916813313961029\n",
      "iteration 3868, loss: 0.009566311724483967\n",
      "iteration 3869, loss: 0.010069366544485092\n",
      "iteration 3870, loss: 0.010291269049048424\n",
      "iteration 3871, loss: 0.009729739278554916\n",
      "iteration 3872, loss: 0.010034001432359219\n",
      "iteration 3873, loss: 0.008861253038048744\n",
      "iteration 3874, loss: 0.01059572584927082\n",
      "iteration 3875, loss: 0.009444445371627808\n",
      "iteration 3876, loss: 0.010586051270365715\n",
      "iteration 3877, loss: 0.009311743080615997\n",
      "iteration 3878, loss: 0.009213345125317574\n",
      "iteration 3879, loss: 0.010437039658427238\n",
      "iteration 3880, loss: 0.010249792598187923\n",
      "iteration 3881, loss: 0.009674429893493652\n",
      "iteration 3882, loss: 0.008395472541451454\n",
      "iteration 3883, loss: 0.010634129866957664\n",
      "iteration 3884, loss: 0.009830227121710777\n",
      "iteration 3885, loss: 0.01000493299216032\n",
      "iteration 3886, loss: 0.0097122173756361\n",
      "iteration 3887, loss: 0.009024376049637794\n",
      "iteration 3888, loss: 0.010211383923888206\n",
      "iteration 3889, loss: 0.009457005187869072\n",
      "iteration 3890, loss: 0.010986552573740482\n",
      "iteration 3891, loss: 0.010530658066272736\n",
      "iteration 3892, loss: 0.01096111536026001\n",
      "iteration 3893, loss: 0.009982878342270851\n",
      "iteration 3894, loss: 0.009776866994798183\n",
      "iteration 3895, loss: 0.010740776546299458\n",
      "iteration 3896, loss: 0.010923508554697037\n",
      "iteration 3897, loss: 0.00840570405125618\n",
      "iteration 3898, loss: 0.010169288143515587\n",
      "iteration 3899, loss: 0.00934031791985035\n",
      "iteration 3900, loss: 0.009166780859231949\n",
      "iteration 3901, loss: 0.010033348575234413\n",
      "iteration 3902, loss: 0.009154034778475761\n",
      "iteration 3903, loss: 0.010591421276330948\n",
      "iteration 3904, loss: 0.009481288492679596\n",
      "iteration 3905, loss: 0.010371632874011993\n",
      "iteration 3906, loss: 0.008854866027832031\n",
      "iteration 3907, loss: 0.009183596819639206\n",
      "iteration 3908, loss: 0.011414257809519768\n",
      "iteration 3909, loss: 0.010523959994316101\n",
      "iteration 3910, loss: 0.008821776136755943\n",
      "iteration 3911, loss: 0.010242636315524578\n",
      "iteration 3912, loss: 0.008822006173431873\n",
      "iteration 3913, loss: 0.009504414163529873\n",
      "iteration 3914, loss: 0.00864460039883852\n",
      "iteration 3915, loss: 0.011003981344401836\n",
      "iteration 3916, loss: 0.010279910638928413\n",
      "iteration 3917, loss: 0.01020834967494011\n",
      "iteration 3918, loss: 0.009402994997799397\n",
      "iteration 3919, loss: 0.010779483243823051\n",
      "iteration 3920, loss: 0.010993396863341331\n",
      "iteration 3921, loss: 0.010726945474743843\n",
      "iteration 3922, loss: 0.010880840942263603\n",
      "iteration 3923, loss: 0.009149184450507164\n",
      "iteration 3924, loss: 0.010726043954491615\n",
      "iteration 3925, loss: 0.009857870638370514\n",
      "iteration 3926, loss: 0.010409124195575714\n",
      "iteration 3927, loss: 0.012278702110052109\n",
      "iteration 3928, loss: 0.009785235859453678\n",
      "iteration 3929, loss: 0.010194174945354462\n",
      "iteration 3930, loss: 0.008702067658305168\n",
      "iteration 3931, loss: 0.009085256606340408\n",
      "iteration 3932, loss: 0.009302156046032906\n",
      "iteration 3933, loss: 0.009162391535937786\n",
      "iteration 3934, loss: 0.00870929192751646\n",
      "iteration 3935, loss: 0.009854251518845558\n",
      "iteration 3936, loss: 0.0077055711299180984\n",
      "iteration 3937, loss: 0.010076848790049553\n",
      "iteration 3938, loss: 0.009208466857671738\n",
      "iteration 3939, loss: 0.01069448422640562\n",
      "iteration 3940, loss: 0.010524165816605091\n",
      "iteration 3941, loss: 0.011459849774837494\n",
      "iteration 3942, loss: 0.009388310834765434\n",
      "iteration 3943, loss: 0.01017053797841072\n",
      "iteration 3944, loss: 0.009588470682501793\n",
      "iteration 3945, loss: 0.01011410541832447\n",
      "iteration 3946, loss: 0.009439650923013687\n",
      "iteration 3947, loss: 0.011665940284729004\n",
      "iteration 3948, loss: 0.010826488956809044\n",
      "iteration 3949, loss: 0.009686965495347977\n",
      "iteration 3950, loss: 0.00875815562903881\n",
      "iteration 3951, loss: 0.012836961075663567\n",
      "iteration 3952, loss: 0.009800320491194725\n",
      "iteration 3953, loss: 0.011053331196308136\n",
      "iteration 3954, loss: 0.00921061635017395\n",
      "iteration 3955, loss: 0.010669229552149773\n",
      "iteration 3956, loss: 0.011267198249697685\n",
      "iteration 3957, loss: 0.008211908861994743\n",
      "iteration 3958, loss: 0.010676411911845207\n",
      "iteration 3959, loss: 0.009229988791048527\n",
      "iteration 3960, loss: 0.009394047781825066\n",
      "iteration 3961, loss: 0.010011252015829086\n",
      "iteration 3962, loss: 0.009470117278397083\n",
      "iteration 3963, loss: 0.009594656527042389\n",
      "iteration 3964, loss: 0.010601190850138664\n",
      "iteration 3965, loss: 0.00917497742921114\n",
      "iteration 3966, loss: 0.008704631589353085\n",
      "iteration 3967, loss: 0.007906715385615826\n",
      "iteration 3968, loss: 0.008065618574619293\n",
      "iteration 3969, loss: 0.009202851913869381\n",
      "iteration 3970, loss: 0.009954096749424934\n",
      "iteration 3971, loss: 0.009449495002627373\n",
      "iteration 3972, loss: 0.009422415867447853\n",
      "iteration 3973, loss: 0.010207125917077065\n",
      "iteration 3974, loss: 0.00913135427981615\n",
      "iteration 3975, loss: 0.009773471392691135\n",
      "iteration 3976, loss: 0.007676407694816589\n",
      "iteration 3977, loss: 0.009014762006700039\n",
      "iteration 3978, loss: 0.01049760915338993\n",
      "iteration 3979, loss: 0.009549424983561039\n",
      "iteration 3980, loss: 0.009334977716207504\n",
      "iteration 3981, loss: 0.010694321244955063\n",
      "iteration 3982, loss: 0.008786015212535858\n",
      "iteration 3983, loss: 0.009219976142048836\n",
      "iteration 3984, loss: 0.010074284859001637\n",
      "iteration 3985, loss: 0.010065766051411629\n",
      "iteration 3986, loss: 0.009935896843671799\n",
      "iteration 3987, loss: 0.01045328751206398\n",
      "iteration 3988, loss: 0.011094694957137108\n",
      "iteration 3989, loss: 0.010109920054674149\n",
      "iteration 3990, loss: 0.010800234973430634\n",
      "iteration 3991, loss: 0.010891744866967201\n",
      "iteration 3992, loss: 0.009728655219078064\n",
      "iteration 3993, loss: 0.009893495589494705\n",
      "iteration 3994, loss: 0.00907327700406313\n",
      "iteration 3995, loss: 0.008286627009510994\n",
      "iteration 3996, loss: 0.007794786244630814\n",
      "iteration 3997, loss: 0.009519331157207489\n",
      "iteration 3998, loss: 0.010405444540083408\n",
      "iteration 3999, loss: 0.009191950783133507\n",
      "iteration 4000, loss: 0.010764367878437042\n",
      "iteration 4001, loss: 0.008195826783776283\n",
      "iteration 4002, loss: 0.010662989690899849\n",
      "iteration 4003, loss: 0.010042223148047924\n",
      "iteration 4004, loss: 0.009412882849574089\n",
      "iteration 4005, loss: 0.010288853198289871\n",
      "iteration 4006, loss: 0.008614454418420792\n",
      "iteration 4007, loss: 0.010042088106274605\n",
      "iteration 4008, loss: 0.008976774290204048\n",
      "iteration 4009, loss: 0.007936609908938408\n",
      "iteration 4010, loss: 0.009394467808306217\n",
      "iteration 4011, loss: 0.01047680526971817\n",
      "iteration 4012, loss: 0.00947397667914629\n",
      "iteration 4013, loss: 0.010575514286756516\n",
      "iteration 4014, loss: 0.00925438106060028\n",
      "iteration 4015, loss: 0.010193757712841034\n",
      "iteration 4016, loss: 0.009607546962797642\n",
      "iteration 4017, loss: 0.008825907483696938\n",
      "iteration 4018, loss: 0.008566921576857567\n",
      "iteration 4019, loss: 0.010052738711237907\n",
      "iteration 4020, loss: 0.010548708960413933\n",
      "iteration 4021, loss: 0.007738157175481319\n",
      "iteration 4022, loss: 0.009625609964132309\n",
      "iteration 4023, loss: 0.0086728036403656\n",
      "iteration 4024, loss: 0.009529010392725468\n",
      "iteration 4025, loss: 0.010305849835276604\n",
      "iteration 4026, loss: 0.00895603932440281\n",
      "iteration 4027, loss: 0.009085943922400475\n",
      "iteration 4028, loss: 0.009791253134608269\n",
      "iteration 4029, loss: 0.009089173749089241\n",
      "iteration 4030, loss: 0.009555386379361153\n",
      "iteration 4031, loss: 0.009073040448129177\n",
      "iteration 4032, loss: 0.010547151789069176\n",
      "iteration 4033, loss: 0.008725309744477272\n",
      "iteration 4034, loss: 0.010070733726024628\n",
      "iteration 4035, loss: 0.009608561173081398\n",
      "iteration 4036, loss: 0.009697209112346172\n",
      "iteration 4037, loss: 0.009734642691910267\n",
      "iteration 4038, loss: 0.008330030366778374\n",
      "iteration 4039, loss: 0.008697772398591042\n",
      "iteration 4040, loss: 0.009966086596250534\n",
      "iteration 4041, loss: 0.009870018810033798\n",
      "iteration 4042, loss: 0.009704574942588806\n",
      "iteration 4043, loss: 0.00827556662261486\n",
      "iteration 4044, loss: 0.009104818105697632\n",
      "iteration 4045, loss: 0.01012613344937563\n",
      "iteration 4046, loss: 0.010100804269313812\n",
      "iteration 4047, loss: 0.00874988455325365\n",
      "iteration 4048, loss: 0.00893398281186819\n",
      "iteration 4049, loss: 0.008045335300266743\n",
      "iteration 4050, loss: 0.009822378866374493\n",
      "iteration 4051, loss: 0.01066637970507145\n",
      "iteration 4052, loss: 0.009216178208589554\n",
      "iteration 4053, loss: 0.0093022920191288\n",
      "iteration 4054, loss: 0.009653648361563683\n",
      "iteration 4055, loss: 0.009853363037109375\n",
      "iteration 4056, loss: 0.010780086740851402\n",
      "iteration 4057, loss: 0.011294569820165634\n",
      "iteration 4058, loss: 0.010706941597163677\n",
      "iteration 4059, loss: 0.008484287187457085\n",
      "iteration 4060, loss: 0.009183268994092941\n",
      "iteration 4061, loss: 0.009657921269536018\n",
      "iteration 4062, loss: 0.007725430652499199\n",
      "iteration 4063, loss: 0.010904178023338318\n",
      "iteration 4064, loss: 0.008230417035520077\n",
      "iteration 4065, loss: 0.009950855746865273\n",
      "iteration 4066, loss: 0.009041104465723038\n",
      "iteration 4067, loss: 0.010217929258942604\n",
      "iteration 4068, loss: 0.00854460895061493\n",
      "iteration 4069, loss: 0.00869116373360157\n",
      "iteration 4070, loss: 0.008068676106631756\n",
      "iteration 4071, loss: 0.010689900256693363\n",
      "iteration 4072, loss: 0.010477031581103802\n",
      "iteration 4073, loss: 0.010543890297412872\n",
      "iteration 4074, loss: 0.0101689537987113\n",
      "iteration 4075, loss: 0.009999865666031837\n",
      "iteration 4076, loss: 0.010221539065241814\n",
      "iteration 4077, loss: 0.009058339521288872\n",
      "iteration 4078, loss: 0.010018205270171165\n",
      "iteration 4079, loss: 0.009532678872346878\n",
      "iteration 4080, loss: 0.008919991552829742\n",
      "iteration 4081, loss: 0.010276934131979942\n",
      "iteration 4082, loss: 0.010360637679696083\n",
      "iteration 4083, loss: 0.009557445533573627\n",
      "iteration 4084, loss: 0.009128976613283157\n",
      "iteration 4085, loss: 0.010162091813981533\n",
      "iteration 4086, loss: 0.008057819679379463\n",
      "iteration 4087, loss: 0.009706128388643265\n",
      "iteration 4088, loss: 0.00969134271144867\n",
      "iteration 4089, loss: 0.00941726379096508\n",
      "iteration 4090, loss: 0.00945212785154581\n",
      "iteration 4091, loss: 0.010310722514986992\n",
      "iteration 4092, loss: 0.009480871260166168\n",
      "iteration 4093, loss: 0.011464657261967659\n",
      "iteration 4094, loss: 0.008296131156384945\n",
      "iteration 4095, loss: 0.009027577936649323\n",
      "iteration 4096, loss: 0.010362165048718452\n",
      "iteration 4097, loss: 0.009576314128935337\n",
      "iteration 4098, loss: 0.009502838365733624\n",
      "iteration 4099, loss: 0.009665479883551598\n",
      "iteration 4100, loss: 0.009121431037783623\n",
      "iteration 4101, loss: 0.008577888831496239\n",
      "iteration 4102, loss: 0.009241316467523575\n",
      "iteration 4103, loss: 0.009420091286301613\n",
      "iteration 4104, loss: 0.010345377959311008\n",
      "iteration 4105, loss: 0.010221694596111774\n",
      "iteration 4106, loss: 0.010734043084084988\n",
      "iteration 4107, loss: 0.009857762604951859\n",
      "iteration 4108, loss: 0.010245632380247116\n",
      "iteration 4109, loss: 0.007389012258499861\n",
      "iteration 4110, loss: 0.00931518618017435\n",
      "iteration 4111, loss: 0.010383917950093746\n",
      "iteration 4112, loss: 0.010888185352087021\n",
      "iteration 4113, loss: 0.00840108934789896\n",
      "iteration 4114, loss: 0.009247852489352226\n",
      "iteration 4115, loss: 0.00904790684580803\n",
      "iteration 4116, loss: 0.008550742641091347\n",
      "iteration 4117, loss: 0.010308077558875084\n",
      "iteration 4118, loss: 0.00872817076742649\n",
      "iteration 4119, loss: 0.009207340888679028\n",
      "iteration 4120, loss: 0.008326293900609016\n",
      "iteration 4121, loss: 0.009681504219770432\n",
      "iteration 4122, loss: 0.008313702419400215\n",
      "iteration 4123, loss: 0.007428278215229511\n",
      "iteration 4124, loss: 0.009939058683812618\n",
      "iteration 4125, loss: 0.01081826537847519\n",
      "iteration 4126, loss: 0.007936578243970871\n",
      "iteration 4127, loss: 0.007605250924825668\n",
      "iteration 4128, loss: 0.009586401283740997\n",
      "iteration 4129, loss: 0.009899865835905075\n",
      "iteration 4130, loss: 0.009212322533130646\n",
      "iteration 4131, loss: 0.008604714646935463\n",
      "iteration 4132, loss: 0.008767787367105484\n",
      "iteration 4133, loss: 0.008699540048837662\n",
      "iteration 4134, loss: 0.008969040587544441\n",
      "iteration 4135, loss: 0.008587678894400597\n",
      "iteration 4136, loss: 0.007606827653944492\n",
      "iteration 4137, loss: 0.007540317252278328\n",
      "iteration 4138, loss: 0.008662217296659946\n",
      "iteration 4139, loss: 0.010118371807038784\n",
      "iteration 4140, loss: 0.008826084434986115\n",
      "iteration 4141, loss: 0.008689021691679955\n",
      "iteration 4142, loss: 0.009995246306061745\n",
      "iteration 4143, loss: 0.009285470470786095\n",
      "iteration 4144, loss: 0.008315213024616241\n",
      "iteration 4145, loss: 0.00846010074019432\n",
      "iteration 4146, loss: 0.00881989672780037\n",
      "iteration 4147, loss: 0.008661441504955292\n",
      "iteration 4148, loss: 0.008872218430042267\n",
      "iteration 4149, loss: 0.00914719793945551\n",
      "iteration 4150, loss: 0.00851198099553585\n",
      "iteration 4151, loss: 0.008282527327537537\n",
      "iteration 4152, loss: 0.009625427424907684\n",
      "iteration 4153, loss: 0.009023915976285934\n",
      "iteration 4154, loss: 0.008332252502441406\n",
      "iteration 4155, loss: 0.009417924098670483\n",
      "iteration 4156, loss: 0.01020977646112442\n",
      "iteration 4157, loss: 0.008523818105459213\n",
      "iteration 4158, loss: 0.009095221757888794\n",
      "iteration 4159, loss: 0.008024653419852257\n",
      "iteration 4160, loss: 0.008298821747303009\n",
      "iteration 4161, loss: 0.007559679914265871\n",
      "iteration 4162, loss: 0.00864595826715231\n",
      "iteration 4163, loss: 0.008798804134130478\n",
      "iteration 4164, loss: 0.008125919848680496\n",
      "iteration 4165, loss: 0.007695688400417566\n",
      "iteration 4166, loss: 0.009558095596730709\n",
      "iteration 4167, loss: 0.008430490270256996\n",
      "iteration 4168, loss: 0.007544318679720163\n",
      "iteration 4169, loss: 0.009117919951677322\n",
      "iteration 4170, loss: 0.009957997128367424\n",
      "iteration 4171, loss: 0.008514642715454102\n",
      "iteration 4172, loss: 0.0076799336820840836\n",
      "iteration 4173, loss: 0.008240203373134136\n",
      "iteration 4174, loss: 0.008729392662644386\n",
      "iteration 4175, loss: 0.007886579260230064\n",
      "iteration 4176, loss: 0.010234092362225056\n",
      "iteration 4177, loss: 0.010300328955054283\n",
      "iteration 4178, loss: 0.009308598935604095\n",
      "iteration 4179, loss: 0.009754743427038193\n",
      "iteration 4180, loss: 0.008897682651877403\n",
      "iteration 4181, loss: 0.008774738758802414\n",
      "iteration 4182, loss: 0.007604791782796383\n",
      "iteration 4183, loss: 0.008734069764614105\n",
      "iteration 4184, loss: 0.008330868557095528\n",
      "iteration 4185, loss: 0.009404201991856098\n",
      "iteration 4186, loss: 0.008821032010018826\n",
      "iteration 4187, loss: 0.008200345560908318\n",
      "iteration 4188, loss: 0.0074875326827168465\n",
      "iteration 4189, loss: 0.009331795386970043\n",
      "iteration 4190, loss: 0.007988909259438515\n",
      "iteration 4191, loss: 0.008017430081963539\n",
      "iteration 4192, loss: 0.009225237183272839\n",
      "iteration 4193, loss: 0.007895476184785366\n",
      "iteration 4194, loss: 0.008780512027442455\n",
      "iteration 4195, loss: 0.009213553741574287\n",
      "iteration 4196, loss: 0.009928221814334393\n",
      "iteration 4197, loss: 0.00848688930273056\n",
      "iteration 4198, loss: 0.00793530885130167\n",
      "iteration 4199, loss: 0.007788260001689196\n",
      "iteration 4200, loss: 0.01045109611004591\n",
      "iteration 4201, loss: 0.009056959301233292\n",
      "iteration 4202, loss: 0.009149586781859398\n",
      "iteration 4203, loss: 0.009989220649003983\n",
      "iteration 4204, loss: 0.008126088418066502\n",
      "iteration 4205, loss: 0.007946539670228958\n",
      "iteration 4206, loss: 0.00952119193971157\n",
      "iteration 4207, loss: 0.008086070418357849\n",
      "iteration 4208, loss: 0.009568816050887108\n",
      "iteration 4209, loss: 0.008728362619876862\n",
      "iteration 4210, loss: 0.008364473469555378\n",
      "iteration 4211, loss: 0.008993188850581646\n",
      "iteration 4212, loss: 0.008860334753990173\n",
      "iteration 4213, loss: 0.009191323071718216\n",
      "iteration 4214, loss: 0.00815562903881073\n",
      "iteration 4215, loss: 0.00826138537377119\n",
      "iteration 4216, loss: 0.009620902128517628\n",
      "iteration 4217, loss: 0.008631179109215736\n",
      "iteration 4218, loss: 0.009231503121554852\n",
      "iteration 4219, loss: 0.009505687281489372\n",
      "iteration 4220, loss: 0.010473694652318954\n",
      "iteration 4221, loss: 0.008595063351094723\n",
      "iteration 4222, loss: 0.00841246172785759\n",
      "iteration 4223, loss: 0.009042749181389809\n",
      "iteration 4224, loss: 0.00802072323858738\n",
      "iteration 4225, loss: 0.008773993700742722\n",
      "iteration 4226, loss: 0.008132291957736015\n",
      "iteration 4227, loss: 0.008197368122637272\n",
      "iteration 4228, loss: 0.010212106630206108\n",
      "iteration 4229, loss: 0.008465597406029701\n",
      "iteration 4230, loss: 0.00766773009672761\n",
      "iteration 4231, loss: 0.010495029389858246\n",
      "iteration 4232, loss: 0.00959726981818676\n",
      "iteration 4233, loss: 0.008811780251562595\n",
      "iteration 4234, loss: 0.008380891755223274\n",
      "iteration 4235, loss: 0.011157205328345299\n",
      "iteration 4236, loss: 0.008558030240237713\n",
      "iteration 4237, loss: 0.009534548968076706\n",
      "iteration 4238, loss: 0.009846845641732216\n",
      "iteration 4239, loss: 0.009840093553066254\n",
      "iteration 4240, loss: 0.009765841998159885\n",
      "iteration 4241, loss: 0.008534317836165428\n",
      "iteration 4242, loss: 0.008851355873048306\n",
      "iteration 4243, loss: 0.009768958203494549\n",
      "iteration 4244, loss: 0.012028268538415432\n",
      "iteration 4245, loss: 0.00967393722385168\n",
      "iteration 4246, loss: 0.010285010561347008\n",
      "iteration 4247, loss: 0.009051407687366009\n",
      "iteration 4248, loss: 0.008772730827331543\n",
      "iteration 4249, loss: 0.008656559512019157\n",
      "iteration 4250, loss: 0.008063016459345818\n",
      "iteration 4251, loss: 0.008908568881452084\n",
      "iteration 4252, loss: 0.0068270014598965645\n",
      "iteration 4253, loss: 0.009800625964999199\n",
      "iteration 4254, loss: 0.008609432727098465\n",
      "iteration 4255, loss: 0.008494306355714798\n",
      "iteration 4256, loss: 0.008677346631884575\n",
      "iteration 4257, loss: 0.008861828595399857\n",
      "iteration 4258, loss: 0.00813380442559719\n",
      "iteration 4259, loss: 0.009243510663509369\n",
      "iteration 4260, loss: 0.009387089870870113\n",
      "iteration 4261, loss: 0.007616139482706785\n",
      "iteration 4262, loss: 0.009350378066301346\n",
      "iteration 4263, loss: 0.009895809926092625\n",
      "iteration 4264, loss: 0.009614004753530025\n",
      "iteration 4265, loss: 0.009058989584445953\n",
      "iteration 4266, loss: 0.008022083900868893\n",
      "iteration 4267, loss: 0.0078864311799407\n",
      "iteration 4268, loss: 0.0075219240970909595\n",
      "iteration 4269, loss: 0.008503764867782593\n",
      "iteration 4270, loss: 0.008405844680964947\n",
      "iteration 4271, loss: 0.007605907041579485\n",
      "iteration 4272, loss: 0.010334096848964691\n",
      "iteration 4273, loss: 0.009439298883080482\n",
      "iteration 4274, loss: 0.008681872859597206\n",
      "iteration 4275, loss: 0.008893723599612713\n",
      "iteration 4276, loss: 0.009190145879983902\n",
      "iteration 4277, loss: 0.008592644706368446\n",
      "iteration 4278, loss: 0.008813271299004555\n",
      "iteration 4279, loss: 0.009858373552560806\n",
      "iteration 4280, loss: 0.010037055239081383\n",
      "iteration 4281, loss: 0.009286830201745033\n",
      "iteration 4282, loss: 0.009236601181328297\n",
      "iteration 4283, loss: 0.007949072867631912\n",
      "iteration 4284, loss: 0.00827895849943161\n",
      "iteration 4285, loss: 0.009129860438406467\n",
      "iteration 4286, loss: 0.009041027165949345\n",
      "iteration 4287, loss: 0.00961928628385067\n",
      "iteration 4288, loss: 0.00843876414000988\n",
      "iteration 4289, loss: 0.007838290184736252\n",
      "iteration 4290, loss: 0.007879512384533882\n",
      "iteration 4291, loss: 0.007528162561357021\n",
      "iteration 4292, loss: 0.009134432300925255\n",
      "iteration 4293, loss: 0.008236544206738472\n",
      "iteration 4294, loss: 0.009530621580779552\n",
      "iteration 4295, loss: 0.008529688231647015\n",
      "iteration 4296, loss: 0.009609315544366837\n",
      "iteration 4297, loss: 0.00875844620168209\n",
      "iteration 4298, loss: 0.009395299479365349\n",
      "iteration 4299, loss: 0.010133689269423485\n",
      "iteration 4300, loss: 0.009392370469868183\n",
      "iteration 4301, loss: 0.009111562743782997\n",
      "iteration 4302, loss: 0.008982095867395401\n",
      "iteration 4303, loss: 0.008536908775568008\n",
      "iteration 4304, loss: 0.00838114321231842\n",
      "iteration 4305, loss: 0.008371329866349697\n",
      "iteration 4306, loss: 0.01019933633506298\n",
      "iteration 4307, loss: 0.008246508426964283\n",
      "iteration 4308, loss: 0.009289069101214409\n",
      "iteration 4309, loss: 0.008852820843458176\n",
      "iteration 4310, loss: 0.00868268869817257\n",
      "iteration 4311, loss: 0.009265953674912453\n",
      "iteration 4312, loss: 0.00908510759472847\n",
      "iteration 4313, loss: 0.008970213122665882\n",
      "iteration 4314, loss: 0.008584892377257347\n",
      "iteration 4315, loss: 0.009810489602386951\n",
      "iteration 4316, loss: 0.008409173227846622\n",
      "iteration 4317, loss: 0.009393732994794846\n",
      "iteration 4318, loss: 0.008157947100698948\n",
      "iteration 4319, loss: 0.008607106283307076\n",
      "iteration 4320, loss: 0.007905509322881699\n",
      "iteration 4321, loss: 0.009511519223451614\n",
      "iteration 4322, loss: 0.007717703003436327\n",
      "iteration 4323, loss: 0.008375849574804306\n",
      "iteration 4324, loss: 0.008386719971895218\n",
      "iteration 4325, loss: 0.009983832016587257\n",
      "iteration 4326, loss: 0.009481941349804401\n",
      "iteration 4327, loss: 0.009281577542424202\n",
      "iteration 4328, loss: 0.008133643306791782\n",
      "iteration 4329, loss: 0.008772583678364754\n",
      "iteration 4330, loss: 0.009533691219985485\n",
      "iteration 4331, loss: 0.008233191445469856\n",
      "iteration 4332, loss: 0.00874749943614006\n",
      "iteration 4333, loss: 0.010378802195191383\n",
      "iteration 4334, loss: 0.008623527362942696\n",
      "iteration 4335, loss: 0.0096931466832757\n",
      "iteration 4336, loss: 0.00908692367374897\n",
      "iteration 4337, loss: 0.00785523559898138\n",
      "iteration 4338, loss: 0.009359223768115044\n",
      "iteration 4339, loss: 0.009138722904026508\n",
      "iteration 4340, loss: 0.009136997163295746\n",
      "iteration 4341, loss: 0.008501365780830383\n",
      "iteration 4342, loss: 0.009032465517520905\n",
      "iteration 4343, loss: 0.008029918186366558\n",
      "iteration 4344, loss: 0.00899421889334917\n",
      "iteration 4345, loss: 0.008066099137067795\n",
      "iteration 4346, loss: 0.010534917935729027\n",
      "iteration 4347, loss: 0.008972708135843277\n",
      "iteration 4348, loss: 0.010092043317854404\n",
      "iteration 4349, loss: 0.009753306396305561\n",
      "iteration 4350, loss: 0.010183156467974186\n",
      "iteration 4351, loss: 0.008227052167057991\n",
      "iteration 4352, loss: 0.010571419261395931\n",
      "iteration 4353, loss: 0.007487356662750244\n",
      "iteration 4354, loss: 0.009723713621497154\n",
      "iteration 4355, loss: 0.00879983976483345\n",
      "iteration 4356, loss: 0.00967609416693449\n",
      "iteration 4357, loss: 0.0091317780315876\n",
      "iteration 4358, loss: 0.00845669861882925\n",
      "iteration 4359, loss: 0.008949149399995804\n",
      "iteration 4360, loss: 0.00983186811208725\n",
      "iteration 4361, loss: 0.009948156774044037\n",
      "iteration 4362, loss: 0.008832279592752457\n",
      "iteration 4363, loss: 0.007999489083886147\n",
      "iteration 4364, loss: 0.009038761258125305\n",
      "iteration 4365, loss: 0.008714225143194199\n",
      "iteration 4366, loss: 0.00882488489151001\n",
      "iteration 4367, loss: 0.009549909271299839\n",
      "iteration 4368, loss: 0.00939388107508421\n",
      "iteration 4369, loss: 0.009157939814031124\n",
      "iteration 4370, loss: 0.007808255963027477\n",
      "iteration 4371, loss: 0.008266113698482513\n",
      "iteration 4372, loss: 0.008902854286134243\n",
      "iteration 4373, loss: 0.008440024219453335\n",
      "iteration 4374, loss: 0.00814719870686531\n",
      "iteration 4375, loss: 0.008928696624934673\n",
      "iteration 4376, loss: 0.008888473734259605\n",
      "iteration 4377, loss: 0.009107181802392006\n",
      "iteration 4378, loss: 0.009195378050208092\n",
      "iteration 4379, loss: 0.00859840027987957\n",
      "iteration 4380, loss: 0.0089160967618227\n",
      "iteration 4381, loss: 0.009050089865922928\n",
      "iteration 4382, loss: 0.00780278816819191\n",
      "iteration 4383, loss: 0.008609229698777199\n",
      "iteration 4384, loss: 0.009052090346813202\n",
      "iteration 4385, loss: 0.010257904417812824\n",
      "iteration 4386, loss: 0.008581790141761303\n",
      "iteration 4387, loss: 0.009214586578309536\n",
      "iteration 4388, loss: 0.009907441213726997\n",
      "iteration 4389, loss: 0.008062132634222507\n",
      "iteration 4390, loss: 0.0091539416462183\n",
      "iteration 4391, loss: 0.008248133584856987\n",
      "iteration 4392, loss: 0.006979765370488167\n",
      "iteration 4393, loss: 0.009718820452690125\n",
      "iteration 4394, loss: 0.008323268964886665\n",
      "iteration 4395, loss: 0.009161152876913548\n",
      "iteration 4396, loss: 0.009176088497042656\n",
      "iteration 4397, loss: 0.008162952959537506\n",
      "iteration 4398, loss: 0.00916203111410141\n",
      "iteration 4399, loss: 0.008641471154987812\n",
      "iteration 4400, loss: 0.01105673797428608\n",
      "iteration 4401, loss: 0.007564089726656675\n",
      "iteration 4402, loss: 0.008662248961627483\n",
      "iteration 4403, loss: 0.00863298773765564\n",
      "iteration 4404, loss: 0.009388837032020092\n",
      "iteration 4405, loss: 0.00884849950671196\n",
      "iteration 4406, loss: 0.008648955263197422\n",
      "iteration 4407, loss: 0.007930094376206398\n",
      "iteration 4408, loss: 0.008002986200153828\n",
      "iteration 4409, loss: 0.0075799631886184216\n",
      "iteration 4410, loss: 0.008610743097960949\n",
      "iteration 4411, loss: 0.009552443400025368\n",
      "iteration 4412, loss: 0.00761131476610899\n",
      "iteration 4413, loss: 0.00725040677934885\n",
      "iteration 4414, loss: 0.008198264054954052\n",
      "iteration 4415, loss: 0.007577708922326565\n",
      "iteration 4416, loss: 0.007474221289157867\n",
      "iteration 4417, loss: 0.009073515422642231\n",
      "iteration 4418, loss: 0.008836952038109303\n",
      "iteration 4419, loss: 0.008401868864893913\n",
      "iteration 4420, loss: 0.00889595691114664\n",
      "iteration 4421, loss: 0.00788951013237238\n",
      "iteration 4422, loss: 0.008297618478536606\n",
      "iteration 4423, loss: 0.008163757622241974\n",
      "iteration 4424, loss: 0.009033769369125366\n",
      "iteration 4425, loss: 0.007999126799404621\n",
      "iteration 4426, loss: 0.008051326498389244\n",
      "iteration 4427, loss: 0.007465560920536518\n",
      "iteration 4428, loss: 0.008055174723267555\n",
      "iteration 4429, loss: 0.008389079943299294\n",
      "iteration 4430, loss: 0.007828185334801674\n",
      "iteration 4431, loss: 0.007824273779988289\n",
      "iteration 4432, loss: 0.006860262248665094\n",
      "iteration 4433, loss: 0.009035550057888031\n",
      "iteration 4434, loss: 0.008287252858281136\n",
      "iteration 4435, loss: 0.00825902633368969\n",
      "iteration 4436, loss: 0.008399462327361107\n",
      "iteration 4437, loss: 0.0086622703820467\n",
      "iteration 4438, loss: 0.008005404844880104\n",
      "iteration 4439, loss: 0.009335501119494438\n",
      "iteration 4440, loss: 0.008921218104660511\n",
      "iteration 4441, loss: 0.009290501475334167\n",
      "iteration 4442, loss: 0.009215614758431911\n",
      "iteration 4443, loss: 0.007830159738659859\n",
      "iteration 4444, loss: 0.00812619924545288\n",
      "iteration 4445, loss: 0.008239100687205791\n",
      "iteration 4446, loss: 0.009583145380020142\n",
      "iteration 4447, loss: 0.007848486304283142\n",
      "iteration 4448, loss: 0.008888248354196548\n",
      "iteration 4449, loss: 0.007458132691681385\n",
      "iteration 4450, loss: 0.008789120241999626\n",
      "iteration 4451, loss: 0.00795734766870737\n",
      "iteration 4452, loss: 0.008208321407437325\n",
      "iteration 4453, loss: 0.00826993864029646\n",
      "iteration 4454, loss: 0.008061205968260765\n",
      "iteration 4455, loss: 0.00744385551661253\n",
      "iteration 4456, loss: 0.008765729144215584\n",
      "iteration 4457, loss: 0.008087867870926857\n",
      "iteration 4458, loss: 0.009447971358895302\n",
      "iteration 4459, loss: 0.00822210218757391\n",
      "iteration 4460, loss: 0.007578975521028042\n",
      "iteration 4461, loss: 0.00851331278681755\n",
      "iteration 4462, loss: 0.008706903085112572\n",
      "iteration 4463, loss: 0.008608504198491573\n",
      "iteration 4464, loss: 0.0067665306851267815\n",
      "iteration 4465, loss: 0.009198580868542194\n",
      "iteration 4466, loss: 0.008366869762539864\n",
      "iteration 4467, loss: 0.008424593135714531\n",
      "iteration 4468, loss: 0.00797591544687748\n",
      "iteration 4469, loss: 0.007661978714168072\n",
      "iteration 4470, loss: 0.00785159319639206\n",
      "iteration 4471, loss: 0.007701173424720764\n",
      "iteration 4472, loss: 0.00955885648727417\n",
      "iteration 4473, loss: 0.007720466703176498\n",
      "iteration 4474, loss: 0.0096951425075531\n",
      "iteration 4475, loss: 0.0074113281443715096\n",
      "iteration 4476, loss: 0.008023548871278763\n",
      "iteration 4477, loss: 0.008049816824495792\n",
      "iteration 4478, loss: 0.008258224464952946\n",
      "iteration 4479, loss: 0.007575749419629574\n",
      "iteration 4480, loss: 0.008488684892654419\n",
      "iteration 4481, loss: 0.008394408971071243\n",
      "iteration 4482, loss: 0.00708753289654851\n",
      "iteration 4483, loss: 0.007309188600629568\n",
      "iteration 4484, loss: 0.00811530277132988\n",
      "iteration 4485, loss: 0.008987711742520332\n",
      "iteration 4486, loss: 0.007852953858673573\n",
      "iteration 4487, loss: 0.008006002753973007\n",
      "iteration 4488, loss: 0.008679086342453957\n",
      "iteration 4489, loss: 0.008942202664911747\n",
      "iteration 4490, loss: 0.009000889956951141\n",
      "iteration 4491, loss: 0.008493166416883469\n",
      "iteration 4492, loss: 0.01121445931494236\n",
      "iteration 4493, loss: 0.009536674246191978\n",
      "iteration 4494, loss: 0.009201427921652794\n",
      "iteration 4495, loss: 0.009224219247698784\n",
      "iteration 4496, loss: 0.008053408935666084\n",
      "iteration 4497, loss: 0.007363147102296352\n",
      "iteration 4498, loss: 0.007784939371049404\n",
      "iteration 4499, loss: 0.00747610442340374\n",
      "iteration 4500, loss: 0.007560063153505325\n",
      "iteration 4501, loss: 0.00854801107198\n",
      "iteration 4502, loss: 0.009040400385856628\n",
      "iteration 4503, loss: 0.008230944164097309\n",
      "iteration 4504, loss: 0.008471237495541573\n",
      "iteration 4505, loss: 0.007390229031443596\n",
      "iteration 4506, loss: 0.009062724187970161\n",
      "iteration 4507, loss: 0.008978339843451977\n",
      "iteration 4508, loss: 0.008156907744705677\n",
      "iteration 4509, loss: 0.007819194346666336\n",
      "iteration 4510, loss: 0.007109097670763731\n",
      "iteration 4511, loss: 0.008853096514940262\n",
      "iteration 4512, loss: 0.008414046838879585\n",
      "iteration 4513, loss: 0.008581260219216347\n",
      "iteration 4514, loss: 0.009147638455033302\n",
      "iteration 4515, loss: 0.008496277034282684\n",
      "iteration 4516, loss: 0.007009259890764952\n",
      "iteration 4517, loss: 0.0073172058910131454\n",
      "iteration 4518, loss: 0.008136298507452011\n",
      "iteration 4519, loss: 0.008281008340418339\n",
      "iteration 4520, loss: 0.007727149873971939\n",
      "iteration 4521, loss: 0.008640581741929054\n",
      "iteration 4522, loss: 0.008310411125421524\n",
      "iteration 4523, loss: 0.008670104667544365\n",
      "iteration 4524, loss: 0.007570519112050533\n",
      "iteration 4525, loss: 0.008704888634383678\n",
      "iteration 4526, loss: 0.009222937747836113\n",
      "iteration 4527, loss: 0.00821345578879118\n",
      "iteration 4528, loss: 0.008305572904646397\n",
      "iteration 4529, loss: 0.006464336067438126\n",
      "iteration 4530, loss: 0.008233577944338322\n",
      "iteration 4531, loss: 0.008450053632259369\n",
      "iteration 4532, loss: 0.008308690041303635\n",
      "iteration 4533, loss: 0.008709782734513283\n",
      "iteration 4534, loss: 0.0076583935879170895\n",
      "iteration 4535, loss: 0.007215629331767559\n",
      "iteration 4536, loss: 0.009140674956142902\n",
      "iteration 4537, loss: 0.009197736158967018\n",
      "iteration 4538, loss: 0.009779862128198147\n",
      "iteration 4539, loss: 0.008297186344861984\n",
      "iteration 4540, loss: 0.008430615067481995\n",
      "iteration 4541, loss: 0.009606443345546722\n",
      "iteration 4542, loss: 0.01047218032181263\n",
      "iteration 4543, loss: 0.00909555982798338\n",
      "iteration 4544, loss: 0.008412969298660755\n",
      "iteration 4545, loss: 0.009328997693955898\n",
      "iteration 4546, loss: 0.00908331386744976\n",
      "iteration 4547, loss: 0.0092244204133749\n",
      "iteration 4548, loss: 0.008473669178783894\n",
      "iteration 4549, loss: 0.010880803689360619\n",
      "iteration 4550, loss: 0.009334583766758442\n",
      "iteration 4551, loss: 0.008380990475416183\n",
      "iteration 4552, loss: 0.00907257478684187\n",
      "iteration 4553, loss: 0.008018411695957184\n",
      "iteration 4554, loss: 0.008966200985014439\n",
      "iteration 4555, loss: 0.007890182547271252\n",
      "iteration 4556, loss: 0.008351530879735947\n",
      "iteration 4557, loss: 0.007456878200173378\n",
      "iteration 4558, loss: 0.009500288404524326\n",
      "iteration 4559, loss: 0.009162567555904388\n",
      "iteration 4560, loss: 0.007726282812654972\n",
      "iteration 4561, loss: 0.007511243224143982\n",
      "iteration 4562, loss: 0.008480376563966274\n",
      "iteration 4563, loss: 0.009348994120955467\n",
      "iteration 4564, loss: 0.00922038871794939\n",
      "iteration 4565, loss: 0.008338489569723606\n",
      "iteration 4566, loss: 0.006740101613104343\n",
      "iteration 4567, loss: 0.008691854774951935\n",
      "iteration 4568, loss: 0.007504329085350037\n",
      "iteration 4569, loss: 0.007163854315876961\n",
      "iteration 4570, loss: 0.007132430095225573\n",
      "iteration 4571, loss: 0.007584265898913145\n",
      "iteration 4572, loss: 0.008139602839946747\n",
      "iteration 4573, loss: 0.008632754907011986\n",
      "iteration 4574, loss: 0.008208039216697216\n",
      "iteration 4575, loss: 0.008208155632019043\n",
      "iteration 4576, loss: 0.007925428450107574\n",
      "iteration 4577, loss: 0.007838010787963867\n",
      "iteration 4578, loss: 0.008676798082888126\n",
      "iteration 4579, loss: 0.007319862022995949\n",
      "iteration 4580, loss: 0.009582634083926678\n",
      "iteration 4581, loss: 0.00820070132613182\n",
      "iteration 4582, loss: 0.008582581765949726\n",
      "iteration 4583, loss: 0.00823720172047615\n",
      "iteration 4584, loss: 0.008291826583445072\n",
      "iteration 4585, loss: 0.009109661914408207\n",
      "iteration 4586, loss: 0.008980260230600834\n",
      "iteration 4587, loss: 0.008135402575135231\n",
      "iteration 4588, loss: 0.007555102929472923\n",
      "iteration 4589, loss: 0.008470974862575531\n",
      "iteration 4590, loss: 0.007823626510798931\n",
      "iteration 4591, loss: 0.009762427769601345\n",
      "iteration 4592, loss: 0.009008289314806461\n",
      "iteration 4593, loss: 0.006896876730024815\n",
      "iteration 4594, loss: 0.008205743506550789\n",
      "iteration 4595, loss: 0.007902011275291443\n",
      "iteration 4596, loss: 0.00906848069280386\n",
      "iteration 4597, loss: 0.008419675752520561\n",
      "iteration 4598, loss: 0.009187305346131325\n",
      "iteration 4599, loss: 0.009106039069592953\n",
      "iteration 4600, loss: 0.008777029812335968\n",
      "iteration 4601, loss: 0.00878392718732357\n",
      "iteration 4602, loss: 0.007892884314060211\n",
      "iteration 4603, loss: 0.009561585262417793\n",
      "iteration 4604, loss: 0.008651632815599442\n",
      "iteration 4605, loss: 0.01029264461249113\n",
      "iteration 4606, loss: 0.006976582109928131\n",
      "iteration 4607, loss: 0.0076507870107889175\n",
      "iteration 4608, loss: 0.008820364251732826\n",
      "iteration 4609, loss: 0.007874755188822746\n",
      "iteration 4610, loss: 0.008237519301474094\n",
      "iteration 4611, loss: 0.007651562802493572\n",
      "iteration 4612, loss: 0.00887577049434185\n",
      "iteration 4613, loss: 0.007325577549636364\n",
      "iteration 4614, loss: 0.006773075088858604\n",
      "iteration 4615, loss: 0.008181903511285782\n",
      "iteration 4616, loss: 0.007458369247615337\n",
      "iteration 4617, loss: 0.007885298691689968\n",
      "iteration 4618, loss: 0.008081438951194286\n",
      "iteration 4619, loss: 0.009072443470358849\n",
      "iteration 4620, loss: 0.009130382910370827\n",
      "iteration 4621, loss: 0.007985986769199371\n",
      "iteration 4622, loss: 0.008271796628832817\n",
      "iteration 4623, loss: 0.007664985489100218\n",
      "iteration 4624, loss: 0.009074763394892216\n",
      "iteration 4625, loss: 0.00853357370942831\n",
      "iteration 4626, loss: 0.007631514221429825\n",
      "iteration 4627, loss: 0.008141160942614079\n",
      "iteration 4628, loss: 0.008879485540091991\n",
      "iteration 4629, loss: 0.008227640762925148\n",
      "iteration 4630, loss: 0.00856719072908163\n",
      "iteration 4631, loss: 0.008857741951942444\n",
      "iteration 4632, loss: 0.006896369159221649\n",
      "iteration 4633, loss: 0.007383138872683048\n",
      "iteration 4634, loss: 0.006689597852528095\n",
      "iteration 4635, loss: 0.0064011551439762115\n",
      "iteration 4636, loss: 0.007776109967380762\n",
      "iteration 4637, loss: 0.00934030395001173\n",
      "iteration 4638, loss: 0.008355272933840752\n",
      "iteration 4639, loss: 0.009524514898657799\n",
      "iteration 4640, loss: 0.008378945291042328\n",
      "iteration 4641, loss: 0.007861903868615627\n",
      "iteration 4642, loss: 0.00949142500758171\n",
      "iteration 4643, loss: 0.007876472547650337\n",
      "iteration 4644, loss: 0.007105781696736813\n",
      "iteration 4645, loss: 0.007855814881622791\n",
      "iteration 4646, loss: 0.007762342691421509\n",
      "iteration 4647, loss: 0.007110784761607647\n",
      "iteration 4648, loss: 0.005858900956809521\n",
      "iteration 4649, loss: 0.007488653063774109\n",
      "iteration 4650, loss: 0.007684121839702129\n",
      "iteration 4651, loss: 0.007486982271075249\n",
      "iteration 4652, loss: 0.007949668914079666\n",
      "iteration 4653, loss: 0.007351852487772703\n",
      "iteration 4654, loss: 0.008516666479408741\n",
      "iteration 4655, loss: 0.007411054335534573\n",
      "iteration 4656, loss: 0.0076102688908576965\n",
      "iteration 4657, loss: 0.007603340782225132\n",
      "iteration 4658, loss: 0.007922740653157234\n",
      "iteration 4659, loss: 0.008222626522183418\n",
      "iteration 4660, loss: 0.0076903486624360085\n",
      "iteration 4661, loss: 0.007241333834826946\n",
      "iteration 4662, loss: 0.005436774808913469\n",
      "iteration 4663, loss: 0.007207297720015049\n",
      "iteration 4664, loss: 0.008194094523787498\n",
      "iteration 4665, loss: 0.007471727207303047\n",
      "iteration 4666, loss: 0.00852827075868845\n",
      "iteration 4667, loss: 0.008043725974857807\n",
      "iteration 4668, loss: 0.00705467676743865\n",
      "iteration 4669, loss: 0.008654288947582245\n",
      "iteration 4670, loss: 0.009152225218713284\n",
      "iteration 4671, loss: 0.008305991068482399\n",
      "iteration 4672, loss: 0.0085978452116251\n",
      "iteration 4673, loss: 0.00836214516311884\n",
      "iteration 4674, loss: 0.007063296157866716\n",
      "iteration 4675, loss: 0.008335430175065994\n",
      "iteration 4676, loss: 0.007643327582627535\n",
      "iteration 4677, loss: 0.008945945650339127\n",
      "iteration 4678, loss: 0.00963388942182064\n",
      "iteration 4679, loss: 0.006855736020952463\n",
      "iteration 4680, loss: 0.007786605507135391\n",
      "iteration 4681, loss: 0.009404003620147705\n",
      "iteration 4682, loss: 0.00609293207526207\n",
      "iteration 4683, loss: 0.007856065407395363\n",
      "iteration 4684, loss: 0.008027124218642712\n",
      "iteration 4685, loss: 0.007314503658562899\n",
      "iteration 4686, loss: 0.006874074228107929\n",
      "iteration 4687, loss: 0.00802053976804018\n",
      "iteration 4688, loss: 0.008557848632335663\n",
      "iteration 4689, loss: 0.007915815338492393\n",
      "iteration 4690, loss: 0.008191350847482681\n",
      "iteration 4691, loss: 0.007618597708642483\n",
      "iteration 4692, loss: 0.008163808844983578\n",
      "iteration 4693, loss: 0.0073865726590156555\n",
      "iteration 4694, loss: 0.007129404693841934\n",
      "iteration 4695, loss: 0.006766415201127529\n",
      "iteration 4696, loss: 0.008517147041857243\n",
      "iteration 4697, loss: 0.0067736003547906876\n",
      "iteration 4698, loss: 0.009342944249510765\n",
      "iteration 4699, loss: 0.008770052343606949\n",
      "iteration 4700, loss: 0.008406855165958405\n",
      "iteration 4701, loss: 0.0090556051582098\n",
      "iteration 4702, loss: 0.0073503125458955765\n",
      "iteration 4703, loss: 0.007932264357805252\n",
      "iteration 4704, loss: 0.00789659470319748\n",
      "iteration 4705, loss: 0.007605733349919319\n",
      "iteration 4706, loss: 0.007785554975271225\n",
      "iteration 4707, loss: 0.008581351488828659\n",
      "iteration 4708, loss: 0.009014416486024857\n",
      "iteration 4709, loss: 0.008380018174648285\n",
      "iteration 4710, loss: 0.007290127221494913\n",
      "iteration 4711, loss: 0.008320728316903114\n",
      "iteration 4712, loss: 0.009145883843302727\n",
      "iteration 4713, loss: 0.007714756764471531\n",
      "iteration 4714, loss: 0.008885564282536507\n",
      "iteration 4715, loss: 0.009597674012184143\n",
      "iteration 4716, loss: 0.008206252939999104\n",
      "iteration 4717, loss: 0.009034445509314537\n",
      "iteration 4718, loss: 0.008415790274739265\n",
      "iteration 4719, loss: 0.0082488302141428\n",
      "iteration 4720, loss: 0.008480336517095566\n",
      "iteration 4721, loss: 0.007244533393532038\n",
      "iteration 4722, loss: 0.007232135161757469\n",
      "iteration 4723, loss: 0.00749661959707737\n",
      "iteration 4724, loss: 0.006759660318493843\n",
      "iteration 4725, loss: 0.0074612535536289215\n",
      "iteration 4726, loss: 0.006773070897907019\n",
      "iteration 4727, loss: 0.00811788160353899\n",
      "iteration 4728, loss: 0.0076223984360694885\n",
      "iteration 4729, loss: 0.008410779759287834\n",
      "iteration 4730, loss: 0.007413147948682308\n",
      "iteration 4731, loss: 0.007780484389513731\n",
      "iteration 4732, loss: 0.00894884206354618\n",
      "iteration 4733, loss: 0.008184146136045456\n",
      "iteration 4734, loss: 0.008059990592300892\n",
      "iteration 4735, loss: 0.007384743541479111\n",
      "iteration 4736, loss: 0.007369662635028362\n",
      "iteration 4737, loss: 0.006565647199749947\n",
      "iteration 4738, loss: 0.008245164528489113\n",
      "iteration 4739, loss: 0.00683982390910387\n",
      "iteration 4740, loss: 0.007932567968964577\n",
      "iteration 4741, loss: 0.008914679288864136\n",
      "iteration 4742, loss: 0.008104195818305016\n",
      "iteration 4743, loss: 0.009770533069968224\n",
      "iteration 4744, loss: 0.009861456230282784\n",
      "iteration 4745, loss: 0.008256309665739536\n",
      "iteration 4746, loss: 0.00874278973788023\n",
      "iteration 4747, loss: 0.008799057453870773\n",
      "iteration 4748, loss: 0.008322201669216156\n",
      "iteration 4749, loss: 0.0078337537124753\n",
      "iteration 4750, loss: 0.007662915159016848\n",
      "iteration 4751, loss: 0.008848573081195354\n",
      "iteration 4752, loss: 0.008853210136294365\n",
      "iteration 4753, loss: 0.006830152124166489\n",
      "iteration 4754, loss: 0.008928075432777405\n",
      "iteration 4755, loss: 0.008214734494686127\n",
      "iteration 4756, loss: 0.008066393435001373\n",
      "iteration 4757, loss: 0.00814276747405529\n",
      "iteration 4758, loss: 0.00800427794456482\n",
      "iteration 4759, loss: 0.008443851955235004\n",
      "iteration 4760, loss: 0.008627776987850666\n",
      "iteration 4761, loss: 0.008504394441843033\n",
      "iteration 4762, loss: 0.007884291931986809\n",
      "iteration 4763, loss: 0.008083839900791645\n",
      "iteration 4764, loss: 0.008049344643950462\n",
      "iteration 4765, loss: 0.008402038365602493\n",
      "iteration 4766, loss: 0.008450545370578766\n",
      "iteration 4767, loss: 0.008068157359957695\n",
      "iteration 4768, loss: 0.008423659950494766\n",
      "iteration 4769, loss: 0.008036764338612556\n",
      "iteration 4770, loss: 0.0076340362429618835\n",
      "iteration 4771, loss: 0.007647261023521423\n",
      "iteration 4772, loss: 0.008593790233135223\n",
      "iteration 4773, loss: 0.00784628838300705\n",
      "iteration 4774, loss: 0.008882449939846992\n",
      "iteration 4775, loss: 0.008296268992125988\n",
      "iteration 4776, loss: 0.007388724479824305\n",
      "iteration 4777, loss: 0.006525458302348852\n",
      "iteration 4778, loss: 0.00725996308028698\n",
      "iteration 4779, loss: 0.007196091581135988\n",
      "iteration 4780, loss: 0.008334037847816944\n",
      "iteration 4781, loss: 0.008776864036917686\n",
      "iteration 4782, loss: 0.007093472871929407\n",
      "iteration 4783, loss: 0.007571178488433361\n",
      "iteration 4784, loss: 0.0074473656713962555\n",
      "iteration 4785, loss: 0.007395257242023945\n",
      "iteration 4786, loss: 0.007272561080753803\n",
      "iteration 4787, loss: 0.008783186785876751\n",
      "iteration 4788, loss: 0.00809682160615921\n",
      "iteration 4789, loss: 0.00807437114417553\n",
      "iteration 4790, loss: 0.009287489578127861\n",
      "iteration 4791, loss: 0.007233417127281427\n",
      "iteration 4792, loss: 0.009778177365660667\n",
      "iteration 4793, loss: 0.007460946217179298\n",
      "iteration 4794, loss: 0.00917067751288414\n",
      "iteration 4795, loss: 0.006549090147018433\n",
      "iteration 4796, loss: 0.006644879933446646\n",
      "iteration 4797, loss: 0.007016128860414028\n",
      "iteration 4798, loss: 0.00772933941334486\n",
      "iteration 4799, loss: 0.008927457965910435\n",
      "iteration 4800, loss: 0.008050777949392796\n",
      "iteration 4801, loss: 0.00797835923731327\n",
      "iteration 4802, loss: 0.0064513664692640305\n",
      "iteration 4803, loss: 0.008904919028282166\n",
      "iteration 4804, loss: 0.009024935774505138\n",
      "iteration 4805, loss: 0.007965407334268093\n",
      "iteration 4806, loss: 0.007097616791725159\n",
      "iteration 4807, loss: 0.007660870440304279\n",
      "iteration 4808, loss: 0.0070040104910731316\n",
      "iteration 4809, loss: 0.00842445157468319\n",
      "iteration 4810, loss: 0.009239790961146355\n",
      "iteration 4811, loss: 0.007115030195564032\n",
      "iteration 4812, loss: 0.007864633575081825\n",
      "iteration 4813, loss: 0.007305154576897621\n",
      "iteration 4814, loss: 0.0074851191602647305\n",
      "iteration 4815, loss: 0.008215248584747314\n",
      "iteration 4816, loss: 0.008351905271410942\n",
      "iteration 4817, loss: 0.00783358421176672\n",
      "iteration 4818, loss: 0.00836094655096531\n",
      "iteration 4819, loss: 0.008326402865350246\n",
      "iteration 4820, loss: 0.007316960487514734\n",
      "iteration 4821, loss: 0.006993297021836042\n",
      "iteration 4822, loss: 0.008361534215509892\n",
      "iteration 4823, loss: 0.006924974266439676\n",
      "iteration 4824, loss: 0.006997754797339439\n",
      "iteration 4825, loss: 0.008499205112457275\n",
      "iteration 4826, loss: 0.00729766022413969\n",
      "iteration 4827, loss: 0.00782480463385582\n",
      "iteration 4828, loss: 0.007359212264418602\n",
      "iteration 4829, loss: 0.006496886257082224\n",
      "iteration 4830, loss: 0.0073471167124807835\n",
      "iteration 4831, loss: 0.007011153735220432\n",
      "iteration 4832, loss: 0.007453863508999348\n",
      "iteration 4833, loss: 0.006808373145759106\n",
      "iteration 4834, loss: 0.0072028725408017635\n",
      "iteration 4835, loss: 0.008340530097484589\n",
      "iteration 4836, loss: 0.007294513285160065\n",
      "iteration 4837, loss: 0.007930170744657516\n",
      "iteration 4838, loss: 0.008554101921617985\n",
      "iteration 4839, loss: 0.005853183567523956\n",
      "iteration 4840, loss: 0.0080955158919096\n",
      "iteration 4841, loss: 0.008250204846262932\n",
      "iteration 4842, loss: 0.007798897102475166\n",
      "iteration 4843, loss: 0.008566182106733322\n",
      "iteration 4844, loss: 0.009296143427491188\n",
      "iteration 4845, loss: 0.007947618141770363\n",
      "iteration 4846, loss: 0.007041699253022671\n",
      "iteration 4847, loss: 0.007336425594985485\n",
      "iteration 4848, loss: 0.008048513904213905\n",
      "iteration 4849, loss: 0.007077322341501713\n",
      "iteration 4850, loss: 0.00851353257894516\n",
      "iteration 4851, loss: 0.007127311546355486\n",
      "iteration 4852, loss: 0.007454869337379932\n",
      "iteration 4853, loss: 0.007650033570826054\n",
      "iteration 4854, loss: 0.00727823656052351\n",
      "iteration 4855, loss: 0.007292353548109531\n",
      "iteration 4856, loss: 0.006790652871131897\n",
      "iteration 4857, loss: 0.006259500049054623\n",
      "iteration 4858, loss: 0.00691189942881465\n",
      "iteration 4859, loss: 0.007352091372013092\n",
      "iteration 4860, loss: 0.00739353708922863\n",
      "iteration 4861, loss: 0.007957777008414268\n",
      "iteration 4862, loss: 0.008513829670846462\n",
      "iteration 4863, loss: 0.006927400827407837\n",
      "iteration 4864, loss: 0.00965910218656063\n",
      "iteration 4865, loss: 0.006891229189932346\n",
      "iteration 4866, loss: 0.007698021829128265\n",
      "iteration 4867, loss: 0.007694276049733162\n",
      "iteration 4868, loss: 0.007526833564043045\n",
      "iteration 4869, loss: 0.007428972516208887\n",
      "iteration 4870, loss: 0.007637397386133671\n",
      "iteration 4871, loss: 0.006910866126418114\n",
      "iteration 4872, loss: 0.008034493774175644\n",
      "iteration 4873, loss: 0.0062392656691372395\n",
      "iteration 4874, loss: 0.006947723217308521\n",
      "iteration 4875, loss: 0.008564703166484833\n",
      "iteration 4876, loss: 0.006647961214184761\n",
      "iteration 4877, loss: 0.007111906073987484\n",
      "iteration 4878, loss: 0.007797766476869583\n",
      "iteration 4879, loss: 0.007547374814748764\n",
      "iteration 4880, loss: 0.007130555342882872\n",
      "iteration 4881, loss: 0.006279261317104101\n",
      "iteration 4882, loss: 0.009291093796491623\n",
      "iteration 4883, loss: 0.007631812710314989\n",
      "iteration 4884, loss: 0.007364409975707531\n",
      "iteration 4885, loss: 0.006710751447826624\n",
      "iteration 4886, loss: 0.008427513763308525\n",
      "iteration 4887, loss: 0.00817409623414278\n",
      "iteration 4888, loss: 0.0075162239372730255\n",
      "iteration 4889, loss: 0.00816308706998825\n",
      "iteration 4890, loss: 0.007711364887654781\n",
      "iteration 4891, loss: 0.006449750624597073\n",
      "iteration 4892, loss: 0.0076276035979390144\n",
      "iteration 4893, loss: 0.007383917458355427\n",
      "iteration 4894, loss: 0.007089721970260143\n",
      "iteration 4895, loss: 0.00774820800870657\n",
      "iteration 4896, loss: 0.006937399506568909\n",
      "iteration 4897, loss: 0.008614866062998772\n",
      "iteration 4898, loss: 0.00739156361669302\n",
      "iteration 4899, loss: 0.008552751503884792\n",
      "iteration 4900, loss: 0.00706695020198822\n",
      "iteration 4901, loss: 0.007869531400501728\n",
      "iteration 4902, loss: 0.007193046156316996\n",
      "iteration 4903, loss: 0.008183281868696213\n",
      "iteration 4904, loss: 0.008654443547129631\n",
      "iteration 4905, loss: 0.006169363856315613\n",
      "iteration 4906, loss: 0.009447082877159119\n",
      "iteration 4907, loss: 0.007902230136096478\n",
      "iteration 4908, loss: 0.006609870120882988\n",
      "iteration 4909, loss: 0.007381427101790905\n",
      "iteration 4910, loss: 0.008688513189554214\n",
      "iteration 4911, loss: 0.007501514628529549\n",
      "iteration 4912, loss: 0.007218095473945141\n",
      "iteration 4913, loss: 0.0076894611120224\n",
      "iteration 4914, loss: 0.008538599126040936\n",
      "iteration 4915, loss: 0.00740198465064168\n",
      "iteration 4916, loss: 0.006847478915005922\n",
      "iteration 4917, loss: 0.007712886668741703\n",
      "iteration 4918, loss: 0.006869407836347818\n",
      "iteration 4919, loss: 0.00769641250371933\n",
      "iteration 4920, loss: 0.008294417522847652\n",
      "iteration 4921, loss: 0.006931204814463854\n",
      "iteration 4922, loss: 0.0079008424654603\n",
      "iteration 4923, loss: 0.008072083815932274\n",
      "iteration 4924, loss: 0.0081278495490551\n",
      "iteration 4925, loss: 0.006581902503967285\n",
      "iteration 4926, loss: 0.007694908417761326\n",
      "iteration 4927, loss: 0.008467461913824081\n",
      "iteration 4928, loss: 0.007166512310504913\n",
      "iteration 4929, loss: 0.008757928386330605\n",
      "iteration 4930, loss: 0.00805678591132164\n",
      "iteration 4931, loss: 0.008595447987318039\n",
      "iteration 4932, loss: 0.007135602179914713\n",
      "iteration 4933, loss: 0.00758424773812294\n",
      "iteration 4934, loss: 0.00713069224730134\n",
      "iteration 4935, loss: 0.00736100971698761\n",
      "iteration 4936, loss: 0.008459808304905891\n",
      "iteration 4937, loss: 0.00781676359474659\n",
      "iteration 4938, loss: 0.007666327059268951\n",
      "iteration 4939, loss: 0.007061678450554609\n",
      "iteration 4940, loss: 0.007996056228876114\n",
      "iteration 4941, loss: 0.0072854552417993546\n",
      "iteration 4942, loss: 0.007976201362907887\n",
      "iteration 4943, loss: 0.0077249943278729916\n",
      "iteration 4944, loss: 0.006985986605286598\n",
      "iteration 4945, loss: 0.00835401564836502\n",
      "iteration 4946, loss: 0.007500398904085159\n",
      "iteration 4947, loss: 0.008028628304600716\n",
      "iteration 4948, loss: 0.008282854221761227\n",
      "iteration 4949, loss: 0.00875093974173069\n",
      "iteration 4950, loss: 0.006461950019001961\n",
      "iteration 4951, loss: 0.008616777136921883\n",
      "iteration 4952, loss: 0.008262368850409985\n",
      "iteration 4953, loss: 0.007857993245124817\n",
      "iteration 4954, loss: 0.00867593102157116\n",
      "iteration 4955, loss: 0.00791778601706028\n",
      "iteration 4956, loss: 0.007689305581152439\n",
      "iteration 4957, loss: 0.008551178500056267\n",
      "iteration 4958, loss: 0.007729636505246162\n",
      "iteration 4959, loss: 0.006686500273644924\n",
      "iteration 4960, loss: 0.0075333015993237495\n",
      "iteration 4961, loss: 0.008991493843495846\n",
      "iteration 4962, loss: 0.00661005824804306\n",
      "iteration 4963, loss: 0.007482566870748997\n",
      "iteration 4964, loss: 0.007531726732850075\n",
      "iteration 4965, loss: 0.007767124101519585\n",
      "iteration 4966, loss: 0.008593617007136345\n",
      "iteration 4967, loss: 0.006247806828469038\n",
      "iteration 4968, loss: 0.007296497467905283\n",
      "iteration 4969, loss: 0.007008621469140053\n",
      "iteration 4970, loss: 0.007315126247704029\n",
      "iteration 4971, loss: 0.008546711876988411\n",
      "iteration 4972, loss: 0.007718781474977732\n",
      "iteration 4973, loss: 0.007137438282370567\n",
      "iteration 4974, loss: 0.006674408446997404\n",
      "iteration 4975, loss: 0.0077682239934802055\n",
      "iteration 4976, loss: 0.008091874420642853\n",
      "iteration 4977, loss: 0.008464407175779343\n",
      "iteration 4978, loss: 0.007949043065309525\n",
      "iteration 4979, loss: 0.008197938092052937\n",
      "iteration 4980, loss: 0.007817058824002743\n",
      "iteration 4981, loss: 0.007566467858850956\n",
      "iteration 4982, loss: 0.007204769644886255\n",
      "iteration 4983, loss: 0.007468866184353828\n",
      "iteration 4984, loss: 0.006912287324666977\n",
      "iteration 4985, loss: 0.008546662516891956\n",
      "iteration 4986, loss: 0.007125098258256912\n",
      "iteration 4987, loss: 0.0076602245680987835\n",
      "iteration 4988, loss: 0.007430676370859146\n",
      "iteration 4989, loss: 0.008213380351662636\n",
      "iteration 4990, loss: 0.007060876116156578\n",
      "iteration 4991, loss: 0.008123979903757572\n",
      "iteration 4992, loss: 0.006896981969475746\n",
      "iteration 4993, loss: 0.00884491391479969\n",
      "iteration 4994, loss: 0.008237789385020733\n",
      "iteration 4995, loss: 0.006003154441714287\n",
      "iteration 4996, loss: 0.007542782928794622\n",
      "iteration 4997, loss: 0.007302606478333473\n",
      "iteration 4998, loss: 0.007612691726535559\n",
      "iteration 4999, loss: 0.007292232476174831\n",
      "iteration 5000, loss: 0.007493113167583942\n",
      "iteration 5001, loss: 0.00723563227802515\n",
      "iteration 5002, loss: 0.0065955230966210365\n",
      "iteration 5003, loss: 0.00849155057221651\n",
      "iteration 5004, loss: 0.008040832355618477\n",
      "iteration 5005, loss: 0.007511009927839041\n",
      "iteration 5006, loss: 0.007257495075464249\n",
      "iteration 5007, loss: 0.007706150412559509\n",
      "iteration 5008, loss: 0.007062433287501335\n",
      "iteration 5009, loss: 0.006180749740451574\n",
      "iteration 5010, loss: 0.007594361901283264\n",
      "iteration 5011, loss: 0.007086614146828651\n",
      "iteration 5012, loss: 0.005535906180739403\n",
      "iteration 5013, loss: 0.005882319062948227\n",
      "iteration 5014, loss: 0.005978213623166084\n",
      "iteration 5015, loss: 0.007891945540904999\n",
      "iteration 5016, loss: 0.006806192919611931\n",
      "iteration 5017, loss: 0.007381994277238846\n",
      "iteration 5018, loss: 0.0070753879845142365\n",
      "iteration 5019, loss: 0.008040276356041431\n",
      "iteration 5020, loss: 0.00674924161285162\n",
      "iteration 5021, loss: 0.007270966190844774\n",
      "iteration 5022, loss: 0.005961431190371513\n",
      "iteration 5023, loss: 0.007310585118830204\n",
      "iteration 5024, loss: 0.007878730073571205\n",
      "iteration 5025, loss: 0.00844830647110939\n",
      "iteration 5026, loss: 0.007444936782121658\n",
      "iteration 5027, loss: 0.006909121759235859\n",
      "iteration 5028, loss: 0.007184949703514576\n",
      "iteration 5029, loss: 0.008284876123070717\n",
      "iteration 5030, loss: 0.006854883395135403\n",
      "iteration 5031, loss: 0.007261190563440323\n",
      "iteration 5032, loss: 0.005977645516395569\n",
      "iteration 5033, loss: 0.006395368836820126\n",
      "iteration 5034, loss: 0.005867910571396351\n",
      "iteration 5035, loss: 0.006420622579753399\n",
      "iteration 5036, loss: 0.006317100487649441\n",
      "iteration 5037, loss: 0.006869633682072163\n",
      "iteration 5038, loss: 0.0068351044319570065\n",
      "iteration 5039, loss: 0.008659969083964825\n",
      "iteration 5040, loss: 0.006827370263636112\n",
      "iteration 5041, loss: 0.007933609187602997\n",
      "iteration 5042, loss: 0.006681868806481361\n",
      "iteration 5043, loss: 0.008383632637560368\n",
      "iteration 5044, loss: 0.00742868660017848\n",
      "iteration 5045, loss: 0.007837389595806599\n",
      "iteration 5046, loss: 0.007288026623427868\n",
      "iteration 5047, loss: 0.0073277875781059265\n",
      "iteration 5048, loss: 0.006386544555425644\n",
      "iteration 5049, loss: 0.006235805340111256\n",
      "iteration 5050, loss: 0.007490203715860844\n",
      "iteration 5051, loss: 0.006670290604233742\n",
      "iteration 5052, loss: 0.00666203023865819\n",
      "iteration 5053, loss: 0.00615908857434988\n",
      "iteration 5054, loss: 0.007929841056466103\n",
      "iteration 5055, loss: 0.007751379627734423\n",
      "iteration 5056, loss: 0.00664004310965538\n",
      "iteration 5057, loss: 0.006934536620974541\n",
      "iteration 5058, loss: 0.007986885495483875\n",
      "iteration 5059, loss: 0.0080843111500144\n",
      "iteration 5060, loss: 0.007472907658666372\n",
      "iteration 5061, loss: 0.00695921340957284\n",
      "iteration 5062, loss: 0.007610965520143509\n",
      "iteration 5063, loss: 0.005949875805526972\n",
      "iteration 5064, loss: 0.006613763514906168\n",
      "iteration 5065, loss: 0.008253289386630058\n",
      "iteration 5066, loss: 0.006976050790399313\n",
      "iteration 5067, loss: 0.006860732100903988\n",
      "iteration 5068, loss: 0.007760551758110523\n",
      "iteration 5069, loss: 0.00685327872633934\n",
      "iteration 5070, loss: 0.0076536438427865505\n",
      "iteration 5071, loss: 0.00641297921538353\n",
      "iteration 5072, loss: 0.007999815046787262\n",
      "iteration 5073, loss: 0.008933790028095245\n",
      "iteration 5074, loss: 0.007334752473980188\n",
      "iteration 5075, loss: 0.006840965710580349\n",
      "iteration 5076, loss: 0.0075532542541623116\n",
      "iteration 5077, loss: 0.008281426504254341\n",
      "iteration 5078, loss: 0.0060173808597028255\n",
      "iteration 5079, loss: 0.008553232997655869\n",
      "iteration 5080, loss: 0.008327096700668335\n",
      "iteration 5081, loss: 0.007054954767227173\n",
      "iteration 5082, loss: 0.007725515868514776\n",
      "iteration 5083, loss: 0.008676815778017044\n",
      "iteration 5084, loss: 0.0074502257630229\n",
      "iteration 5085, loss: 0.008503954857587814\n",
      "iteration 5086, loss: 0.007806982845067978\n",
      "iteration 5087, loss: 0.007063251920044422\n",
      "iteration 5088, loss: 0.008241835981607437\n",
      "iteration 5089, loss: 0.008037874475121498\n",
      "iteration 5090, loss: 0.006583391223102808\n",
      "iteration 5091, loss: 0.007562781684100628\n",
      "iteration 5092, loss: 0.007999489083886147\n",
      "iteration 5093, loss: 0.006434505805373192\n",
      "iteration 5094, loss: 0.00716615654528141\n",
      "iteration 5095, loss: 0.007714536972343922\n",
      "iteration 5096, loss: 0.0067766024731099606\n",
      "iteration 5097, loss: 0.006318245083093643\n",
      "iteration 5098, loss: 0.006836012005805969\n",
      "iteration 5099, loss: 0.008677971549332142\n",
      "iteration 5100, loss: 0.008392825722694397\n",
      "iteration 5101, loss: 0.0084067452698946\n",
      "iteration 5102, loss: 0.0069296592846512794\n",
      "iteration 5103, loss: 0.006949034985154867\n",
      "iteration 5104, loss: 0.007607749197632074\n",
      "iteration 5105, loss: 0.006922868080437183\n",
      "iteration 5106, loss: 0.007452494464814663\n",
      "iteration 5107, loss: 0.007440553046762943\n",
      "iteration 5108, loss: 0.007365281693637371\n",
      "iteration 5109, loss: 0.0068257590755820274\n",
      "iteration 5110, loss: 0.006968841888010502\n",
      "iteration 5111, loss: 0.007484215311706066\n",
      "iteration 5112, loss: 0.007450764067471027\n",
      "iteration 5113, loss: 0.006832617335021496\n",
      "iteration 5114, loss: 0.0074887024238705635\n",
      "iteration 5115, loss: 0.006559434812515974\n",
      "iteration 5116, loss: 0.00798612367361784\n",
      "iteration 5117, loss: 0.007578064221888781\n",
      "iteration 5118, loss: 0.007681441027671099\n",
      "iteration 5119, loss: 0.007445627357810736\n",
      "iteration 5120, loss: 0.007055715657770634\n",
      "iteration 5121, loss: 0.006847639102488756\n",
      "iteration 5122, loss: 0.007889954373240471\n",
      "iteration 5123, loss: 0.0074737221002578735\n",
      "iteration 5124, loss: 0.006653902120888233\n",
      "iteration 5125, loss: 0.007051326334476471\n",
      "iteration 5126, loss: 0.008682150393724442\n",
      "iteration 5127, loss: 0.007866399362683296\n",
      "iteration 5128, loss: 0.008438952267169952\n",
      "iteration 5129, loss: 0.007366702891886234\n",
      "iteration 5130, loss: 0.00693506421521306\n",
      "iteration 5131, loss: 0.007141031790524721\n",
      "iteration 5132, loss: 0.0072247632779181\n",
      "iteration 5133, loss: 0.007049528416246176\n",
      "iteration 5134, loss: 0.006525946781039238\n",
      "iteration 5135, loss: 0.007960686460137367\n",
      "iteration 5136, loss: 0.00735612865537405\n",
      "iteration 5137, loss: 0.008112303912639618\n",
      "iteration 5138, loss: 0.0072427671402692795\n",
      "iteration 5139, loss: 0.0076157888397574425\n",
      "iteration 5140, loss: 0.006437178701162338\n",
      "iteration 5141, loss: 0.0071668922901153564\n",
      "iteration 5142, loss: 0.006222729105502367\n",
      "iteration 5143, loss: 0.007407241500914097\n",
      "iteration 5144, loss: 0.0076919663697481155\n",
      "iteration 5145, loss: 0.006644275039434433\n",
      "iteration 5146, loss: 0.0079676304012537\n",
      "iteration 5147, loss: 0.00697313342243433\n",
      "iteration 5148, loss: 0.007056787610054016\n",
      "iteration 5149, loss: 0.006860798224806786\n",
      "iteration 5150, loss: 0.006689534522593021\n",
      "iteration 5151, loss: 0.005975876934826374\n",
      "iteration 5152, loss: 0.007436923682689667\n",
      "iteration 5153, loss: 0.005428903736174107\n",
      "iteration 5154, loss: 0.007936838082969189\n",
      "iteration 5155, loss: 0.008064109832048416\n",
      "iteration 5156, loss: 0.007142809219658375\n",
      "iteration 5157, loss: 0.006788408849388361\n",
      "iteration 5158, loss: 0.008176203817129135\n",
      "iteration 5159, loss: 0.006930134724825621\n",
      "iteration 5160, loss: 0.007062424439936876\n",
      "iteration 5161, loss: 0.007535882294178009\n",
      "iteration 5162, loss: 0.005889331456273794\n",
      "iteration 5163, loss: 0.0064010657370090485\n",
      "iteration 5164, loss: 0.0062308479100465775\n",
      "iteration 5165, loss: 0.006981604732573032\n",
      "iteration 5166, loss: 0.006498738192021847\n",
      "iteration 5167, loss: 0.006489819847047329\n",
      "iteration 5168, loss: 0.007252992130815983\n",
      "iteration 5169, loss: 0.00652497261762619\n",
      "iteration 5170, loss: 0.007167136296629906\n",
      "iteration 5171, loss: 0.006825955118983984\n",
      "iteration 5172, loss: 0.0065721916034817696\n",
      "iteration 5173, loss: 0.006399673409759998\n",
      "iteration 5174, loss: 0.0070725660771131516\n",
      "iteration 5175, loss: 0.007592596113681793\n",
      "iteration 5176, loss: 0.0072593893855810165\n",
      "iteration 5177, loss: 0.007105004042387009\n",
      "iteration 5178, loss: 0.0069496422074735165\n",
      "iteration 5179, loss: 0.007160366512835026\n",
      "iteration 5180, loss: 0.006915799807757139\n",
      "iteration 5181, loss: 0.006864333525300026\n",
      "iteration 5182, loss: 0.007087839767336845\n",
      "iteration 5183, loss: 0.0076538878493011\n",
      "iteration 5184, loss: 0.006886979565024376\n",
      "iteration 5185, loss: 0.007045948877930641\n",
      "iteration 5186, loss: 0.008604360744357109\n",
      "iteration 5187, loss: 0.008068950846791267\n",
      "iteration 5188, loss: 0.00643578777089715\n",
      "iteration 5189, loss: 0.007015639450401068\n",
      "iteration 5190, loss: 0.006993595976382494\n",
      "iteration 5191, loss: 0.006027288734912872\n",
      "iteration 5192, loss: 0.005648158024996519\n",
      "iteration 5193, loss: 0.006778954528272152\n",
      "iteration 5194, loss: 0.008750042878091335\n",
      "iteration 5195, loss: 0.007244554348289967\n",
      "iteration 5196, loss: 0.006999223493039608\n",
      "iteration 5197, loss: 0.007701874244958162\n",
      "iteration 5198, loss: 0.005868837237358093\n",
      "iteration 5199, loss: 0.0074966237880289555\n",
      "iteration 5200, loss: 0.005719565786421299\n",
      "iteration 5201, loss: 0.0060266247019171715\n",
      "iteration 5202, loss: 0.006608221679925919\n",
      "iteration 5203, loss: 0.007227474823594093\n",
      "iteration 5204, loss: 0.00704530905932188\n",
      "iteration 5205, loss: 0.007371450774371624\n",
      "iteration 5206, loss: 0.006501273717731237\n",
      "iteration 5207, loss: 0.00647497083991766\n",
      "iteration 5208, loss: 0.007920660078525543\n",
      "iteration 5209, loss: 0.006718684919178486\n",
      "iteration 5210, loss: 0.00663447380065918\n",
      "iteration 5211, loss: 0.007714089937508106\n",
      "iteration 5212, loss: 0.007952926680445671\n",
      "iteration 5213, loss: 0.007597137708216906\n",
      "iteration 5214, loss: 0.006115824915468693\n",
      "iteration 5215, loss: 0.007059655152261257\n",
      "iteration 5216, loss: 0.006321798078715801\n",
      "iteration 5217, loss: 0.006318362429738045\n",
      "iteration 5218, loss: 0.0065122125670313835\n",
      "iteration 5219, loss: 0.007958411239087582\n",
      "iteration 5220, loss: 0.008741935715079308\n",
      "iteration 5221, loss: 0.006459541618824005\n",
      "iteration 5222, loss: 0.0073982165195047855\n",
      "iteration 5223, loss: 0.007284264080226421\n",
      "iteration 5224, loss: 0.006567418109625578\n",
      "iteration 5225, loss: 0.007264005020260811\n",
      "iteration 5226, loss: 0.007404453121125698\n",
      "iteration 5227, loss: 0.006239299662411213\n",
      "iteration 5228, loss: 0.007416845299303532\n",
      "iteration 5229, loss: 0.008103402331471443\n",
      "iteration 5230, loss: 0.007793840952217579\n",
      "iteration 5231, loss: 0.00658738287165761\n",
      "iteration 5232, loss: 0.007374703884124756\n",
      "iteration 5233, loss: 0.007350423838943243\n",
      "iteration 5234, loss: 0.005888697225600481\n",
      "iteration 5235, loss: 0.008909372612833977\n",
      "iteration 5236, loss: 0.007512671407312155\n",
      "iteration 5237, loss: 0.0072174519300460815\n",
      "iteration 5238, loss: 0.007095906883478165\n",
      "iteration 5239, loss: 0.008147726766765118\n",
      "iteration 5240, loss: 0.007796322461217642\n",
      "iteration 5241, loss: 0.007245140615850687\n",
      "iteration 5242, loss: 0.006494073197245598\n",
      "iteration 5243, loss: 0.0062723783776164055\n",
      "iteration 5244, loss: 0.0062748733907938\n",
      "iteration 5245, loss: 0.007996163330972195\n",
      "iteration 5246, loss: 0.008305643685162067\n",
      "iteration 5247, loss: 0.006176627241075039\n",
      "iteration 5248, loss: 0.008762084878981113\n",
      "iteration 5249, loss: 0.007091657258570194\n",
      "iteration 5250, loss: 0.006255949381738901\n",
      "iteration 5251, loss: 0.007473404053598642\n",
      "iteration 5252, loss: 0.006368737202137709\n",
      "iteration 5253, loss: 0.007092833984643221\n",
      "iteration 5254, loss: 0.007505578920245171\n",
      "iteration 5255, loss: 0.005716165993362665\n",
      "iteration 5256, loss: 0.007300873752683401\n",
      "iteration 5257, loss: 0.007428616285324097\n",
      "iteration 5258, loss: 0.008350234478712082\n",
      "iteration 5259, loss: 0.008065527305006981\n",
      "iteration 5260, loss: 0.007489219307899475\n",
      "iteration 5261, loss: 0.005641690455377102\n",
      "iteration 5262, loss: 0.005504906177520752\n",
      "iteration 5263, loss: 0.007477820385247469\n",
      "iteration 5264, loss: 0.007505199406296015\n",
      "iteration 5265, loss: 0.007795411627739668\n",
      "iteration 5266, loss: 0.006309603340923786\n",
      "iteration 5267, loss: 0.006651632487773895\n",
      "iteration 5268, loss: 0.006585154682397842\n",
      "iteration 5269, loss: 0.007339170202612877\n",
      "iteration 5270, loss: 0.006932543125003576\n",
      "iteration 5271, loss: 0.006872131954878569\n",
      "iteration 5272, loss: 0.006256183609366417\n",
      "iteration 5273, loss: 0.007251807954162359\n",
      "iteration 5274, loss: 0.007072551175951958\n",
      "iteration 5275, loss: 0.007447347976267338\n",
      "iteration 5276, loss: 0.006576755084097385\n",
      "iteration 5277, loss: 0.007986076176166534\n",
      "iteration 5278, loss: 0.006368126254528761\n",
      "iteration 5279, loss: 0.0069379499182105064\n",
      "iteration 5280, loss: 0.007143941707909107\n",
      "iteration 5281, loss: 0.006803218275308609\n",
      "iteration 5282, loss: 0.006528966128826141\n",
      "iteration 5283, loss: 0.006639176979660988\n",
      "iteration 5284, loss: 0.007058965507894754\n",
      "iteration 5285, loss: 0.007026602514088154\n",
      "iteration 5286, loss: 0.007608143612742424\n",
      "iteration 5287, loss: 0.006868706084787846\n",
      "iteration 5288, loss: 0.006535008549690247\n",
      "iteration 5289, loss: 0.007570095360279083\n",
      "iteration 5290, loss: 0.007610556203871965\n",
      "iteration 5291, loss: 0.0074909888207912445\n",
      "iteration 5292, loss: 0.006846177391707897\n",
      "iteration 5293, loss: 0.007157276384532452\n",
      "iteration 5294, loss: 0.006860978901386261\n",
      "iteration 5295, loss: 0.006812858395278454\n",
      "iteration 5296, loss: 0.007530023343861103\n",
      "iteration 5297, loss: 0.006814200431108475\n",
      "iteration 5298, loss: 0.007546874228864908\n",
      "iteration 5299, loss: 0.006604715716093779\n",
      "iteration 5300, loss: 0.006555387284606695\n",
      "iteration 5301, loss: 0.007573026232421398\n",
      "iteration 5302, loss: 0.007023069076240063\n",
      "iteration 5303, loss: 0.007009859196841717\n",
      "iteration 5304, loss: 0.008197400718927383\n",
      "iteration 5305, loss: 0.007462484296411276\n",
      "iteration 5306, loss: 0.00686305109411478\n",
      "iteration 5307, loss: 0.008924713358283043\n",
      "iteration 5308, loss: 0.006680305581539869\n",
      "iteration 5309, loss: 0.0064183189533650875\n",
      "iteration 5310, loss: 0.007693263702094555\n",
      "iteration 5311, loss: 0.006765575613826513\n",
      "iteration 5312, loss: 0.006113571114838123\n",
      "iteration 5313, loss: 0.008067993447184563\n",
      "iteration 5314, loss: 0.006443043239414692\n",
      "iteration 5315, loss: 0.008184796199202538\n",
      "iteration 5316, loss: 0.008159676566720009\n",
      "iteration 5317, loss: 0.0075960587710142136\n",
      "iteration 5318, loss: 0.008306702598929405\n",
      "iteration 5319, loss: 0.007064766716212034\n",
      "iteration 5320, loss: 0.008455094881355762\n",
      "iteration 5321, loss: 0.006761153228580952\n",
      "iteration 5322, loss: 0.006985212676227093\n",
      "iteration 5323, loss: 0.0065173255279660225\n",
      "iteration 5324, loss: 0.00767653388902545\n",
      "iteration 5325, loss: 0.005965396761894226\n",
      "iteration 5326, loss: 0.00662324158474803\n",
      "iteration 5327, loss: 0.007800440303981304\n",
      "iteration 5328, loss: 0.007431601174175739\n",
      "iteration 5329, loss: 0.007544585503637791\n",
      "iteration 5330, loss: 0.007064555771648884\n",
      "iteration 5331, loss: 0.006515870802104473\n",
      "iteration 5332, loss: 0.005431634373962879\n",
      "iteration 5333, loss: 0.006381564773619175\n",
      "iteration 5334, loss: 0.006494409870356321\n",
      "iteration 5335, loss: 0.006797922775149345\n",
      "iteration 5336, loss: 0.006703996565192938\n",
      "iteration 5337, loss: 0.006432250142097473\n",
      "iteration 5338, loss: 0.007163931615650654\n",
      "iteration 5339, loss: 0.006623039022088051\n",
      "iteration 5340, loss: 0.007154376246035099\n",
      "iteration 5341, loss: 0.007366751320660114\n",
      "iteration 5342, loss: 0.006678116042166948\n",
      "iteration 5343, loss: 0.007619693875312805\n",
      "iteration 5344, loss: 0.007884964346885681\n",
      "iteration 5345, loss: 0.006856471765786409\n",
      "iteration 5346, loss: 0.007964964024722576\n",
      "iteration 5347, loss: 0.006311430595815182\n",
      "iteration 5348, loss: 0.0064797960221767426\n",
      "iteration 5349, loss: 0.007812870666384697\n",
      "iteration 5350, loss: 0.007178634405136108\n",
      "iteration 5351, loss: 0.0078653609380126\n",
      "iteration 5352, loss: 0.008379744365811348\n",
      "iteration 5353, loss: 0.006366072688251734\n",
      "iteration 5354, loss: 0.007709519471973181\n",
      "iteration 5355, loss: 0.007193006109446287\n",
      "iteration 5356, loss: 0.007395888678729534\n",
      "iteration 5357, loss: 0.007107179146260023\n",
      "iteration 5358, loss: 0.006619851570576429\n",
      "iteration 5359, loss: 0.0067328899167478085\n",
      "iteration 5360, loss: 0.008247625082731247\n",
      "iteration 5361, loss: 0.007103844545781612\n",
      "iteration 5362, loss: 0.008557585999369621\n",
      "iteration 5363, loss: 0.006857112981379032\n",
      "iteration 5364, loss: 0.006361954379826784\n",
      "iteration 5365, loss: 0.006508622784167528\n",
      "iteration 5366, loss: 0.006743961945176125\n",
      "iteration 5367, loss: 0.008202123455703259\n",
      "iteration 5368, loss: 0.005952583160251379\n",
      "iteration 5369, loss: 0.0061094509437680244\n",
      "iteration 5370, loss: 0.008336595259606838\n",
      "iteration 5371, loss: 0.006368430331349373\n",
      "iteration 5372, loss: 0.007044297643005848\n",
      "iteration 5373, loss: 0.006575801409780979\n",
      "iteration 5374, loss: 0.007710880134254694\n",
      "iteration 5375, loss: 0.007242906838655472\n",
      "iteration 5376, loss: 0.008028295822441578\n",
      "iteration 5377, loss: 0.007069457322359085\n",
      "iteration 5378, loss: 0.006488654762506485\n",
      "iteration 5379, loss: 0.00733043672516942\n",
      "iteration 5380, loss: 0.007550526410341263\n",
      "iteration 5381, loss: 0.006788845174014568\n",
      "iteration 5382, loss: 0.006574178114533424\n",
      "iteration 5383, loss: 0.007004935760051012\n",
      "iteration 5384, loss: 0.006601735018193722\n",
      "iteration 5385, loss: 0.006043106783181429\n",
      "iteration 5386, loss: 0.008150123059749603\n",
      "iteration 5387, loss: 0.005430079996585846\n",
      "iteration 5388, loss: 0.007813379168510437\n",
      "iteration 5389, loss: 0.0070487200282514095\n",
      "iteration 5390, loss: 0.006867946125566959\n",
      "iteration 5391, loss: 0.007478706073015928\n",
      "iteration 5392, loss: 0.006437687668949366\n",
      "iteration 5393, loss: 0.006955135613679886\n",
      "iteration 5394, loss: 0.0063478536903858185\n",
      "iteration 5395, loss: 0.007652970030903816\n",
      "iteration 5396, loss: 0.008150852285325527\n",
      "iteration 5397, loss: 0.006416013464331627\n",
      "iteration 5398, loss: 0.00701553001999855\n",
      "iteration 5399, loss: 0.0062076738104224205\n",
      "iteration 5400, loss: 0.006065741181373596\n",
      "iteration 5401, loss: 0.007440836168825626\n",
      "iteration 5402, loss: 0.006187812425196171\n",
      "iteration 5403, loss: 0.007211228366941214\n",
      "iteration 5404, loss: 0.006215140223503113\n",
      "iteration 5405, loss: 0.007021800614893436\n",
      "iteration 5406, loss: 0.006926999893039465\n",
      "iteration 5407, loss: 0.006957498379051685\n",
      "iteration 5408, loss: 0.006449031177908182\n",
      "iteration 5409, loss: 0.007086312398314476\n",
      "iteration 5410, loss: 0.006663505919277668\n",
      "iteration 5411, loss: 0.006919346749782562\n",
      "iteration 5412, loss: 0.007208721712231636\n",
      "iteration 5413, loss: 0.006530212704092264\n",
      "iteration 5414, loss: 0.0065536173060536385\n",
      "iteration 5415, loss: 0.00598187418654561\n",
      "iteration 5416, loss: 0.007171451114118099\n",
      "iteration 5417, loss: 0.006664834450930357\n",
      "iteration 5418, loss: 0.007135631516575813\n",
      "iteration 5419, loss: 0.006587005220353603\n",
      "iteration 5420, loss: 0.006868747528642416\n",
      "iteration 5421, loss: 0.006608657073229551\n",
      "iteration 5422, loss: 0.007598740980029106\n",
      "iteration 5423, loss: 0.006365978624671698\n",
      "iteration 5424, loss: 0.0059720855206251144\n",
      "iteration 5425, loss: 0.006316686049103737\n",
      "iteration 5426, loss: 0.006519178859889507\n",
      "iteration 5427, loss: 0.00647809449583292\n",
      "iteration 5428, loss: 0.007475334219634533\n",
      "iteration 5429, loss: 0.006854437291622162\n",
      "iteration 5430, loss: 0.00770210986956954\n",
      "iteration 5431, loss: 0.006785813719034195\n",
      "iteration 5432, loss: 0.006264758296310902\n",
      "iteration 5433, loss: 0.007218694314360619\n",
      "iteration 5434, loss: 0.007986937649548054\n",
      "iteration 5435, loss: 0.006602885201573372\n",
      "iteration 5436, loss: 0.006792067550122738\n",
      "iteration 5437, loss: 0.006448012311011553\n",
      "iteration 5438, loss: 0.006254194770008326\n",
      "iteration 5439, loss: 0.006741641089320183\n",
      "iteration 5440, loss: 0.007269249763339758\n",
      "iteration 5441, loss: 0.006387866567820311\n",
      "iteration 5442, loss: 0.00591649953275919\n",
      "iteration 5443, loss: 0.006839024368673563\n",
      "iteration 5444, loss: 0.00645810179412365\n",
      "iteration 5445, loss: 0.006879840046167374\n",
      "iteration 5446, loss: 0.006632660049945116\n",
      "iteration 5447, loss: 0.006529920268803835\n",
      "iteration 5448, loss: 0.00685239490121603\n",
      "iteration 5449, loss: 0.008055215701460838\n",
      "iteration 5450, loss: 0.007462969049811363\n",
      "iteration 5451, loss: 0.008106810040771961\n",
      "iteration 5452, loss: 0.00711803650483489\n",
      "iteration 5453, loss: 0.007280838675796986\n",
      "iteration 5454, loss: 0.0073111215606331825\n",
      "iteration 5455, loss: 0.006007794290781021\n",
      "iteration 5456, loss: 0.006288370117545128\n",
      "iteration 5457, loss: 0.006566493771970272\n",
      "iteration 5458, loss: 0.00762212835252285\n",
      "iteration 5459, loss: 0.007787112146615982\n",
      "iteration 5460, loss: 0.0058854809030890465\n",
      "iteration 5461, loss: 0.006500448565930128\n",
      "iteration 5462, loss: 0.006297438405454159\n",
      "iteration 5463, loss: 0.006483182311058044\n",
      "iteration 5464, loss: 0.007208040915429592\n",
      "iteration 5465, loss: 0.0067627825774252415\n",
      "iteration 5466, loss: 0.007679690606892109\n",
      "iteration 5467, loss: 0.007647548336535692\n",
      "iteration 5468, loss: 0.008108903653919697\n",
      "iteration 5469, loss: 0.007747428026050329\n",
      "iteration 5470, loss: 0.0071753570809960365\n",
      "iteration 5471, loss: 0.007327742408961058\n",
      "iteration 5472, loss: 0.00662490539252758\n",
      "iteration 5473, loss: 0.006295396015048027\n",
      "iteration 5474, loss: 0.008177625015377998\n",
      "iteration 5475, loss: 0.006623686756938696\n",
      "iteration 5476, loss: 0.006218550261110067\n",
      "iteration 5477, loss: 0.00652961153537035\n",
      "iteration 5478, loss: 0.0061020017601549625\n",
      "iteration 5479, loss: 0.006679232232272625\n",
      "iteration 5480, loss: 0.007452754769474268\n",
      "iteration 5481, loss: 0.007517855614423752\n",
      "iteration 5482, loss: 0.006436449941247702\n",
      "iteration 5483, loss: 0.006901984103024006\n",
      "iteration 5484, loss: 0.0060967388562858105\n",
      "iteration 5485, loss: 0.007147245109081268\n",
      "iteration 5486, loss: 0.005909875500947237\n",
      "iteration 5487, loss: 0.006071553099900484\n",
      "iteration 5488, loss: 0.006053035147488117\n",
      "iteration 5489, loss: 0.0069179111160337925\n",
      "iteration 5490, loss: 0.005977684631943703\n",
      "iteration 5491, loss: 0.0066744377836585045\n",
      "iteration 5492, loss: 0.006975339259952307\n",
      "iteration 5493, loss: 0.005895780399441719\n",
      "iteration 5494, loss: 0.00703051732853055\n",
      "iteration 5495, loss: 0.005655769258737564\n",
      "iteration 5496, loss: 0.005804825108498335\n",
      "iteration 5497, loss: 0.007047427352517843\n",
      "iteration 5498, loss: 0.006825448479503393\n",
      "iteration 5499, loss: 0.006763780489563942\n",
      "iteration 5500, loss: 0.006120133213698864\n",
      "iteration 5501, loss: 0.006608800962567329\n",
      "iteration 5502, loss: 0.006545286625623703\n",
      "iteration 5503, loss: 0.006184997968375683\n",
      "iteration 5504, loss: 0.006145358085632324\n",
      "iteration 5505, loss: 0.005486390553414822\n",
      "iteration 5506, loss: 0.006332859862595797\n",
      "iteration 5507, loss: 0.007867123931646347\n",
      "iteration 5508, loss: 0.007251326460391283\n",
      "iteration 5509, loss: 0.0068630194291472435\n",
      "iteration 5510, loss: 0.007118423469364643\n",
      "iteration 5511, loss: 0.00656378036364913\n",
      "iteration 5512, loss: 0.005376753397285938\n",
      "iteration 5513, loss: 0.007016840390861034\n",
      "iteration 5514, loss: 0.005837226286530495\n",
      "iteration 5515, loss: 0.006663608364760876\n",
      "iteration 5516, loss: 0.0066457511857151985\n",
      "iteration 5517, loss: 0.007311267778277397\n",
      "iteration 5518, loss: 0.008139011450111866\n",
      "iteration 5519, loss: 0.007179952226579189\n",
      "iteration 5520, loss: 0.006644885987043381\n",
      "iteration 5521, loss: 0.0059882476925849915\n",
      "iteration 5522, loss: 0.0065629854798316956\n",
      "iteration 5523, loss: 0.006531915627419949\n",
      "iteration 5524, loss: 0.007875215262174606\n",
      "iteration 5525, loss: 0.005766852293163538\n",
      "iteration 5526, loss: 0.00561893405392766\n",
      "iteration 5527, loss: 0.00726945698261261\n",
      "iteration 5528, loss: 0.006490994710475206\n",
      "iteration 5529, loss: 0.006717280019074678\n",
      "iteration 5530, loss: 0.007381035014986992\n",
      "iteration 5531, loss: 0.0067280554212629795\n",
      "iteration 5532, loss: 0.006551973521709442\n",
      "iteration 5533, loss: 0.007038535550236702\n",
      "iteration 5534, loss: 0.0057944441214203835\n",
      "iteration 5535, loss: 0.0071065379306674\n",
      "iteration 5536, loss: 0.005917910486459732\n",
      "iteration 5537, loss: 0.005877644754946232\n",
      "iteration 5538, loss: 0.006276889704167843\n",
      "iteration 5539, loss: 0.0070117139257490635\n",
      "iteration 5540, loss: 0.006521509028971195\n",
      "iteration 5541, loss: 0.006192611530423164\n",
      "iteration 5542, loss: 0.0061364625580608845\n",
      "iteration 5543, loss: 0.005892646498978138\n",
      "iteration 5544, loss: 0.006214580498635769\n",
      "iteration 5545, loss: 0.006330215837806463\n",
      "iteration 5546, loss: 0.006675794720649719\n",
      "iteration 5547, loss: 0.007361123338341713\n",
      "iteration 5548, loss: 0.00531639764085412\n",
      "iteration 5549, loss: 0.006269089877605438\n",
      "iteration 5550, loss: 0.006699494086205959\n",
      "iteration 5551, loss: 0.005657839588820934\n",
      "iteration 5552, loss: 0.006270404905080795\n",
      "iteration 5553, loss: 0.005685812793672085\n",
      "iteration 5554, loss: 0.0059935408644378185\n",
      "iteration 5555, loss: 0.006533830426633358\n",
      "iteration 5556, loss: 0.007163694594055414\n",
      "iteration 5557, loss: 0.005228693597018719\n",
      "iteration 5558, loss: 0.0073333680629730225\n",
      "iteration 5559, loss: 0.007284085266292095\n",
      "iteration 5560, loss: 0.006361284293234348\n",
      "iteration 5561, loss: 0.0063115572556853294\n",
      "iteration 5562, loss: 0.005592917092144489\n",
      "iteration 5563, loss: 0.005917259491980076\n",
      "iteration 5564, loss: 0.0052718231454491615\n",
      "iteration 5565, loss: 0.006810694467276335\n",
      "iteration 5566, loss: 0.007281527388840914\n",
      "iteration 5567, loss: 0.0060967616736888885\n",
      "iteration 5568, loss: 0.006151325535029173\n",
      "iteration 5569, loss: 0.007254199590533972\n",
      "iteration 5570, loss: 0.0059420522302389145\n",
      "iteration 5571, loss: 0.00673280656337738\n",
      "iteration 5572, loss: 0.007521463092416525\n",
      "iteration 5573, loss: 0.006454718764871359\n",
      "iteration 5574, loss: 0.007006521336734295\n",
      "iteration 5575, loss: 0.0069052474573254585\n",
      "iteration 5576, loss: 0.006019921042025089\n",
      "iteration 5577, loss: 0.007299796678125858\n",
      "iteration 5578, loss: 0.006227005738765001\n",
      "iteration 5579, loss: 0.006352769210934639\n",
      "iteration 5580, loss: 0.007486354559659958\n",
      "iteration 5581, loss: 0.007095355540513992\n",
      "iteration 5582, loss: 0.0063153947703540325\n",
      "iteration 5583, loss: 0.006047253496944904\n",
      "iteration 5584, loss: 0.007252517621964216\n",
      "iteration 5585, loss: 0.0072012427262961864\n",
      "iteration 5586, loss: 0.007871084846556187\n",
      "iteration 5587, loss: 0.006861408706754446\n",
      "iteration 5588, loss: 0.006561047863215208\n",
      "iteration 5589, loss: 0.006944384425878525\n",
      "iteration 5590, loss: 0.006723583210259676\n",
      "iteration 5591, loss: 0.007677649147808552\n",
      "iteration 5592, loss: 0.007745802402496338\n",
      "iteration 5593, loss: 0.0063478583469986916\n",
      "iteration 5594, loss: 0.007896354421973228\n",
      "iteration 5595, loss: 0.006421305239200592\n",
      "iteration 5596, loss: 0.00789364892989397\n",
      "iteration 5597, loss: 0.005889412946999073\n",
      "iteration 5598, loss: 0.006964202970266342\n",
      "iteration 5599, loss: 0.006720299832522869\n",
      "iteration 5600, loss: 0.006153297610580921\n",
      "iteration 5601, loss: 0.005890267435461283\n",
      "iteration 5602, loss: 0.006741107441484928\n",
      "iteration 5603, loss: 0.0056519294157624245\n",
      "iteration 5604, loss: 0.006901499815285206\n",
      "iteration 5605, loss: 0.006721085403114557\n",
      "iteration 5606, loss: 0.00749262934550643\n",
      "iteration 5607, loss: 0.006802678573876619\n",
      "iteration 5608, loss: 0.006498354487121105\n",
      "iteration 5609, loss: 0.007617396302521229\n",
      "iteration 5610, loss: 0.005910350009799004\n",
      "iteration 5611, loss: 0.0064922962337732315\n",
      "iteration 5612, loss: 0.0068223499692976475\n",
      "iteration 5613, loss: 0.0073707690462470055\n",
      "iteration 5614, loss: 0.007212115451693535\n",
      "iteration 5615, loss: 0.006123707164078951\n",
      "iteration 5616, loss: 0.005643014796078205\n",
      "iteration 5617, loss: 0.006709864363074303\n",
      "iteration 5618, loss: 0.007411981001496315\n",
      "iteration 5619, loss: 0.006257044617086649\n",
      "iteration 5620, loss: 0.006009264849126339\n",
      "iteration 5621, loss: 0.005447227507829666\n",
      "iteration 5622, loss: 0.006437389180064201\n",
      "iteration 5623, loss: 0.00631329882889986\n",
      "iteration 5624, loss: 0.006271770223975182\n",
      "iteration 5625, loss: 0.006465445272624493\n",
      "iteration 5626, loss: 0.005640213843435049\n",
      "iteration 5627, loss: 0.006888646632432938\n",
      "iteration 5628, loss: 0.00653748819604516\n",
      "iteration 5629, loss: 0.006119005382061005\n",
      "iteration 5630, loss: 0.006256191059947014\n",
      "iteration 5631, loss: 0.005434044636785984\n",
      "iteration 5632, loss: 0.0059616644866764545\n",
      "iteration 5633, loss: 0.005880672950297594\n",
      "iteration 5634, loss: 0.006812353618443012\n",
      "iteration 5635, loss: 0.005311815533787012\n",
      "iteration 5636, loss: 0.0060946326702833176\n",
      "iteration 5637, loss: 0.006307669449597597\n",
      "iteration 5638, loss: 0.006991829257458448\n",
      "iteration 5639, loss: 0.006857011467218399\n",
      "iteration 5640, loss: 0.006554440129548311\n",
      "iteration 5641, loss: 0.004897448234260082\n",
      "iteration 5642, loss: 0.006239325739443302\n",
      "iteration 5643, loss: 0.006972292438149452\n",
      "iteration 5644, loss: 0.0072276778519153595\n",
      "iteration 5645, loss: 0.00595437828451395\n",
      "iteration 5646, loss: 0.007396243512630463\n",
      "iteration 5647, loss: 0.006587434560060501\n",
      "iteration 5648, loss: 0.00651588337495923\n",
      "iteration 5649, loss: 0.007079815957695246\n",
      "iteration 5650, loss: 0.005712045822292566\n",
      "iteration 5651, loss: 0.0075875381007790565\n",
      "iteration 5652, loss: 0.0066546485759317875\n",
      "iteration 5653, loss: 0.005558604374527931\n",
      "iteration 5654, loss: 0.007329464890062809\n",
      "iteration 5655, loss: 0.006403010338544846\n",
      "iteration 5656, loss: 0.005943796597421169\n",
      "iteration 5657, loss: 0.006420416291803122\n",
      "iteration 5658, loss: 0.0062994323670864105\n",
      "iteration 5659, loss: 0.006749344989657402\n",
      "iteration 5660, loss: 0.004693010821938515\n",
      "iteration 5661, loss: 0.00622169766575098\n",
      "iteration 5662, loss: 0.006109609268605709\n",
      "iteration 5663, loss: 0.005824986845254898\n",
      "iteration 5664, loss: 0.005499073304235935\n",
      "iteration 5665, loss: 0.006888561882078648\n",
      "iteration 5666, loss: 0.006510820239782333\n",
      "iteration 5667, loss: 0.005794652737677097\n",
      "iteration 5668, loss: 0.006359828636050224\n",
      "iteration 5669, loss: 0.0064797354862093925\n",
      "iteration 5670, loss: 0.006025894545018673\n",
      "iteration 5671, loss: 0.007282024249434471\n",
      "iteration 5672, loss: 0.005249462556093931\n",
      "iteration 5673, loss: 0.007145172916352749\n",
      "iteration 5674, loss: 0.006194457411766052\n",
      "iteration 5675, loss: 0.006358232349157333\n",
      "iteration 5676, loss: 0.006853814236819744\n",
      "iteration 5677, loss: 0.008005652576684952\n",
      "iteration 5678, loss: 0.006713241804391146\n",
      "iteration 5679, loss: 0.006309499032795429\n",
      "iteration 5680, loss: 0.005930914543569088\n",
      "iteration 5681, loss: 0.006842568516731262\n",
      "iteration 5682, loss: 0.006057442165911198\n",
      "iteration 5683, loss: 0.0064575509168207645\n",
      "iteration 5684, loss: 0.00729872053489089\n",
      "iteration 5685, loss: 0.006875905673950911\n",
      "iteration 5686, loss: 0.005704800598323345\n",
      "iteration 5687, loss: 0.006215706001967192\n",
      "iteration 5688, loss: 0.006360990926623344\n",
      "iteration 5689, loss: 0.006226213648915291\n",
      "iteration 5690, loss: 0.007466479204595089\n",
      "iteration 5691, loss: 0.00667707109823823\n",
      "iteration 5692, loss: 0.00640508159995079\n",
      "iteration 5693, loss: 0.006927552632987499\n",
      "iteration 5694, loss: 0.0056904759258031845\n",
      "iteration 5695, loss: 0.007278063800185919\n",
      "iteration 5696, loss: 0.006743993144482374\n",
      "iteration 5697, loss: 0.006510341539978981\n",
      "iteration 5698, loss: 0.005895302165299654\n",
      "iteration 5699, loss: 0.006653734017163515\n",
      "iteration 5700, loss: 0.006967473775148392\n",
      "iteration 5701, loss: 0.006075805984437466\n",
      "iteration 5702, loss: 0.0066278474405407906\n",
      "iteration 5703, loss: 0.006563236005604267\n",
      "iteration 5704, loss: 0.006742309778928757\n",
      "iteration 5705, loss: 0.00840752199292183\n",
      "iteration 5706, loss: 0.006225802004337311\n",
      "iteration 5707, loss: 0.004994753282517195\n",
      "iteration 5708, loss: 0.006885900162160397\n",
      "iteration 5709, loss: 0.006695718504488468\n",
      "iteration 5710, loss: 0.006479821167886257\n",
      "iteration 5711, loss: 0.006697908043861389\n",
      "iteration 5712, loss: 0.006539918482303619\n",
      "iteration 5713, loss: 0.007319950498640537\n",
      "iteration 5714, loss: 0.007666838821023703\n",
      "iteration 5715, loss: 0.005512834060937166\n",
      "iteration 5716, loss: 0.005946471355855465\n",
      "iteration 5717, loss: 0.007104892749339342\n",
      "iteration 5718, loss: 0.006442226469516754\n",
      "iteration 5719, loss: 0.006000700406730175\n",
      "iteration 5720, loss: 0.0066397543996572495\n",
      "iteration 5721, loss: 0.007032871246337891\n",
      "iteration 5722, loss: 0.007516070269048214\n",
      "iteration 5723, loss: 0.006334495730698109\n",
      "iteration 5724, loss: 0.006074786186218262\n",
      "iteration 5725, loss: 0.007459534797817469\n",
      "iteration 5726, loss: 0.006002245470881462\n",
      "iteration 5727, loss: 0.0069948588497936726\n",
      "iteration 5728, loss: 0.006096450611948967\n",
      "iteration 5729, loss: 0.006822825409471989\n",
      "iteration 5730, loss: 0.005384707823395729\n",
      "iteration 5731, loss: 0.006657679099589586\n",
      "iteration 5732, loss: 0.006473219953477383\n",
      "iteration 5733, loss: 0.006433796603232622\n",
      "iteration 5734, loss: 0.006872480735182762\n",
      "iteration 5735, loss: 0.006357245147228241\n",
      "iteration 5736, loss: 0.005591234192252159\n",
      "iteration 5737, loss: 0.005604799836874008\n",
      "iteration 5738, loss: 0.005811833776533604\n",
      "iteration 5739, loss: 0.007273373194038868\n",
      "iteration 5740, loss: 0.006720744073390961\n",
      "iteration 5741, loss: 0.006656008772552013\n",
      "iteration 5742, loss: 0.005909457802772522\n",
      "iteration 5743, loss: 0.007015028037130833\n",
      "iteration 5744, loss: 0.006446165964007378\n",
      "iteration 5745, loss: 0.006074739154428244\n",
      "iteration 5746, loss: 0.005821706727147102\n",
      "iteration 5747, loss: 0.006424775347113609\n",
      "iteration 5748, loss: 0.007452249992638826\n",
      "iteration 5749, loss: 0.006294526159763336\n",
      "iteration 5750, loss: 0.006254809908568859\n",
      "iteration 5751, loss: 0.0062994519248604774\n",
      "iteration 5752, loss: 0.005185882095247507\n",
      "iteration 5753, loss: 0.006497347727417946\n",
      "iteration 5754, loss: 0.005974362604320049\n",
      "iteration 5755, loss: 0.006025334820151329\n",
      "iteration 5756, loss: 0.0065235854126513\n",
      "iteration 5757, loss: 0.006119530647993088\n",
      "iteration 5758, loss: 0.005760283209383488\n",
      "iteration 5759, loss: 0.006008939351886511\n",
      "iteration 5760, loss: 0.00590939586982131\n",
      "iteration 5761, loss: 0.005180465057492256\n",
      "iteration 5762, loss: 0.00500455591827631\n",
      "iteration 5763, loss: 0.004925156943500042\n",
      "iteration 5764, loss: 0.006092638708651066\n",
      "iteration 5765, loss: 0.00658378517255187\n",
      "iteration 5766, loss: 0.005713808350265026\n",
      "iteration 5767, loss: 0.005381344351917505\n",
      "iteration 5768, loss: 0.007049561478197575\n",
      "iteration 5769, loss: 0.0073684509843587875\n",
      "iteration 5770, loss: 0.005633747670799494\n",
      "iteration 5771, loss: 0.005983181297779083\n",
      "iteration 5772, loss: 0.007242504972964525\n",
      "iteration 5773, loss: 0.006116434931755066\n",
      "iteration 5774, loss: 0.006149553693830967\n",
      "iteration 5775, loss: 0.00684832688421011\n",
      "iteration 5776, loss: 0.005452282726764679\n",
      "iteration 5777, loss: 0.006934559438377619\n",
      "iteration 5778, loss: 0.007355101872235537\n",
      "iteration 5779, loss: 0.006180909927934408\n",
      "iteration 5780, loss: 0.006780953612178564\n",
      "iteration 5781, loss: 0.006254466716200113\n",
      "iteration 5782, loss: 0.005474102217704058\n",
      "iteration 5783, loss: 0.005742715671658516\n",
      "iteration 5784, loss: 0.007501782383769751\n",
      "iteration 5785, loss: 0.006547073367983103\n",
      "iteration 5786, loss: 0.005853228736668825\n",
      "iteration 5787, loss: 0.005441638175398111\n",
      "iteration 5788, loss: 0.005900168791413307\n",
      "iteration 5789, loss: 0.00553327240049839\n",
      "iteration 5790, loss: 0.0071861157193779945\n",
      "iteration 5791, loss: 0.007169198710471392\n",
      "iteration 5792, loss: 0.00668512424454093\n",
      "iteration 5793, loss: 0.006823346950113773\n",
      "iteration 5794, loss: 0.0070286220870912075\n",
      "iteration 5795, loss: 0.0063901981338858604\n",
      "iteration 5796, loss: 0.00600337702780962\n",
      "iteration 5797, loss: 0.005447623319923878\n",
      "iteration 5798, loss: 0.007112122140824795\n",
      "iteration 5799, loss: 0.007163518574088812\n",
      "iteration 5800, loss: 0.007048307918012142\n",
      "iteration 5801, loss: 0.0064126611687242985\n",
      "iteration 5802, loss: 0.0061838882975280285\n",
      "iteration 5803, loss: 0.006561370100826025\n",
      "iteration 5804, loss: 0.005478447303175926\n",
      "iteration 5805, loss: 0.007218504324555397\n",
      "iteration 5806, loss: 0.005988837219774723\n",
      "iteration 5807, loss: 0.005881734658032656\n",
      "iteration 5808, loss: 0.0052271271124482155\n",
      "iteration 5809, loss: 0.00594027666375041\n",
      "iteration 5810, loss: 0.007309453561902046\n",
      "iteration 5811, loss: 0.005573469214141369\n",
      "iteration 5812, loss: 0.006464317440986633\n",
      "iteration 5813, loss: 0.0065993936732411385\n",
      "iteration 5814, loss: 0.006144490092992783\n",
      "iteration 5815, loss: 0.0064389449544250965\n",
      "iteration 5816, loss: 0.006787941791117191\n",
      "iteration 5817, loss: 0.005628787912428379\n",
      "iteration 5818, loss: 0.005234964191913605\n",
      "iteration 5819, loss: 0.006085876375436783\n",
      "iteration 5820, loss: 0.006092436611652374\n",
      "iteration 5821, loss: 0.005569238215684891\n",
      "iteration 5822, loss: 0.006093533243983984\n",
      "iteration 5823, loss: 0.006625683046877384\n",
      "iteration 5824, loss: 0.005885941907763481\n",
      "iteration 5825, loss: 0.006787756457924843\n",
      "iteration 5826, loss: 0.006379606202244759\n",
      "iteration 5827, loss: 0.005506276618689299\n",
      "iteration 5828, loss: 0.00575726293027401\n",
      "iteration 5829, loss: 0.0068654268980026245\n",
      "iteration 5830, loss: 0.006091325543820858\n",
      "iteration 5831, loss: 0.00649070180952549\n",
      "iteration 5832, loss: 0.005758859682828188\n",
      "iteration 5833, loss: 0.005775962956249714\n",
      "iteration 5834, loss: 0.007115041371434927\n",
      "iteration 5835, loss: 0.007306703366339207\n",
      "iteration 5836, loss: 0.005742146633565426\n",
      "iteration 5837, loss: 0.006262548733502626\n",
      "iteration 5838, loss: 0.006111915223300457\n",
      "iteration 5839, loss: 0.0066970293410122395\n",
      "iteration 5840, loss: 0.005881590768694878\n",
      "iteration 5841, loss: 0.006535341963171959\n",
      "iteration 5842, loss: 0.006428714841604233\n",
      "iteration 5843, loss: 0.00635541882365942\n",
      "iteration 5844, loss: 0.005429508164525032\n",
      "iteration 5845, loss: 0.005371949169784784\n",
      "iteration 5846, loss: 0.005432195961475372\n",
      "iteration 5847, loss: 0.005624232813715935\n",
      "iteration 5848, loss: 0.0063231103122234344\n",
      "iteration 5849, loss: 0.005358975380659103\n",
      "iteration 5850, loss: 0.006583378650248051\n",
      "iteration 5851, loss: 0.006790956482291222\n",
      "iteration 5852, loss: 0.006787607446312904\n",
      "iteration 5853, loss: 0.006165171042084694\n",
      "iteration 5854, loss: 0.00683353329077363\n",
      "iteration 5855, loss: 0.006448965985327959\n",
      "iteration 5856, loss: 0.006192563101649284\n",
      "iteration 5857, loss: 0.005673506297171116\n",
      "iteration 5858, loss: 0.006357188336551189\n",
      "iteration 5859, loss: 0.005566166248172522\n",
      "iteration 5860, loss: 0.005707729607820511\n",
      "iteration 5861, loss: 0.006874577142298222\n",
      "iteration 5862, loss: 0.005252137314528227\n",
      "iteration 5863, loss: 0.005403716117143631\n",
      "iteration 5864, loss: 0.006712079513818026\n",
      "iteration 5865, loss: 0.005879743956029415\n",
      "iteration 5866, loss: 0.005346555262804031\n",
      "iteration 5867, loss: 0.004940269514918327\n",
      "iteration 5868, loss: 0.006289918906986713\n",
      "iteration 5869, loss: 0.00701097771525383\n",
      "iteration 5870, loss: 0.006186893675476313\n",
      "iteration 5871, loss: 0.005775989033281803\n",
      "iteration 5872, loss: 0.006638272665441036\n",
      "iteration 5873, loss: 0.006716569885611534\n",
      "iteration 5874, loss: 0.006091818679124117\n",
      "iteration 5875, loss: 0.006094109266996384\n",
      "iteration 5876, loss: 0.006817846558988094\n",
      "iteration 5877, loss: 0.005344037897884846\n",
      "iteration 5878, loss: 0.006467380560934544\n",
      "iteration 5879, loss: 0.00529527710750699\n",
      "iteration 5880, loss: 0.0065468112006783485\n",
      "iteration 5881, loss: 0.006262483075261116\n",
      "iteration 5882, loss: 0.006391598843038082\n",
      "iteration 5883, loss: 0.006582693196833134\n",
      "iteration 5884, loss: 0.007845655083656311\n",
      "iteration 5885, loss: 0.004750007763504982\n",
      "iteration 5886, loss: 0.00575685128569603\n",
      "iteration 5887, loss: 0.006508768070489168\n",
      "iteration 5888, loss: 0.006460611242800951\n",
      "iteration 5889, loss: 0.005755597725510597\n",
      "iteration 5890, loss: 0.0064062993042171\n",
      "iteration 5891, loss: 0.006184319034218788\n",
      "iteration 5892, loss: 0.004911466501653194\n",
      "iteration 5893, loss: 0.005993001163005829\n",
      "iteration 5894, loss: 0.005977411288768053\n",
      "iteration 5895, loss: 0.005908699706196785\n",
      "iteration 5896, loss: 0.004982498940080404\n",
      "iteration 5897, loss: 0.007205613888800144\n",
      "iteration 5898, loss: 0.006150701083242893\n",
      "iteration 5899, loss: 0.00804866198450327\n",
      "iteration 5900, loss: 0.006633637472987175\n",
      "iteration 5901, loss: 0.005764724221080542\n",
      "iteration 5902, loss: 0.007302670739591122\n",
      "iteration 5903, loss: 0.006176690571010113\n",
      "iteration 5904, loss: 0.005138567183166742\n",
      "iteration 5905, loss: 0.006558811757713556\n",
      "iteration 5906, loss: 0.0070628877729177475\n",
      "iteration 5907, loss: 0.0067670829594135284\n",
      "iteration 5908, loss: 0.006230898201465607\n",
      "iteration 5909, loss: 0.006911096628755331\n",
      "iteration 5910, loss: 0.006927371025085449\n",
      "iteration 5911, loss: 0.008205581456422806\n",
      "iteration 5912, loss: 0.006872320082038641\n",
      "iteration 5913, loss: 0.006356917321681976\n",
      "iteration 5914, loss: 0.006676346994936466\n",
      "iteration 5915, loss: 0.006704031955450773\n",
      "iteration 5916, loss: 0.006507182028144598\n",
      "iteration 5917, loss: 0.005437105428427458\n",
      "iteration 5918, loss: 0.006060021463781595\n",
      "iteration 5919, loss: 0.006303135771304369\n",
      "iteration 5920, loss: 0.0051383995451033115\n",
      "iteration 5921, loss: 0.0050361743196845055\n",
      "iteration 5922, loss: 0.00702743511646986\n",
      "iteration 5923, loss: 0.006002036854624748\n",
      "iteration 5924, loss: 0.006612996570765972\n",
      "iteration 5925, loss: 0.005264134146273136\n",
      "iteration 5926, loss: 0.005153282545506954\n",
      "iteration 5927, loss: 0.0061887530609965324\n",
      "iteration 5928, loss: 0.0058048260398209095\n",
      "iteration 5929, loss: 0.005692473612725735\n",
      "iteration 5930, loss: 0.006691461428999901\n",
      "iteration 5931, loss: 0.005811522714793682\n",
      "iteration 5932, loss: 0.006192672997713089\n",
      "iteration 5933, loss: 0.006386332679539919\n",
      "iteration 5934, loss: 0.00602584145963192\n",
      "iteration 5935, loss: 0.00626026839017868\n",
      "iteration 5936, loss: 0.005754702724516392\n",
      "iteration 5937, loss: 0.005858344957232475\n",
      "iteration 5938, loss: 0.005746678914874792\n",
      "iteration 5939, loss: 0.004890456795692444\n",
      "iteration 5940, loss: 0.005726420786231756\n",
      "iteration 5941, loss: 0.006086551118642092\n",
      "iteration 5942, loss: 0.006260559894144535\n",
      "iteration 5943, loss: 0.006292866542935371\n",
      "iteration 5944, loss: 0.006039600819349289\n",
      "iteration 5945, loss: 0.006869704928249121\n",
      "iteration 5946, loss: 0.007004329934716225\n",
      "iteration 5947, loss: 0.006194395013153553\n",
      "iteration 5948, loss: 0.005486265756189823\n",
      "iteration 5949, loss: 0.005699819885194302\n",
      "iteration 5950, loss: 0.006720988545566797\n",
      "iteration 5951, loss: 0.005322454497218132\n",
      "iteration 5952, loss: 0.004924704320728779\n",
      "iteration 5953, loss: 0.005009048152714968\n",
      "iteration 5954, loss: 0.00589217571541667\n",
      "iteration 5955, loss: 0.005775074474513531\n",
      "iteration 5956, loss: 0.0060204011388123035\n",
      "iteration 5957, loss: 0.00615193834528327\n",
      "iteration 5958, loss: 0.004915585275739431\n",
      "iteration 5959, loss: 0.0061434609815478325\n",
      "iteration 5960, loss: 0.006718139164149761\n",
      "iteration 5961, loss: 0.006157323718070984\n",
      "iteration 5962, loss: 0.005111311096698046\n",
      "iteration 5963, loss: 0.005701891612261534\n",
      "iteration 5964, loss: 0.0065970392897725105\n",
      "iteration 5965, loss: 0.005518622696399689\n",
      "iteration 5966, loss: 0.005234315525740385\n",
      "iteration 5967, loss: 0.00514646340161562\n",
      "iteration 5968, loss: 0.005432636011391878\n",
      "iteration 5969, loss: 0.00568810012191534\n",
      "iteration 5970, loss: 0.005964383482933044\n",
      "iteration 5971, loss: 0.006442564073950052\n",
      "iteration 5972, loss: 0.005325996316969395\n",
      "iteration 5973, loss: 0.005558189004659653\n",
      "iteration 5974, loss: 0.006027450319379568\n",
      "iteration 5975, loss: 0.006836475804448128\n",
      "iteration 5976, loss: 0.0060154651291668415\n",
      "iteration 5977, loss: 0.005241744685918093\n",
      "iteration 5978, loss: 0.005585387349128723\n",
      "iteration 5979, loss: 0.0048733483999967575\n",
      "iteration 5980, loss: 0.005838567391037941\n",
      "iteration 5981, loss: 0.004913000389933586\n",
      "iteration 5982, loss: 0.006042244844138622\n",
      "iteration 5983, loss: 0.005445702467113733\n",
      "iteration 5984, loss: 0.005153944715857506\n",
      "iteration 5985, loss: 0.0050241416320204735\n",
      "iteration 5986, loss: 0.005170670337975025\n",
      "iteration 5987, loss: 0.005945240147411823\n",
      "iteration 5988, loss: 0.005937873385846615\n",
      "iteration 5989, loss: 0.005842672660946846\n",
      "iteration 5990, loss: 0.005913671106100082\n",
      "iteration 5991, loss: 0.005623622797429562\n",
      "iteration 5992, loss: 0.006582712754607201\n",
      "iteration 5993, loss: 0.006806243676692247\n",
      "iteration 5994, loss: 0.005481722764670849\n",
      "iteration 5995, loss: 0.005841952748596668\n",
      "iteration 5996, loss: 0.005801930092275143\n",
      "iteration 5997, loss: 0.005666548386216164\n",
      "iteration 5998, loss: 0.0059935953468084335\n",
      "iteration 5999, loss: 0.005712995771318674\n",
      "iteration 6000, loss: 0.0052553219720721245\n",
      "iteration 6001, loss: 0.006151437759399414\n",
      "iteration 6002, loss: 0.005210659001022577\n",
      "iteration 6003, loss: 0.005971813574433327\n",
      "iteration 6004, loss: 0.005779408384114504\n",
      "iteration 6005, loss: 0.006028749980032444\n",
      "iteration 6006, loss: 0.006016349419951439\n",
      "iteration 6007, loss: 0.00597385736182332\n",
      "iteration 6008, loss: 0.005580051802098751\n",
      "iteration 6009, loss: 0.007003655657172203\n",
      "iteration 6010, loss: 0.006759887561202049\n",
      "iteration 6011, loss: 0.0065990714356303215\n",
      "iteration 6012, loss: 0.0068627470172941685\n",
      "iteration 6013, loss: 0.006232419982552528\n",
      "iteration 6014, loss: 0.0049372161738574505\n",
      "iteration 6015, loss: 0.006517031230032444\n",
      "iteration 6016, loss: 0.006447697523981333\n",
      "iteration 6017, loss: 0.005635295528918505\n",
      "iteration 6018, loss: 0.005867662839591503\n",
      "iteration 6019, loss: 0.005910906940698624\n",
      "iteration 6020, loss: 0.0066972216591238976\n",
      "iteration 6021, loss: 0.005577742122113705\n",
      "iteration 6022, loss: 0.0068597691133618355\n",
      "iteration 6023, loss: 0.0052528781816363335\n",
      "iteration 6024, loss: 0.005508014932274818\n",
      "iteration 6025, loss: 0.005775308236479759\n",
      "iteration 6026, loss: 0.007798860780894756\n",
      "iteration 6027, loss: 0.006005176808685064\n",
      "iteration 6028, loss: 0.005251280963420868\n",
      "iteration 6029, loss: 0.006492805667221546\n",
      "iteration 6030, loss: 0.005648321937769651\n",
      "iteration 6031, loss: 0.0055894022807478905\n",
      "iteration 6032, loss: 0.006058714352548122\n",
      "iteration 6033, loss: 0.006378022953867912\n",
      "iteration 6034, loss: 0.005249508656561375\n",
      "iteration 6035, loss: 0.004793571308255196\n",
      "iteration 6036, loss: 0.0058193933218717575\n",
      "iteration 6037, loss: 0.006268481723964214\n",
      "iteration 6038, loss: 0.004689822904765606\n",
      "iteration 6039, loss: 0.0053298212587833405\n",
      "iteration 6040, loss: 0.005955211818218231\n",
      "iteration 6041, loss: 0.005159127525985241\n",
      "iteration 6042, loss: 0.005091914441436529\n",
      "iteration 6043, loss: 0.005807274021208286\n",
      "iteration 6044, loss: 0.005777277052402496\n",
      "iteration 6045, loss: 0.0057237534783780575\n",
      "iteration 6046, loss: 0.005574614275246859\n",
      "iteration 6047, loss: 0.0062677618116140366\n",
      "iteration 6048, loss: 0.0072809429839253426\n",
      "iteration 6049, loss: 0.005959838628768921\n",
      "iteration 6050, loss: 0.006292611360549927\n",
      "iteration 6051, loss: 0.00562471617013216\n",
      "iteration 6052, loss: 0.005013367161154747\n",
      "iteration 6053, loss: 0.005923465825617313\n",
      "iteration 6054, loss: 0.0061282264068722725\n",
      "iteration 6055, loss: 0.005627871491014957\n",
      "iteration 6056, loss: 0.006116163916885853\n",
      "iteration 6057, loss: 0.0059524886310100555\n",
      "iteration 6058, loss: 0.007090924773365259\n",
      "iteration 6059, loss: 0.005937791429460049\n",
      "iteration 6060, loss: 0.0066060638055205345\n",
      "iteration 6061, loss: 0.00572203379124403\n",
      "iteration 6062, loss: 0.006557406857609749\n",
      "iteration 6063, loss: 0.006769396364688873\n",
      "iteration 6064, loss: 0.005863445810973644\n",
      "iteration 6065, loss: 0.005252103321254253\n",
      "iteration 6066, loss: 0.005832004360854626\n",
      "iteration 6067, loss: 0.005777963437139988\n",
      "iteration 6068, loss: 0.00539307901635766\n",
      "iteration 6069, loss: 0.005881922319531441\n",
      "iteration 6070, loss: 0.005510705057531595\n",
      "iteration 6071, loss: 0.005805801600217819\n",
      "iteration 6072, loss: 0.006001859903335571\n",
      "iteration 6073, loss: 0.005607301369309425\n",
      "iteration 6074, loss: 0.005575506016612053\n",
      "iteration 6075, loss: 0.005434677004814148\n",
      "iteration 6076, loss: 0.005625755060464144\n",
      "iteration 6077, loss: 0.00622808001935482\n",
      "iteration 6078, loss: 0.007487154565751553\n",
      "iteration 6079, loss: 0.0062582725659012794\n",
      "iteration 6080, loss: 0.005206473171710968\n",
      "iteration 6081, loss: 0.004855578765273094\n",
      "iteration 6082, loss: 0.006346015725284815\n",
      "iteration 6083, loss: 0.00717310793697834\n",
      "iteration 6084, loss: 0.006548976060003042\n",
      "iteration 6085, loss: 0.005248866509646177\n",
      "iteration 6086, loss: 0.006449501030147076\n",
      "iteration 6087, loss: 0.005297113209962845\n",
      "iteration 6088, loss: 0.006337552797049284\n",
      "iteration 6089, loss: 0.005419745575636625\n",
      "iteration 6090, loss: 0.0051923757418990135\n",
      "iteration 6091, loss: 0.006304945331066847\n",
      "iteration 6092, loss: 0.005294087342917919\n",
      "iteration 6093, loss: 0.005410642828792334\n",
      "iteration 6094, loss: 0.006409371271729469\n",
      "iteration 6095, loss: 0.006124746985733509\n",
      "iteration 6096, loss: 0.006209133192896843\n",
      "iteration 6097, loss: 0.005863877013325691\n",
      "iteration 6098, loss: 0.005414879415184259\n",
      "iteration 6099, loss: 0.005829300731420517\n",
      "iteration 6100, loss: 0.005496635101735592\n",
      "iteration 6101, loss: 0.0046149492263793945\n",
      "iteration 6102, loss: 0.0059475670568645\n",
      "iteration 6103, loss: 0.005488858558237553\n",
      "iteration 6104, loss: 0.0052111269906163216\n",
      "iteration 6105, loss: 0.005678701680153608\n",
      "iteration 6106, loss: 0.006351812742650509\n",
      "iteration 6107, loss: 0.007006186991930008\n",
      "iteration 6108, loss: 0.006094994954764843\n",
      "iteration 6109, loss: 0.005876217968761921\n",
      "iteration 6110, loss: 0.0056557129137218\n",
      "iteration 6111, loss: 0.005415749736130238\n",
      "iteration 6112, loss: 0.005331791006028652\n",
      "iteration 6113, loss: 0.005652089603245258\n",
      "iteration 6114, loss: 0.005751130171120167\n",
      "iteration 6115, loss: 0.005735749378800392\n",
      "iteration 6116, loss: 0.005604630336165428\n",
      "iteration 6117, loss: 0.007282738573849201\n",
      "iteration 6118, loss: 0.005225717090070248\n",
      "iteration 6119, loss: 0.006185461301356554\n",
      "iteration 6120, loss: 0.0053601814433932304\n",
      "iteration 6121, loss: 0.004516120068728924\n",
      "iteration 6122, loss: 0.005439951084554195\n",
      "iteration 6123, loss: 0.0050912220031023026\n",
      "iteration 6124, loss: 0.005801228806376457\n",
      "iteration 6125, loss: 0.005598976276814938\n",
      "iteration 6126, loss: 0.006046556401997805\n",
      "iteration 6127, loss: 0.005160558968782425\n",
      "iteration 6128, loss: 0.004863807000219822\n",
      "iteration 6129, loss: 0.006494753994047642\n",
      "iteration 6130, loss: 0.005208679009228945\n",
      "iteration 6131, loss: 0.005317346192896366\n",
      "iteration 6132, loss: 0.005883046425879002\n",
      "iteration 6133, loss: 0.006267956458032131\n",
      "iteration 6134, loss: 0.005559457466006279\n",
      "iteration 6135, loss: 0.004868968389928341\n",
      "iteration 6136, loss: 0.005108975805342197\n",
      "iteration 6137, loss: 0.005246648099273443\n",
      "iteration 6138, loss: 0.005640294402837753\n",
      "iteration 6139, loss: 0.0045696282759308815\n",
      "iteration 6140, loss: 0.004987492226064205\n",
      "iteration 6141, loss: 0.0051712822169065475\n",
      "iteration 6142, loss: 0.005812730640172958\n",
      "iteration 6143, loss: 0.005513627547770739\n",
      "iteration 6144, loss: 0.005018765106797218\n",
      "iteration 6145, loss: 0.005664056167006493\n",
      "iteration 6146, loss: 0.0055407313629984856\n",
      "iteration 6147, loss: 0.005716538056731224\n",
      "iteration 6148, loss: 0.0058772326447069645\n",
      "iteration 6149, loss: 0.00626761931926012\n",
      "iteration 6150, loss: 0.005304216407239437\n",
      "iteration 6151, loss: 0.007062757853418589\n",
      "iteration 6152, loss: 0.005734137259423733\n",
      "iteration 6153, loss: 0.005992466118186712\n",
      "iteration 6154, loss: 0.005822916515171528\n",
      "iteration 6155, loss: 0.00654677813872695\n",
      "iteration 6156, loss: 0.005314145237207413\n",
      "iteration 6157, loss: 0.00623799255117774\n",
      "iteration 6158, loss: 0.004616634454578161\n",
      "iteration 6159, loss: 0.005450837314128876\n",
      "iteration 6160, loss: 0.00617911946028471\n",
      "iteration 6161, loss: 0.006220432464033365\n",
      "iteration 6162, loss: 0.005968834273517132\n",
      "iteration 6163, loss: 0.0064511988312006\n",
      "iteration 6164, loss: 0.005353795364499092\n",
      "iteration 6165, loss: 0.005083727650344372\n",
      "iteration 6166, loss: 0.005267101805657148\n",
      "iteration 6167, loss: 0.005571211222559214\n",
      "iteration 6168, loss: 0.006540839560329914\n",
      "iteration 6169, loss: 0.004999096505343914\n",
      "iteration 6170, loss: 0.005370236001908779\n",
      "iteration 6171, loss: 0.006733627058565617\n",
      "iteration 6172, loss: 0.005758678540587425\n",
      "iteration 6173, loss: 0.0062257712706923485\n",
      "iteration 6174, loss: 0.006756157614290714\n",
      "iteration 6175, loss: 0.004813461564481258\n",
      "iteration 6176, loss: 0.0066386135295033455\n",
      "iteration 6177, loss: 0.00558911869302392\n",
      "iteration 6178, loss: 0.005372366867959499\n",
      "iteration 6179, loss: 0.005435977131128311\n",
      "iteration 6180, loss: 0.005754675716161728\n",
      "iteration 6181, loss: 0.005326779559254646\n",
      "iteration 6182, loss: 0.0047987159341573715\n",
      "iteration 6183, loss: 0.005235087126493454\n",
      "iteration 6184, loss: 0.006596757099032402\n",
      "iteration 6185, loss: 0.005433844868093729\n",
      "iteration 6186, loss: 0.007480164058506489\n",
      "iteration 6187, loss: 0.005769332870841026\n",
      "iteration 6188, loss: 0.006171255838125944\n",
      "iteration 6189, loss: 0.0050298431888222694\n",
      "iteration 6190, loss: 0.006290998309850693\n",
      "iteration 6191, loss: 0.007179799489676952\n",
      "iteration 6192, loss: 0.005325339734554291\n",
      "iteration 6193, loss: 0.00553127983585\n",
      "iteration 6194, loss: 0.0071825603954494\n",
      "iteration 6195, loss: 0.005377616733312607\n",
      "iteration 6196, loss: 0.0058088283985853195\n",
      "iteration 6197, loss: 0.005250145215541124\n",
      "iteration 6198, loss: 0.006307361181825399\n",
      "iteration 6199, loss: 0.00676827784627676\n",
      "iteration 6200, loss: 0.0059859915636479855\n",
      "iteration 6201, loss: 0.006431044545024633\n",
      "iteration 6202, loss: 0.005348977632820606\n",
      "iteration 6203, loss: 0.00532944779843092\n",
      "iteration 6204, loss: 0.005351852625608444\n",
      "iteration 6205, loss: 0.005484603811055422\n",
      "iteration 6206, loss: 0.007159873843193054\n",
      "iteration 6207, loss: 0.006107130087912083\n",
      "iteration 6208, loss: 0.00597846694290638\n",
      "iteration 6209, loss: 0.005774240475147963\n",
      "iteration 6210, loss: 0.0056149703450500965\n",
      "iteration 6211, loss: 0.006307301111519337\n",
      "iteration 6212, loss: 0.005568644497543573\n",
      "iteration 6213, loss: 0.004958758130669594\n",
      "iteration 6214, loss: 0.004895886406302452\n",
      "iteration 6215, loss: 0.005777974613010883\n",
      "iteration 6216, loss: 0.004988400265574455\n",
      "iteration 6217, loss: 0.005299578420817852\n",
      "iteration 6218, loss: 0.005790515337139368\n",
      "iteration 6219, loss: 0.0056202104315161705\n",
      "iteration 6220, loss: 0.006006232462823391\n",
      "iteration 6221, loss: 0.005394541658461094\n",
      "iteration 6222, loss: 0.006918986327946186\n",
      "iteration 6223, loss: 0.005781209096312523\n",
      "iteration 6224, loss: 0.006351211108267307\n",
      "iteration 6225, loss: 0.005905390717089176\n",
      "iteration 6226, loss: 0.005901875905692577\n",
      "iteration 6227, loss: 0.0063683451153337955\n",
      "iteration 6228, loss: 0.006403788458555937\n",
      "iteration 6229, loss: 0.006946577690541744\n",
      "iteration 6230, loss: 0.004765072837471962\n",
      "iteration 6231, loss: 0.006068185903131962\n",
      "iteration 6232, loss: 0.006159748882055283\n",
      "iteration 6233, loss: 0.0059147062711417675\n",
      "iteration 6234, loss: 0.005973354447633028\n",
      "iteration 6235, loss: 0.006278885528445244\n",
      "iteration 6236, loss: 0.005321537144482136\n",
      "iteration 6237, loss: 0.005870447028428316\n",
      "iteration 6238, loss: 0.006411430425941944\n",
      "iteration 6239, loss: 0.005246532149612904\n",
      "iteration 6240, loss: 0.005964864976704121\n",
      "iteration 6241, loss: 0.004884491674602032\n",
      "iteration 6242, loss: 0.00533801456913352\n",
      "iteration 6243, loss: 0.006241472437977791\n",
      "iteration 6244, loss: 0.006824337877333164\n",
      "iteration 6245, loss: 0.0061154598370194435\n",
      "iteration 6246, loss: 0.006652937736362219\n",
      "iteration 6247, loss: 0.00554835656657815\n",
      "iteration 6248, loss: 0.005757927428930998\n",
      "iteration 6249, loss: 0.006363885477185249\n",
      "iteration 6250, loss: 0.005967648699879646\n",
      "iteration 6251, loss: 0.0070024640299379826\n",
      "iteration 6252, loss: 0.005468928720802069\n",
      "iteration 6253, loss: 0.006919276900589466\n",
      "iteration 6254, loss: 0.0057455324567854404\n",
      "iteration 6255, loss: 0.007070363033562899\n",
      "iteration 6256, loss: 0.006870081648230553\n",
      "iteration 6257, loss: 0.006737354677170515\n",
      "iteration 6258, loss: 0.006042099557816982\n",
      "iteration 6259, loss: 0.004879169166088104\n",
      "iteration 6260, loss: 0.005540733225643635\n",
      "iteration 6261, loss: 0.005688062869012356\n",
      "iteration 6262, loss: 0.005567366257309914\n",
      "iteration 6263, loss: 0.0056247832253575325\n",
      "iteration 6264, loss: 0.005660470575094223\n",
      "iteration 6265, loss: 0.005743023939430714\n",
      "iteration 6266, loss: 0.0056418864987790585\n",
      "iteration 6267, loss: 0.006111219059675932\n",
      "iteration 6268, loss: 0.005304322112351656\n",
      "iteration 6269, loss: 0.0052301171235740185\n",
      "iteration 6270, loss: 0.005382767878472805\n",
      "iteration 6271, loss: 0.005960366688668728\n",
      "iteration 6272, loss: 0.005554754287004471\n",
      "iteration 6273, loss: 0.0054943193681538105\n",
      "iteration 6274, loss: 0.005472484510391951\n",
      "iteration 6275, loss: 0.005588230676949024\n",
      "iteration 6276, loss: 0.006906845606863499\n",
      "iteration 6277, loss: 0.005678710527718067\n",
      "iteration 6278, loss: 0.005999797489494085\n",
      "iteration 6279, loss: 0.005423856433480978\n",
      "iteration 6280, loss: 0.005924585275352001\n",
      "iteration 6281, loss: 0.005746613256633282\n",
      "iteration 6282, loss: 0.006236742250621319\n",
      "iteration 6283, loss: 0.006542246788740158\n",
      "iteration 6284, loss: 0.005729577504098415\n",
      "iteration 6285, loss: 0.005267714150249958\n",
      "iteration 6286, loss: 0.005866842344403267\n",
      "iteration 6287, loss: 0.004972266964614391\n",
      "iteration 6288, loss: 0.006519329734146595\n",
      "iteration 6289, loss: 0.005480217281728983\n",
      "iteration 6290, loss: 0.005840734578669071\n",
      "iteration 6291, loss: 0.0059755477122962475\n",
      "iteration 6292, loss: 0.004961431957781315\n",
      "iteration 6293, loss: 0.005041494965553284\n",
      "iteration 6294, loss: 0.004359752871096134\n",
      "iteration 6295, loss: 0.006328179966658354\n",
      "iteration 6296, loss: 0.00540580041706562\n",
      "iteration 6297, loss: 0.005599568132311106\n",
      "iteration 6298, loss: 0.007044420577585697\n",
      "iteration 6299, loss: 0.006426832173019648\n",
      "iteration 6300, loss: 0.005240130238234997\n",
      "iteration 6301, loss: 0.006568433716893196\n",
      "iteration 6302, loss: 0.006447915453463793\n",
      "iteration 6303, loss: 0.0056759146973490715\n",
      "iteration 6304, loss: 0.005437173880636692\n",
      "iteration 6305, loss: 0.005586464889347553\n",
      "iteration 6306, loss: 0.005068029277026653\n",
      "iteration 6307, loss: 0.005587138235569\n",
      "iteration 6308, loss: 0.006287562660872936\n",
      "iteration 6309, loss: 0.004968516994267702\n",
      "iteration 6310, loss: 0.006446708459407091\n",
      "iteration 6311, loss: 0.005378939677029848\n",
      "iteration 6312, loss: 0.005698810797184706\n",
      "iteration 6313, loss: 0.005567680578678846\n",
      "iteration 6314, loss: 0.006593877915292978\n",
      "iteration 6315, loss: 0.005472961813211441\n",
      "iteration 6316, loss: 0.005150653887540102\n",
      "iteration 6317, loss: 0.006316312588751316\n",
      "iteration 6318, loss: 0.0056482525542378426\n",
      "iteration 6319, loss: 0.006289598997682333\n",
      "iteration 6320, loss: 0.005625420715659857\n",
      "iteration 6321, loss: 0.005322893150150776\n",
      "iteration 6322, loss: 0.005747221410274506\n",
      "iteration 6323, loss: 0.0052223606035113335\n",
      "iteration 6324, loss: 0.0056349243968725204\n",
      "iteration 6325, loss: 0.005837844219058752\n",
      "iteration 6326, loss: 0.0049467068165540695\n",
      "iteration 6327, loss: 0.004888306837528944\n",
      "iteration 6328, loss: 0.004704584367573261\n",
      "iteration 6329, loss: 0.00618242472410202\n",
      "iteration 6330, loss: 0.00681864470243454\n",
      "iteration 6331, loss: 0.0050728945061564445\n",
      "iteration 6332, loss: 0.0050887856632471085\n",
      "iteration 6333, loss: 0.005596180446445942\n",
      "iteration 6334, loss: 0.005950236693024635\n",
      "iteration 6335, loss: 0.005791983567178249\n",
      "iteration 6336, loss: 0.005005169659852982\n",
      "iteration 6337, loss: 0.004709976725280285\n",
      "iteration 6338, loss: 0.004913638811558485\n",
      "iteration 6339, loss: 0.005023999605327845\n",
      "iteration 6340, loss: 0.005479499697685242\n",
      "iteration 6341, loss: 0.006045621819794178\n",
      "iteration 6342, loss: 0.005113454069942236\n",
      "iteration 6343, loss: 0.005062788724899292\n",
      "iteration 6344, loss: 0.005807211622595787\n",
      "iteration 6345, loss: 0.006013135425746441\n",
      "iteration 6346, loss: 0.005502143409103155\n",
      "iteration 6347, loss: 0.004820533096790314\n",
      "iteration 6348, loss: 0.004852081649005413\n",
      "iteration 6349, loss: 0.005230536684393883\n",
      "iteration 6350, loss: 0.005506252869963646\n",
      "iteration 6351, loss: 0.0058361138217151165\n",
      "iteration 6352, loss: 0.0050939107313752174\n",
      "iteration 6353, loss: 0.004961691331118345\n",
      "iteration 6354, loss: 0.005328471772372723\n",
      "iteration 6355, loss: 0.004865462426096201\n",
      "iteration 6356, loss: 0.005045396275818348\n",
      "iteration 6357, loss: 0.006232651881873608\n",
      "iteration 6358, loss: 0.006142736412584782\n",
      "iteration 6359, loss: 0.004422635771334171\n",
      "iteration 6360, loss: 0.0062093669548630714\n",
      "iteration 6361, loss: 0.005931301042437553\n",
      "iteration 6362, loss: 0.005047699902206659\n",
      "iteration 6363, loss: 0.005583332851529121\n",
      "iteration 6364, loss: 0.005547357723116875\n",
      "iteration 6365, loss: 0.0060866521671414375\n",
      "iteration 6366, loss: 0.006972339935600758\n",
      "iteration 6367, loss: 0.005535111762583256\n",
      "iteration 6368, loss: 0.005563301965594292\n",
      "iteration 6369, loss: 0.005187963135540485\n",
      "iteration 6370, loss: 0.004991147667169571\n",
      "iteration 6371, loss: 0.005674716085195541\n",
      "iteration 6372, loss: 0.005577456206083298\n",
      "iteration 6373, loss: 0.005597329698503017\n",
      "iteration 6374, loss: 0.006509069353342056\n",
      "iteration 6375, loss: 0.006043986417353153\n",
      "iteration 6376, loss: 0.0057100821286439896\n",
      "iteration 6377, loss: 0.005119582638144493\n",
      "iteration 6378, loss: 0.005262005142867565\n",
      "iteration 6379, loss: 0.006139989476650953\n",
      "iteration 6380, loss: 0.005116260144859552\n",
      "iteration 6381, loss: 0.005293702706694603\n",
      "iteration 6382, loss: 0.0052809519693255424\n",
      "iteration 6383, loss: 0.004686453379690647\n",
      "iteration 6384, loss: 0.0050875055603682995\n",
      "iteration 6385, loss: 0.005371192470192909\n",
      "iteration 6386, loss: 0.005789341405034065\n",
      "iteration 6387, loss: 0.0047275107353925705\n",
      "iteration 6388, loss: 0.0049636634066700935\n",
      "iteration 6389, loss: 0.004266303963959217\n",
      "iteration 6390, loss: 0.005217129364609718\n",
      "iteration 6391, loss: 0.005580224096775055\n",
      "iteration 6392, loss: 0.004995342344045639\n",
      "iteration 6393, loss: 0.005768763832747936\n",
      "iteration 6394, loss: 0.005222964100539684\n",
      "iteration 6395, loss: 0.005309091880917549\n",
      "iteration 6396, loss: 0.0056626442819833755\n",
      "iteration 6397, loss: 0.005382026545703411\n",
      "iteration 6398, loss: 0.005022706463932991\n",
      "iteration 6399, loss: 0.005724071990698576\n",
      "iteration 6400, loss: 0.005837827920913696\n",
      "iteration 6401, loss: 0.006021806038916111\n",
      "iteration 6402, loss: 0.005526731722056866\n",
      "iteration 6403, loss: 0.004645054694265127\n",
      "iteration 6404, loss: 0.005770289339125156\n",
      "iteration 6405, loss: 0.005106402561068535\n",
      "iteration 6406, loss: 0.005675900261849165\n",
      "iteration 6407, loss: 0.005898988805711269\n",
      "iteration 6408, loss: 0.005941196344792843\n",
      "iteration 6409, loss: 0.005743415094912052\n",
      "iteration 6410, loss: 0.0058503043837845325\n",
      "iteration 6411, loss: 0.004999031312763691\n",
      "iteration 6412, loss: 0.005877768620848656\n",
      "iteration 6413, loss: 0.005370994098484516\n",
      "iteration 6414, loss: 0.005741949193179607\n",
      "iteration 6415, loss: 0.005401305388659239\n",
      "iteration 6416, loss: 0.005238458048552275\n",
      "iteration 6417, loss: 0.005607118830084801\n",
      "iteration 6418, loss: 0.004535582847893238\n",
      "iteration 6419, loss: 0.005627688951790333\n",
      "iteration 6420, loss: 0.00591782433912158\n",
      "iteration 6421, loss: 0.005188344977796078\n",
      "iteration 6422, loss: 0.006146273110061884\n",
      "iteration 6423, loss: 0.006284905131906271\n",
      "iteration 6424, loss: 0.005066830199211836\n",
      "iteration 6425, loss: 0.005000852979719639\n",
      "iteration 6426, loss: 0.005447604227811098\n",
      "iteration 6427, loss: 0.00506144855171442\n",
      "iteration 6428, loss: 0.00535949133336544\n",
      "iteration 6429, loss: 0.005710810422897339\n",
      "iteration 6430, loss: 0.005634488072246313\n",
      "iteration 6431, loss: 0.005488710477948189\n",
      "iteration 6432, loss: 0.005546601489186287\n",
      "iteration 6433, loss: 0.005060119554400444\n",
      "iteration 6434, loss: 0.005068235099315643\n",
      "iteration 6435, loss: 0.004806879907846451\n",
      "iteration 6436, loss: 0.005597266834229231\n",
      "iteration 6437, loss: 0.00476291636005044\n",
      "iteration 6438, loss: 0.005311436951160431\n",
      "iteration 6439, loss: 0.006028058938682079\n",
      "iteration 6440, loss: 0.005080507602542639\n",
      "iteration 6441, loss: 0.005578231066465378\n",
      "iteration 6442, loss: 0.004807422868907452\n",
      "iteration 6443, loss: 0.005380604881793261\n",
      "iteration 6444, loss: 0.005357998423278332\n",
      "iteration 6445, loss: 0.0049833254888653755\n",
      "iteration 6446, loss: 0.006441163364797831\n",
      "iteration 6447, loss: 0.0052033644169569016\n",
      "iteration 6448, loss: 0.005143759306520224\n",
      "iteration 6449, loss: 0.005248917266726494\n",
      "iteration 6450, loss: 0.00508747436106205\n",
      "iteration 6451, loss: 0.00484368484467268\n",
      "iteration 6452, loss: 0.006322894245386124\n",
      "iteration 6453, loss: 0.00547832902520895\n",
      "iteration 6454, loss: 0.005841362755745649\n",
      "iteration 6455, loss: 0.004694784060120583\n",
      "iteration 6456, loss: 0.006207362748682499\n",
      "iteration 6457, loss: 0.00584352295845747\n",
      "iteration 6458, loss: 0.005797985475510359\n",
      "iteration 6459, loss: 0.006174630951136351\n",
      "iteration 6460, loss: 0.005732259247452021\n",
      "iteration 6461, loss: 0.005225649103522301\n",
      "iteration 6462, loss: 0.007014676928520203\n",
      "iteration 6463, loss: 0.0051712715066969395\n",
      "iteration 6464, loss: 0.004615797195583582\n",
      "iteration 6465, loss: 0.005832253023982048\n",
      "iteration 6466, loss: 0.005811769049614668\n",
      "iteration 6467, loss: 0.004945419728755951\n",
      "iteration 6468, loss: 0.005842119920998812\n",
      "iteration 6469, loss: 0.005466913804411888\n",
      "iteration 6470, loss: 0.005881697870790958\n",
      "iteration 6471, loss: 0.005172786768525839\n",
      "iteration 6472, loss: 0.004415735602378845\n",
      "iteration 6473, loss: 0.006450032815337181\n",
      "iteration 6474, loss: 0.006175371818244457\n",
      "iteration 6475, loss: 0.006048665381968021\n",
      "iteration 6476, loss: 0.005503972060978413\n",
      "iteration 6477, loss: 0.004272662103176117\n",
      "iteration 6478, loss: 0.004896374419331551\n",
      "iteration 6479, loss: 0.006194750778377056\n",
      "iteration 6480, loss: 0.006257676053792238\n",
      "iteration 6481, loss: 0.00503434706479311\n",
      "iteration 6482, loss: 0.006290297023952007\n",
      "iteration 6483, loss: 0.006064475979655981\n",
      "iteration 6484, loss: 0.005878871772438288\n",
      "iteration 6485, loss: 0.0053001767955720425\n",
      "iteration 6486, loss: 0.005231206305325031\n",
      "iteration 6487, loss: 0.00463074678555131\n",
      "iteration 6488, loss: 0.005626948084682226\n",
      "iteration 6489, loss: 0.005386596079915762\n",
      "iteration 6490, loss: 0.0049878861755132675\n",
      "iteration 6491, loss: 0.006594870705157518\n",
      "iteration 6492, loss: 0.00459268968552351\n",
      "iteration 6493, loss: 0.004670864902436733\n",
      "iteration 6494, loss: 0.00519179180264473\n",
      "iteration 6495, loss: 0.005706028081476688\n",
      "iteration 6496, loss: 0.004898396320641041\n",
      "iteration 6497, loss: 0.006033736281096935\n",
      "iteration 6498, loss: 0.004955026786774397\n",
      "iteration 6499, loss: 0.0055238110944628716\n",
      "iteration 6500, loss: 0.005830582231283188\n",
      "iteration 6501, loss: 0.006448907777667046\n",
      "iteration 6502, loss: 0.004190437030047178\n",
      "iteration 6503, loss: 0.00563451275229454\n",
      "iteration 6504, loss: 0.005422031506896019\n",
      "iteration 6505, loss: 0.0048722741194069386\n",
      "iteration 6506, loss: 0.005153017584234476\n",
      "iteration 6507, loss: 0.004621218889951706\n",
      "iteration 6508, loss: 0.005586246959865093\n",
      "iteration 6509, loss: 0.004886536858975887\n",
      "iteration 6510, loss: 0.005546986125409603\n",
      "iteration 6511, loss: 0.004643218591809273\n",
      "iteration 6512, loss: 0.005143302958458662\n",
      "iteration 6513, loss: 0.005203578155487776\n",
      "iteration 6514, loss: 0.005006621591746807\n",
      "iteration 6515, loss: 0.004956590943038464\n",
      "iteration 6516, loss: 0.00539771094918251\n",
      "iteration 6517, loss: 0.0058921636082232\n",
      "iteration 6518, loss: 0.005184636451303959\n",
      "iteration 6519, loss: 0.004301100969314575\n",
      "iteration 6520, loss: 0.004851063713431358\n",
      "iteration 6521, loss: 0.005386959761381149\n",
      "iteration 6522, loss: 0.004612877499312162\n",
      "iteration 6523, loss: 0.004883516579866409\n",
      "iteration 6524, loss: 0.005234966520220041\n",
      "iteration 6525, loss: 0.0057889437302947044\n",
      "iteration 6526, loss: 0.0050928471609950066\n",
      "iteration 6527, loss: 0.005121942609548569\n",
      "iteration 6528, loss: 0.0055902921594679356\n",
      "iteration 6529, loss: 0.006212923210114241\n",
      "iteration 6530, loss: 0.005784990265965462\n",
      "iteration 6531, loss: 0.0055302223190665245\n",
      "iteration 6532, loss: 0.006218527443706989\n",
      "iteration 6533, loss: 0.005126959178596735\n",
      "iteration 6534, loss: 0.005387107376009226\n",
      "iteration 6535, loss: 0.005562628619372845\n",
      "iteration 6536, loss: 0.005537929944694042\n",
      "iteration 6537, loss: 0.005851617082953453\n",
      "iteration 6538, loss: 0.0050227949395775795\n",
      "iteration 6539, loss: 0.00509948655962944\n",
      "iteration 6540, loss: 0.0059653641656041145\n",
      "iteration 6541, loss: 0.0050356267020106316\n",
      "iteration 6542, loss: 0.005234160926192999\n",
      "iteration 6543, loss: 0.0057063475251197815\n",
      "iteration 6544, loss: 0.00579361617565155\n",
      "iteration 6545, loss: 0.005784101318567991\n",
      "iteration 6546, loss: 0.005643995013087988\n",
      "iteration 6547, loss: 0.006182005628943443\n",
      "iteration 6548, loss: 0.005455165170133114\n",
      "iteration 6549, loss: 0.00498682726174593\n",
      "iteration 6550, loss: 0.005587179213762283\n",
      "iteration 6551, loss: 0.005583113059401512\n",
      "iteration 6552, loss: 0.006196250207722187\n",
      "iteration 6553, loss: 0.004799692891538143\n",
      "iteration 6554, loss: 0.005309425760060549\n",
      "iteration 6555, loss: 0.0048781633377075195\n",
      "iteration 6556, loss: 0.005330600775778294\n",
      "iteration 6557, loss: 0.006561620160937309\n",
      "iteration 6558, loss: 0.004677223041653633\n",
      "iteration 6559, loss: 0.0052144285291433334\n",
      "iteration 6560, loss: 0.005347825586795807\n",
      "iteration 6561, loss: 0.005839996039867401\n",
      "iteration 6562, loss: 0.0042639560997486115\n",
      "iteration 6563, loss: 0.005797108169645071\n",
      "iteration 6564, loss: 0.00492300046607852\n",
      "iteration 6565, loss: 0.00505408039316535\n",
      "iteration 6566, loss: 0.005782836116850376\n",
      "iteration 6567, loss: 0.005363799631595612\n",
      "iteration 6568, loss: 0.005162307061254978\n",
      "iteration 6569, loss: 0.005534926895052195\n",
      "iteration 6570, loss: 0.0057630715891718864\n",
      "iteration 6571, loss: 0.005438489373773336\n",
      "iteration 6572, loss: 0.005839201621711254\n",
      "iteration 6573, loss: 0.004996803589165211\n",
      "iteration 6574, loss: 0.005401523783802986\n",
      "iteration 6575, loss: 0.004777371883392334\n",
      "iteration 6576, loss: 0.005698747932910919\n",
      "iteration 6577, loss: 0.004407615400850773\n",
      "iteration 6578, loss: 0.004721594508737326\n",
      "iteration 6579, loss: 0.005311604123562574\n",
      "iteration 6580, loss: 0.005225787404924631\n",
      "iteration 6581, loss: 0.004712096881121397\n",
      "iteration 6582, loss: 0.005461087450385094\n",
      "iteration 6583, loss: 0.004846640862524509\n",
      "iteration 6584, loss: 0.005802149884402752\n",
      "iteration 6585, loss: 0.005198215134441853\n",
      "iteration 6586, loss: 0.005138058215379715\n",
      "iteration 6587, loss: 0.004712671972811222\n",
      "iteration 6588, loss: 0.005407241638749838\n",
      "iteration 6589, loss: 0.005580813158303499\n",
      "iteration 6590, loss: 0.005478474777191877\n",
      "iteration 6591, loss: 0.006341724656522274\n",
      "iteration 6592, loss: 0.004798064474016428\n",
      "iteration 6593, loss: 0.0059376247227191925\n",
      "iteration 6594, loss: 0.005496286787092686\n",
      "iteration 6595, loss: 0.004694641567766666\n",
      "iteration 6596, loss: 0.006086129695177078\n",
      "iteration 6597, loss: 0.005883361678570509\n",
      "iteration 6598, loss: 0.004488795530050993\n",
      "iteration 6599, loss: 0.00636229757219553\n",
      "iteration 6600, loss: 0.0047531453892588615\n",
      "iteration 6601, loss: 0.005702341441065073\n",
      "iteration 6602, loss: 0.005313033703714609\n",
      "iteration 6603, loss: 0.005339137744158506\n",
      "iteration 6604, loss: 0.004972912836819887\n",
      "iteration 6605, loss: 0.005256804637610912\n",
      "iteration 6606, loss: 0.0061470260843634605\n",
      "iteration 6607, loss: 0.006072012707591057\n",
      "iteration 6608, loss: 0.005507327616214752\n",
      "iteration 6609, loss: 0.0046961065381765366\n",
      "iteration 6610, loss: 0.004426219034940004\n",
      "iteration 6611, loss: 0.004990539513528347\n",
      "iteration 6612, loss: 0.00646216468885541\n",
      "iteration 6613, loss: 0.006282336078584194\n",
      "iteration 6614, loss: 0.005354758817702532\n",
      "iteration 6615, loss: 0.004805302247405052\n",
      "iteration 6616, loss: 0.005879852920770645\n",
      "iteration 6617, loss: 0.005735273938626051\n",
      "iteration 6618, loss: 0.005461730062961578\n",
      "iteration 6619, loss: 0.005357699003070593\n",
      "iteration 6620, loss: 0.006727835163474083\n",
      "iteration 6621, loss: 0.005853853654116392\n",
      "iteration 6622, loss: 0.005365966819226742\n",
      "iteration 6623, loss: 0.005561624653637409\n",
      "iteration 6624, loss: 0.005926036275923252\n",
      "iteration 6625, loss: 0.005134299397468567\n",
      "iteration 6626, loss: 0.005414474289864302\n",
      "iteration 6627, loss: 0.006143072620034218\n",
      "iteration 6628, loss: 0.005152486264705658\n",
      "iteration 6629, loss: 0.005018090829253197\n",
      "iteration 6630, loss: 0.00492201279848814\n",
      "iteration 6631, loss: 0.005564074032008648\n",
      "iteration 6632, loss: 0.005079771392047405\n",
      "iteration 6633, loss: 0.0061960709281265736\n",
      "iteration 6634, loss: 0.004623135551810265\n",
      "iteration 6635, loss: 0.005824700929224491\n",
      "iteration 6636, loss: 0.004560000263154507\n",
      "iteration 6637, loss: 0.005886049009859562\n",
      "iteration 6638, loss: 0.005239679478108883\n",
      "iteration 6639, loss: 0.005735329817980528\n",
      "iteration 6640, loss: 0.0050417231395840645\n",
      "iteration 6641, loss: 0.004427919164299965\n",
      "iteration 6642, loss: 0.004573323298245668\n",
      "iteration 6643, loss: 0.0050354525446891785\n",
      "iteration 6644, loss: 0.00531103927642107\n",
      "iteration 6645, loss: 0.0048964316956698895\n",
      "iteration 6646, loss: 0.004141912795603275\n",
      "iteration 6647, loss: 0.005363755859434605\n",
      "iteration 6648, loss: 0.004439406096935272\n",
      "iteration 6649, loss: 0.004828499164432287\n",
      "iteration 6650, loss: 0.005715486593544483\n",
      "iteration 6651, loss: 0.0052876658737659454\n",
      "iteration 6652, loss: 0.005384683609008789\n",
      "iteration 6653, loss: 0.004843372851610184\n",
      "iteration 6654, loss: 0.005100882146507502\n",
      "iteration 6655, loss: 0.004366222303360701\n",
      "iteration 6656, loss: 0.005609704647213221\n",
      "iteration 6657, loss: 0.005353492684662342\n",
      "iteration 6658, loss: 0.005119273439049721\n",
      "iteration 6659, loss: 0.005183102563023567\n",
      "iteration 6660, loss: 0.005462823901325464\n",
      "iteration 6661, loss: 0.005544767715036869\n",
      "iteration 6662, loss: 0.00516819441691041\n",
      "iteration 6663, loss: 0.005516920704394579\n",
      "iteration 6664, loss: 0.005619308911263943\n",
      "iteration 6665, loss: 0.004709260538220406\n",
      "iteration 6666, loss: 0.005724651739001274\n",
      "iteration 6667, loss: 0.00504877045750618\n",
      "iteration 6668, loss: 0.0058805132284760475\n",
      "iteration 6669, loss: 0.0051155914552509785\n",
      "iteration 6670, loss: 0.005988316610455513\n",
      "iteration 6671, loss: 0.005339306779205799\n",
      "iteration 6672, loss: 0.004800831899046898\n",
      "iteration 6673, loss: 0.005640992894768715\n",
      "iteration 6674, loss: 0.004835037514567375\n",
      "iteration 6675, loss: 0.005000503733754158\n",
      "iteration 6676, loss: 0.004771625157445669\n",
      "iteration 6677, loss: 0.004677383229136467\n",
      "iteration 6678, loss: 0.005507512949407101\n",
      "iteration 6679, loss: 0.004553722217679024\n",
      "iteration 6680, loss: 0.0051821377128362656\n",
      "iteration 6681, loss: 0.0048019904643297195\n",
      "iteration 6682, loss: 0.005087491124868393\n",
      "iteration 6683, loss: 0.004265584051609039\n",
      "iteration 6684, loss: 0.005099263042211533\n",
      "iteration 6685, loss: 0.004976905416697264\n",
      "iteration 6686, loss: 0.004377951845526695\n",
      "iteration 6687, loss: 0.005359653849154711\n",
      "iteration 6688, loss: 0.005149028729647398\n",
      "iteration 6689, loss: 0.004684871062636375\n",
      "iteration 6690, loss: 0.004613806493580341\n",
      "iteration 6691, loss: 0.004729697946459055\n",
      "iteration 6692, loss: 0.004874460399150848\n",
      "iteration 6693, loss: 0.0051247235387563705\n",
      "iteration 6694, loss: 0.004981641657650471\n",
      "iteration 6695, loss: 0.006968013476580381\n",
      "iteration 6696, loss: 0.004320945590734482\n",
      "iteration 6697, loss: 0.004692140035331249\n",
      "iteration 6698, loss: 0.0054860832169651985\n",
      "iteration 6699, loss: 0.005401714704930782\n",
      "iteration 6700, loss: 0.005396736785769463\n",
      "iteration 6701, loss: 0.00472414493560791\n",
      "iteration 6702, loss: 0.0051202294416725636\n",
      "iteration 6703, loss: 0.00510867265984416\n",
      "iteration 6704, loss: 0.006148874759674072\n",
      "iteration 6705, loss: 0.005097179673612118\n",
      "iteration 6706, loss: 0.005501313600689173\n",
      "iteration 6707, loss: 0.005540977232158184\n",
      "iteration 6708, loss: 0.005239348858594894\n",
      "iteration 6709, loss: 0.00473952479660511\n",
      "iteration 6710, loss: 0.004749319516122341\n",
      "iteration 6711, loss: 0.004880934953689575\n",
      "iteration 6712, loss: 0.00473274290561676\n",
      "iteration 6713, loss: 0.005144855938851833\n",
      "iteration 6714, loss: 0.004839027300477028\n",
      "iteration 6715, loss: 0.005253652110695839\n",
      "iteration 6716, loss: 0.005612870212644339\n",
      "iteration 6717, loss: 0.005725310184061527\n",
      "iteration 6718, loss: 0.005276805721223354\n",
      "iteration 6719, loss: 0.004280190449208021\n",
      "iteration 6720, loss: 0.004340093117207289\n",
      "iteration 6721, loss: 0.004545345902442932\n",
      "iteration 6722, loss: 0.005320005584508181\n",
      "iteration 6723, loss: 0.005220848135650158\n",
      "iteration 6724, loss: 0.004829098470509052\n",
      "iteration 6725, loss: 0.004752103239297867\n",
      "iteration 6726, loss: 0.005044640973210335\n",
      "iteration 6727, loss: 0.005749545991420746\n",
      "iteration 6728, loss: 0.005450624041259289\n",
      "iteration 6729, loss: 0.005910294130444527\n",
      "iteration 6730, loss: 0.005485082045197487\n",
      "iteration 6731, loss: 0.005273692309856415\n",
      "iteration 6732, loss: 0.005320307333022356\n",
      "iteration 6733, loss: 0.00460242573171854\n",
      "iteration 6734, loss: 0.006256644614040852\n",
      "iteration 6735, loss: 0.00578423123806715\n",
      "iteration 6736, loss: 0.004487755708396435\n",
      "iteration 6737, loss: 0.005046104080975056\n",
      "iteration 6738, loss: 0.00540537666529417\n",
      "iteration 6739, loss: 0.0052351634949445724\n",
      "iteration 6740, loss: 0.00494752312079072\n",
      "iteration 6741, loss: 0.005214713513851166\n",
      "iteration 6742, loss: 0.004930525086820126\n",
      "iteration 6743, loss: 0.005758843384683132\n",
      "iteration 6744, loss: 0.00496278703212738\n",
      "iteration 6745, loss: 0.004795639775693417\n",
      "iteration 6746, loss: 0.005740173161029816\n",
      "iteration 6747, loss: 0.005434509366750717\n",
      "iteration 6748, loss: 0.004809063859283924\n",
      "iteration 6749, loss: 0.005621057935059071\n",
      "iteration 6750, loss: 0.005166697781533003\n",
      "iteration 6751, loss: 0.004921796265989542\n",
      "iteration 6752, loss: 0.0043061235919594765\n",
      "iteration 6753, loss: 0.005160393659025431\n",
      "iteration 6754, loss: 0.0052034384571015835\n",
      "iteration 6755, loss: 0.004723107907921076\n",
      "iteration 6756, loss: 0.005476776044815779\n",
      "iteration 6757, loss: 0.005469252355396748\n",
      "iteration 6758, loss: 0.00458342581987381\n",
      "iteration 6759, loss: 0.005024760030210018\n",
      "iteration 6760, loss: 0.004255923442542553\n",
      "iteration 6761, loss: 0.004758201539516449\n",
      "iteration 6762, loss: 0.005416307132691145\n",
      "iteration 6763, loss: 0.005674965679645538\n",
      "iteration 6764, loss: 0.004779503680765629\n",
      "iteration 6765, loss: 0.005238111596554518\n",
      "iteration 6766, loss: 0.004599499516189098\n",
      "iteration 6767, loss: 0.004774510394781828\n",
      "iteration 6768, loss: 0.0046181888319551945\n",
      "iteration 6769, loss: 0.006155550479888916\n",
      "iteration 6770, loss: 0.00550390500575304\n",
      "iteration 6771, loss: 0.004683519247919321\n",
      "iteration 6772, loss: 0.0052563780918717384\n",
      "iteration 6773, loss: 0.0052338335663080215\n",
      "iteration 6774, loss: 0.006171523593366146\n",
      "iteration 6775, loss: 0.005346700083464384\n",
      "iteration 6776, loss: 0.0057279253378510475\n",
      "iteration 6777, loss: 0.004881423898041248\n",
      "iteration 6778, loss: 0.005498378537595272\n",
      "iteration 6779, loss: 0.005272211506962776\n",
      "iteration 6780, loss: 0.005111328326165676\n",
      "iteration 6781, loss: 0.005368490237742662\n",
      "iteration 6782, loss: 0.00605387007817626\n",
      "iteration 6783, loss: 0.005359825678169727\n",
      "iteration 6784, loss: 0.00588676892220974\n",
      "iteration 6785, loss: 0.006223740521818399\n",
      "iteration 6786, loss: 0.005027000792324543\n",
      "iteration 6787, loss: 0.005847605876624584\n",
      "iteration 6788, loss: 0.004511107690632343\n",
      "iteration 6789, loss: 0.004452401772141457\n",
      "iteration 6790, loss: 0.004882726818323135\n",
      "iteration 6791, loss: 0.005264031235128641\n",
      "iteration 6792, loss: 0.005178581923246384\n",
      "iteration 6793, loss: 0.0050114174373447895\n",
      "iteration 6794, loss: 0.0048442743718624115\n",
      "iteration 6795, loss: 0.005441867280751467\n",
      "iteration 6796, loss: 0.004782103467732668\n",
      "iteration 6797, loss: 0.0049588740803301334\n",
      "iteration 6798, loss: 0.0037461717147380114\n",
      "iteration 6799, loss: 0.004718787968158722\n",
      "iteration 6800, loss: 0.005225125700235367\n",
      "iteration 6801, loss: 0.004965615924447775\n",
      "iteration 6802, loss: 0.00497543765231967\n",
      "iteration 6803, loss: 0.0048025525175035\n",
      "iteration 6804, loss: 0.005978789646178484\n",
      "iteration 6805, loss: 0.004751709755510092\n",
      "iteration 6806, loss: 0.005335456226021051\n",
      "iteration 6807, loss: 0.005809984169900417\n",
      "iteration 6808, loss: 0.004660463891923428\n",
      "iteration 6809, loss: 0.005355633795261383\n",
      "iteration 6810, loss: 0.005306493025273085\n",
      "iteration 6811, loss: 0.004279058426618576\n",
      "iteration 6812, loss: 0.005620036274194717\n",
      "iteration 6813, loss: 0.0054889279417693615\n",
      "iteration 6814, loss: 0.004071315284818411\n",
      "iteration 6815, loss: 0.005134676117449999\n",
      "iteration 6816, loss: 0.004254197236150503\n",
      "iteration 6817, loss: 0.005399844143539667\n",
      "iteration 6818, loss: 0.004363908898085356\n",
      "iteration 6819, loss: 0.004802048671990633\n",
      "iteration 6820, loss: 0.00446409173309803\n",
      "iteration 6821, loss: 0.005204540677368641\n",
      "iteration 6822, loss: 0.004468702711164951\n",
      "iteration 6823, loss: 0.004868901334702969\n",
      "iteration 6824, loss: 0.006804491858929396\n",
      "iteration 6825, loss: 0.004674999043345451\n",
      "iteration 6826, loss: 0.005269469693303108\n",
      "iteration 6827, loss: 0.005320955533534288\n",
      "iteration 6828, loss: 0.0044599901884794235\n",
      "iteration 6829, loss: 0.005687692202627659\n",
      "iteration 6830, loss: 0.004784291610121727\n",
      "iteration 6831, loss: 0.004422611091285944\n",
      "iteration 6832, loss: 0.005600641947239637\n",
      "iteration 6833, loss: 0.004520894959568977\n",
      "iteration 6834, loss: 0.0043819257989525795\n",
      "iteration 6835, loss: 0.004548884928226471\n",
      "iteration 6836, loss: 0.0048352619633078575\n",
      "iteration 6837, loss: 0.005293502472341061\n",
      "iteration 6838, loss: 0.005235899705439806\n",
      "iteration 6839, loss: 0.005086028017103672\n",
      "iteration 6840, loss: 0.005104375071823597\n",
      "iteration 6841, loss: 0.0051378654316067696\n",
      "iteration 6842, loss: 0.004076265264302492\n",
      "iteration 6843, loss: 0.004491222091019154\n",
      "iteration 6844, loss: 0.0045129600912332535\n",
      "iteration 6845, loss: 0.004344100598245859\n",
      "iteration 6846, loss: 0.0050436751917004585\n",
      "iteration 6847, loss: 0.004462461918592453\n",
      "iteration 6848, loss: 0.0046397908590734005\n",
      "iteration 6849, loss: 0.004583332687616348\n",
      "iteration 6850, loss: 0.004272555932402611\n",
      "iteration 6851, loss: 0.004101909231394529\n",
      "iteration 6852, loss: 0.004759804345667362\n",
      "iteration 6853, loss: 0.004777491092681885\n",
      "iteration 6854, loss: 0.005005004815757275\n",
      "iteration 6855, loss: 0.004082005470991135\n",
      "iteration 6856, loss: 0.004373875912278891\n",
      "iteration 6857, loss: 0.005173009354621172\n",
      "iteration 6858, loss: 0.004389459267258644\n",
      "iteration 6859, loss: 0.0047385795041918755\n",
      "iteration 6860, loss: 0.00486443005502224\n",
      "iteration 6861, loss: 0.004334436729550362\n",
      "iteration 6862, loss: 0.005423217546194792\n",
      "iteration 6863, loss: 0.0045895054936409\n",
      "iteration 6864, loss: 0.005412485916167498\n",
      "iteration 6865, loss: 0.00547331478446722\n",
      "iteration 6866, loss: 0.004903084598481655\n",
      "iteration 6867, loss: 0.0053229606710374355\n",
      "iteration 6868, loss: 0.0035685759503394365\n",
      "iteration 6869, loss: 0.004580870270729065\n",
      "iteration 6870, loss: 0.004522357601672411\n",
      "iteration 6871, loss: 0.004870252683758736\n",
      "iteration 6872, loss: 0.004379616118967533\n",
      "iteration 6873, loss: 0.005278637632727623\n",
      "iteration 6874, loss: 0.005524307489395142\n",
      "iteration 6875, loss: 0.004236174747347832\n",
      "iteration 6876, loss: 0.004192837979644537\n",
      "iteration 6877, loss: 0.0053596217185258865\n",
      "iteration 6878, loss: 0.004591438453644514\n",
      "iteration 6879, loss: 0.004523832350969315\n",
      "iteration 6880, loss: 0.0048972237855196\n",
      "iteration 6881, loss: 0.004727226682007313\n",
      "iteration 6882, loss: 0.00594220170751214\n",
      "iteration 6883, loss: 0.005191533826291561\n",
      "iteration 6884, loss: 0.004608619026839733\n",
      "iteration 6885, loss: 0.004723262973129749\n",
      "iteration 6886, loss: 0.004551168996840715\n",
      "iteration 6887, loss: 0.004835749976336956\n",
      "iteration 6888, loss: 0.004667461849749088\n",
      "iteration 6889, loss: 0.004593079909682274\n",
      "iteration 6890, loss: 0.005586748942732811\n",
      "iteration 6891, loss: 0.004623241722583771\n",
      "iteration 6892, loss: 0.005193356424570084\n",
      "iteration 6893, loss: 0.004293736070394516\n",
      "iteration 6894, loss: 0.005993214435875416\n",
      "iteration 6895, loss: 0.003988820128142834\n",
      "iteration 6896, loss: 0.005362285301089287\n",
      "iteration 6897, loss: 0.004898393526673317\n",
      "iteration 6898, loss: 0.005472153890877962\n",
      "iteration 6899, loss: 0.005231929011642933\n",
      "iteration 6900, loss: 0.004510111175477505\n",
      "iteration 6901, loss: 0.0046328045427799225\n",
      "iteration 6902, loss: 0.00541355786845088\n",
      "iteration 6903, loss: 0.00546732172369957\n",
      "iteration 6904, loss: 0.005148371681571007\n",
      "iteration 6905, loss: 0.005276049487292767\n",
      "iteration 6906, loss: 0.004783841781318188\n",
      "iteration 6907, loss: 0.004227818921208382\n",
      "iteration 6908, loss: 0.004763012286275625\n",
      "iteration 6909, loss: 0.0053327856585383415\n",
      "iteration 6910, loss: 0.005432233214378357\n",
      "iteration 6911, loss: 0.004242860712110996\n",
      "iteration 6912, loss: 0.005088919773697853\n",
      "iteration 6913, loss: 0.0056748585775494576\n",
      "iteration 6914, loss: 0.004949290305376053\n",
      "iteration 6915, loss: 0.004235418513417244\n",
      "iteration 6916, loss: 0.005128286778926849\n",
      "iteration 6917, loss: 0.005216244608163834\n",
      "iteration 6918, loss: 0.005384891759604216\n",
      "iteration 6919, loss: 0.004600687883794308\n",
      "iteration 6920, loss: 0.004871118813753128\n",
      "iteration 6921, loss: 0.004169029649347067\n",
      "iteration 6922, loss: 0.004710639826953411\n",
      "iteration 6923, loss: 0.004366984125226736\n",
      "iteration 6924, loss: 0.0050519309006631374\n",
      "iteration 6925, loss: 0.004216846544295549\n",
      "iteration 6926, loss: 0.005875427275896072\n",
      "iteration 6927, loss: 0.0046010008081793785\n",
      "iteration 6928, loss: 0.005380570888519287\n",
      "iteration 6929, loss: 0.004952294752001762\n",
      "iteration 6930, loss: 0.004198640119284391\n",
      "iteration 6931, loss: 0.005701178219169378\n",
      "iteration 6932, loss: 0.00523302610963583\n",
      "iteration 6933, loss: 0.0038788961246609688\n",
      "iteration 6934, loss: 0.00500012282282114\n",
      "iteration 6935, loss: 0.005139228422194719\n",
      "iteration 6936, loss: 0.005942385643720627\n",
      "iteration 6937, loss: 0.004589126445353031\n",
      "iteration 6938, loss: 0.005340958014130592\n",
      "iteration 6939, loss: 0.005104770418256521\n",
      "iteration 6940, loss: 0.0058629484847188\n",
      "iteration 6941, loss: 0.005221170838922262\n",
      "iteration 6942, loss: 0.004321419168263674\n",
      "iteration 6943, loss: 0.0050333538092672825\n",
      "iteration 6944, loss: 0.006022028159350157\n",
      "iteration 6945, loss: 0.0047819968312978745\n",
      "iteration 6946, loss: 0.004700211808085442\n",
      "iteration 6947, loss: 0.005864289589226246\n",
      "iteration 6948, loss: 0.005234610289335251\n",
      "iteration 6949, loss: 0.004316363483667374\n",
      "iteration 6950, loss: 0.0051778266206383705\n",
      "iteration 6951, loss: 0.004883039742708206\n",
      "iteration 6952, loss: 0.004462550859898329\n",
      "iteration 6953, loss: 0.00473991222679615\n",
      "iteration 6954, loss: 0.004425513558089733\n",
      "iteration 6955, loss: 0.005008777603507042\n",
      "iteration 6956, loss: 0.005688132718205452\n",
      "iteration 6957, loss: 0.00472651282325387\n",
      "iteration 6958, loss: 0.005320526659488678\n",
      "iteration 6959, loss: 0.005097082816064358\n",
      "iteration 6960, loss: 0.005405823700129986\n",
      "iteration 6961, loss: 0.005611586384475231\n",
      "iteration 6962, loss: 0.004434032831341028\n",
      "iteration 6963, loss: 0.004898451268672943\n",
      "iteration 6964, loss: 0.005118769593536854\n",
      "iteration 6965, loss: 0.005125308409333229\n",
      "iteration 6966, loss: 0.004901882726699114\n",
      "iteration 6967, loss: 0.005309706553816795\n",
      "iteration 6968, loss: 0.004722530487924814\n",
      "iteration 6969, loss: 0.00520634651184082\n",
      "iteration 6970, loss: 0.004640351980924606\n",
      "iteration 6971, loss: 0.004892155062407255\n",
      "iteration 6972, loss: 0.004023479297757149\n",
      "iteration 6973, loss: 0.005407604388892651\n",
      "iteration 6974, loss: 0.0048277294263243675\n",
      "iteration 6975, loss: 0.0038808188401162624\n",
      "iteration 6976, loss: 0.004997133277356625\n",
      "iteration 6977, loss: 0.004480977077037096\n",
      "iteration 6978, loss: 0.004644910804927349\n",
      "iteration 6979, loss: 0.0053227972239255905\n",
      "iteration 6980, loss: 0.004719373304396868\n",
      "iteration 6981, loss: 0.004811189137399197\n",
      "iteration 6982, loss: 0.0042915004305541515\n",
      "iteration 6983, loss: 0.004500212147831917\n",
      "iteration 6984, loss: 0.0051017096266150475\n",
      "iteration 6985, loss: 0.005489751230925322\n",
      "iteration 6986, loss: 0.005618961527943611\n",
      "iteration 6987, loss: 0.004833226092159748\n",
      "iteration 6988, loss: 0.004917290061712265\n",
      "iteration 6989, loss: 0.004344081040471792\n",
      "iteration 6990, loss: 0.005377511028200388\n",
      "iteration 6991, loss: 0.005644453223794699\n",
      "iteration 6992, loss: 0.0044515375047922134\n",
      "iteration 6993, loss: 0.004395570605993271\n",
      "iteration 6994, loss: 0.004650403745472431\n",
      "iteration 6995, loss: 0.004158535972237587\n",
      "iteration 6996, loss: 0.004297913052141666\n",
      "iteration 6997, loss: 0.004427657462656498\n",
      "iteration 6998, loss: 0.00598134147003293\n",
      "iteration 6999, loss: 0.00467905355617404\n",
      "iteration 7000, loss: 0.005691030994057655\n",
      "iteration 7001, loss: 0.004829475190490484\n",
      "iteration 7002, loss: 0.005077887326478958\n",
      "iteration 7003, loss: 0.005716501735150814\n",
      "iteration 7004, loss: 0.004934830591082573\n",
      "iteration 7005, loss: 0.0046594706363976\n",
      "iteration 7006, loss: 0.0046837907284498215\n",
      "iteration 7007, loss: 0.005017412826418877\n",
      "iteration 7008, loss: 0.004531565122306347\n",
      "iteration 7009, loss: 0.0042763156816363335\n",
      "iteration 7010, loss: 0.004355297423899174\n",
      "iteration 7011, loss: 0.0039627524092793465\n",
      "iteration 7012, loss: 0.0047437939792871475\n",
      "iteration 7013, loss: 0.005296111106872559\n",
      "iteration 7014, loss: 0.005327175371348858\n",
      "iteration 7015, loss: 0.003929950296878815\n",
      "iteration 7016, loss: 0.0039627645164728165\n",
      "iteration 7017, loss: 0.005225293803960085\n",
      "iteration 7018, loss: 0.004824742209166288\n",
      "iteration 7019, loss: 0.00445395614951849\n",
      "iteration 7020, loss: 0.0048784115351736546\n",
      "iteration 7021, loss: 0.0041339211165905\n",
      "iteration 7022, loss: 0.004798334091901779\n",
      "iteration 7023, loss: 0.005952861160039902\n",
      "iteration 7024, loss: 0.0044307708740234375\n",
      "iteration 7025, loss: 0.005225172732025385\n",
      "iteration 7026, loss: 0.004108226392418146\n",
      "iteration 7027, loss: 0.005847391672432423\n",
      "iteration 7028, loss: 0.005128112155944109\n",
      "iteration 7029, loss: 0.004908764734864235\n",
      "iteration 7030, loss: 0.004799674265086651\n",
      "iteration 7031, loss: 0.0042023370042443275\n",
      "iteration 7032, loss: 0.003997775726020336\n",
      "iteration 7033, loss: 0.004673069342970848\n",
      "iteration 7034, loss: 0.004779290873557329\n",
      "iteration 7035, loss: 0.004886819049715996\n",
      "iteration 7036, loss: 0.004546684212982655\n",
      "iteration 7037, loss: 0.005031000357121229\n",
      "iteration 7038, loss: 0.005330306012183428\n",
      "iteration 7039, loss: 0.005339908413589001\n",
      "iteration 7040, loss: 0.004717102739959955\n",
      "iteration 7041, loss: 0.005320330150425434\n",
      "iteration 7042, loss: 0.0047953445464372635\n",
      "iteration 7043, loss: 0.0049033514223992825\n",
      "iteration 7044, loss: 0.004893887788057327\n",
      "iteration 7045, loss: 0.004310590215027332\n",
      "iteration 7046, loss: 0.00429243128746748\n",
      "iteration 7047, loss: 0.004515564069151878\n",
      "iteration 7048, loss: 0.004199271090328693\n",
      "iteration 7049, loss: 0.00410458305850625\n",
      "iteration 7050, loss: 0.004693862050771713\n",
      "iteration 7051, loss: 0.004620565101504326\n",
      "iteration 7052, loss: 0.005090745165944099\n",
      "iteration 7053, loss: 0.004282962065190077\n",
      "iteration 7054, loss: 0.005085258278995752\n",
      "iteration 7055, loss: 0.00439638365060091\n",
      "iteration 7056, loss: 0.005078352987766266\n",
      "iteration 7057, loss: 0.004900007508695126\n",
      "iteration 7058, loss: 0.004857022315263748\n",
      "iteration 7059, loss: 0.005212457850575447\n",
      "iteration 7060, loss: 0.004789983853697777\n",
      "iteration 7061, loss: 0.005164117086678743\n",
      "iteration 7062, loss: 0.003967828117311001\n",
      "iteration 7063, loss: 0.004587395116686821\n",
      "iteration 7064, loss: 0.00496648158878088\n",
      "iteration 7065, loss: 0.0040717581287026405\n",
      "iteration 7066, loss: 0.005043655168265104\n",
      "iteration 7067, loss: 0.004333707503974438\n",
      "iteration 7068, loss: 0.004955893382430077\n",
      "iteration 7069, loss: 0.004369613714516163\n",
      "iteration 7070, loss: 0.0051549281924963\n",
      "iteration 7071, loss: 0.004109424538910389\n",
      "iteration 7072, loss: 0.005059640854597092\n",
      "iteration 7073, loss: 0.005075638648122549\n",
      "iteration 7074, loss: 0.0049678971990942955\n",
      "iteration 7075, loss: 0.004440194461494684\n",
      "iteration 7076, loss: 0.004593482241034508\n",
      "iteration 7077, loss: 0.004106313921511173\n",
      "iteration 7078, loss: 0.003867470659315586\n",
      "iteration 7079, loss: 0.004289236385375261\n",
      "iteration 7080, loss: 0.004531862214207649\n",
      "iteration 7081, loss: 0.004488985985517502\n",
      "iteration 7082, loss: 0.00460857292637229\n",
      "iteration 7083, loss: 0.004331762902438641\n",
      "iteration 7084, loss: 0.004167527426034212\n",
      "iteration 7085, loss: 0.0050414372235536575\n",
      "iteration 7086, loss: 0.0041983239352703094\n",
      "iteration 7087, loss: 0.005232734140008688\n",
      "iteration 7088, loss: 0.004783943295478821\n",
      "iteration 7089, loss: 0.004499019123613834\n",
      "iteration 7090, loss: 0.0041655139066278934\n",
      "iteration 7091, loss: 0.0050052134320139885\n",
      "iteration 7092, loss: 0.004974878393113613\n",
      "iteration 7093, loss: 0.004402291961014271\n",
      "iteration 7094, loss: 0.004358737729489803\n",
      "iteration 7095, loss: 0.005406515207141638\n",
      "iteration 7096, loss: 0.004947093315422535\n",
      "iteration 7097, loss: 0.0047707767225801945\n",
      "iteration 7098, loss: 0.004961209371685982\n",
      "iteration 7099, loss: 0.00431411899626255\n",
      "iteration 7100, loss: 0.005428853444755077\n",
      "iteration 7101, loss: 0.004780365154147148\n",
      "iteration 7102, loss: 0.004917692393064499\n",
      "iteration 7103, loss: 0.0048559075221419334\n",
      "iteration 7104, loss: 0.004954911302775145\n",
      "iteration 7105, loss: 0.004168580286204815\n",
      "iteration 7106, loss: 0.004008195362985134\n",
      "iteration 7107, loss: 0.004169953987002373\n",
      "iteration 7108, loss: 0.005555344745516777\n",
      "iteration 7109, loss: 0.004951183218508959\n",
      "iteration 7110, loss: 0.005086937919259071\n",
      "iteration 7111, loss: 0.004903605207800865\n",
      "iteration 7112, loss: 0.004585166461765766\n",
      "iteration 7113, loss: 0.004703152924776077\n",
      "iteration 7114, loss: 0.00461178133264184\n",
      "iteration 7115, loss: 0.005030717235058546\n",
      "iteration 7116, loss: 0.004863222129642963\n",
      "iteration 7117, loss: 0.005025923252105713\n",
      "iteration 7118, loss: 0.005052032880485058\n",
      "iteration 7119, loss: 0.005210746079683304\n",
      "iteration 7120, loss: 0.004979541525244713\n",
      "iteration 7121, loss: 0.0033884223084896803\n",
      "iteration 7122, loss: 0.0045318445190787315\n",
      "iteration 7123, loss: 0.0049156853929162025\n",
      "iteration 7124, loss: 0.0040955012664198875\n",
      "iteration 7125, loss: 0.004261373076587915\n",
      "iteration 7126, loss: 0.004497387446463108\n",
      "iteration 7127, loss: 0.005532916635274887\n",
      "iteration 7128, loss: 0.004967803135514259\n",
      "iteration 7129, loss: 0.004206682555377483\n",
      "iteration 7130, loss: 0.004939769860357046\n",
      "iteration 7131, loss: 0.004492333624511957\n",
      "iteration 7132, loss: 0.005015703849494457\n",
      "iteration 7133, loss: 0.004162916913628578\n",
      "iteration 7134, loss: 0.005016424227505922\n",
      "iteration 7135, loss: 0.0045841531828045845\n",
      "iteration 7136, loss: 0.004338444210588932\n",
      "iteration 7137, loss: 0.00439395010471344\n",
      "iteration 7138, loss: 0.004165705293416977\n",
      "iteration 7139, loss: 0.003552595153450966\n",
      "iteration 7140, loss: 0.005084966775029898\n",
      "iteration 7141, loss: 0.0047195954248309135\n",
      "iteration 7142, loss: 0.004183633252978325\n",
      "iteration 7143, loss: 0.0052001578733325005\n",
      "iteration 7144, loss: 0.004747987724840641\n",
      "iteration 7145, loss: 0.00526445172727108\n",
      "iteration 7146, loss: 0.005531257949769497\n",
      "iteration 7147, loss: 0.004220631904900074\n",
      "iteration 7148, loss: 0.004716765601187944\n",
      "iteration 7149, loss: 0.004702553153038025\n",
      "iteration 7150, loss: 0.0048393323086202145\n",
      "iteration 7151, loss: 0.00590753648430109\n",
      "iteration 7152, loss: 0.004713223781436682\n",
      "iteration 7153, loss: 0.005535866599529982\n",
      "iteration 7154, loss: 0.004656226374208927\n",
      "iteration 7155, loss: 0.005225664004683495\n",
      "iteration 7156, loss: 0.005022366996854544\n",
      "iteration 7157, loss: 0.004545052535831928\n",
      "iteration 7158, loss: 0.004509779158979654\n",
      "iteration 7159, loss: 0.004262740723788738\n",
      "iteration 7160, loss: 0.004440341144800186\n",
      "iteration 7161, loss: 0.004080723039805889\n",
      "iteration 7162, loss: 0.0047189025208354\n",
      "iteration 7163, loss: 0.004977920092642307\n",
      "iteration 7164, loss: 0.004747339524328709\n",
      "iteration 7165, loss: 0.0048101539723575115\n",
      "iteration 7166, loss: 0.004431433044373989\n",
      "iteration 7167, loss: 0.00454086996614933\n",
      "iteration 7168, loss: 0.005333240143954754\n",
      "iteration 7169, loss: 0.005773424170911312\n",
      "iteration 7170, loss: 0.005114641040563583\n",
      "iteration 7171, loss: 0.004102795384824276\n",
      "iteration 7172, loss: 0.004826463293284178\n",
      "iteration 7173, loss: 0.004955411888659\n",
      "iteration 7174, loss: 0.004355509765446186\n",
      "iteration 7175, loss: 0.0047721159644424915\n",
      "iteration 7176, loss: 0.004050496034324169\n",
      "iteration 7177, loss: 0.005204072222113609\n",
      "iteration 7178, loss: 0.005312013905495405\n",
      "iteration 7179, loss: 0.005155664868652821\n",
      "iteration 7180, loss: 0.004312247969210148\n",
      "iteration 7181, loss: 0.003541728248819709\n",
      "iteration 7182, loss: 0.004088925197720528\n",
      "iteration 7183, loss: 0.004814545623958111\n",
      "iteration 7184, loss: 0.004786037374287844\n",
      "iteration 7185, loss: 0.004490221850574017\n",
      "iteration 7186, loss: 0.004236151464283466\n",
      "iteration 7187, loss: 0.004104794468730688\n",
      "iteration 7188, loss: 0.003721841610968113\n",
      "iteration 7189, loss: 0.0043410914950072765\n",
      "iteration 7190, loss: 0.003910109866410494\n",
      "iteration 7191, loss: 0.004465832374989986\n",
      "iteration 7192, loss: 0.005438941530883312\n",
      "iteration 7193, loss: 0.004084686283022165\n",
      "iteration 7194, loss: 0.0038416977040469646\n",
      "iteration 7195, loss: 0.004313556011766195\n",
      "iteration 7196, loss: 0.004733403213322163\n",
      "iteration 7197, loss: 0.004928219132125378\n",
      "iteration 7198, loss: 0.004408951848745346\n",
      "iteration 7199, loss: 0.004849208518862724\n",
      "iteration 7200, loss: 0.004328555427491665\n",
      "iteration 7201, loss: 0.005395935848355293\n",
      "iteration 7202, loss: 0.004967987537384033\n",
      "iteration 7203, loss: 0.004643542692065239\n",
      "iteration 7204, loss: 0.004021327011287212\n",
      "iteration 7205, loss: 0.0039802659302949905\n",
      "iteration 7206, loss: 0.005465405061841011\n",
      "iteration 7207, loss: 0.00429002707824111\n",
      "iteration 7208, loss: 0.004362901672720909\n",
      "iteration 7209, loss: 0.004307212308049202\n",
      "iteration 7210, loss: 0.004367009736597538\n",
      "iteration 7211, loss: 0.004499478731304407\n",
      "iteration 7212, loss: 0.00390960369259119\n",
      "iteration 7213, loss: 0.0042344508692622185\n",
      "iteration 7214, loss: 0.004952483810484409\n",
      "iteration 7215, loss: 0.004550514277070761\n",
      "iteration 7216, loss: 0.005033544264733791\n",
      "iteration 7217, loss: 0.004905503708869219\n",
      "iteration 7218, loss: 0.003872182220220566\n",
      "iteration 7219, loss: 0.004135866649448872\n",
      "iteration 7220, loss: 0.004553042817860842\n",
      "iteration 7221, loss: 0.004515946842730045\n",
      "iteration 7222, loss: 0.005245502572506666\n",
      "iteration 7223, loss: 0.004503991454839706\n",
      "iteration 7224, loss: 0.004604908172041178\n",
      "iteration 7225, loss: 0.004584671929478645\n",
      "iteration 7226, loss: 0.004526007920503616\n",
      "iteration 7227, loss: 0.00500554358586669\n",
      "iteration 7228, loss: 0.004305283538997173\n",
      "iteration 7229, loss: 0.004501853138208389\n",
      "iteration 7230, loss: 0.004125583916902542\n",
      "iteration 7231, loss: 0.005193301942199469\n",
      "iteration 7232, loss: 0.004395464900881052\n",
      "iteration 7233, loss: 0.004697849974036217\n",
      "iteration 7234, loss: 0.003750703763216734\n",
      "iteration 7235, loss: 0.004050246439874172\n",
      "iteration 7236, loss: 0.0038247620686888695\n",
      "iteration 7237, loss: 0.004784764721989632\n",
      "iteration 7238, loss: 0.004491480533033609\n",
      "iteration 7239, loss: 0.00463126040995121\n",
      "iteration 7240, loss: 0.004188778344541788\n",
      "iteration 7241, loss: 0.0038707859348505735\n",
      "iteration 7242, loss: 0.004770112689584494\n",
      "iteration 7243, loss: 0.0042685167863965034\n",
      "iteration 7244, loss: 0.004788490943610668\n",
      "iteration 7245, loss: 0.00433921255171299\n",
      "iteration 7246, loss: 0.004784643184393644\n",
      "iteration 7247, loss: 0.004252888262271881\n",
      "iteration 7248, loss: 0.004521224182099104\n",
      "iteration 7249, loss: 0.00417742133140564\n",
      "iteration 7250, loss: 0.005029492080211639\n",
      "iteration 7251, loss: 0.004064390901476145\n",
      "iteration 7252, loss: 0.004420270211994648\n",
      "iteration 7253, loss: 0.00509067066013813\n",
      "iteration 7254, loss: 0.004083815962076187\n",
      "iteration 7255, loss: 0.00530158169567585\n",
      "iteration 7256, loss: 0.004706753883510828\n",
      "iteration 7257, loss: 0.004185915924608707\n",
      "iteration 7258, loss: 0.0053030820563435555\n",
      "iteration 7259, loss: 0.004026406444609165\n",
      "iteration 7260, loss: 0.005214347504079342\n",
      "iteration 7261, loss: 0.00396055867895484\n",
      "iteration 7262, loss: 0.004660401958972216\n",
      "iteration 7263, loss: 0.0044028302654623985\n",
      "iteration 7264, loss: 0.00479095196351409\n",
      "iteration 7265, loss: 0.005726636853069067\n",
      "iteration 7266, loss: 0.005052383989095688\n",
      "iteration 7267, loss: 0.004619372542947531\n",
      "iteration 7268, loss: 0.004470843356102705\n",
      "iteration 7269, loss: 0.005698377266526222\n",
      "iteration 7270, loss: 0.005385121330618858\n",
      "iteration 7271, loss: 0.006682346574962139\n",
      "iteration 7272, loss: 0.005047298036515713\n",
      "iteration 7273, loss: 0.004323435947299004\n",
      "iteration 7274, loss: 0.0052834320813417435\n",
      "iteration 7275, loss: 0.0040991841815412045\n",
      "iteration 7276, loss: 0.0043968092650175095\n",
      "iteration 7277, loss: 0.005221240222454071\n",
      "iteration 7278, loss: 0.004129507578909397\n",
      "iteration 7279, loss: 0.005499805323779583\n",
      "iteration 7280, loss: 0.004144161008298397\n",
      "iteration 7281, loss: 0.00448848120868206\n",
      "iteration 7282, loss: 0.004156749229878187\n",
      "iteration 7283, loss: 0.004878745414316654\n",
      "iteration 7284, loss: 0.004410249181091785\n",
      "iteration 7285, loss: 0.005223068408668041\n",
      "iteration 7286, loss: 0.005492831114679575\n",
      "iteration 7287, loss: 0.004375577904284\n",
      "iteration 7288, loss: 0.004463255405426025\n",
      "iteration 7289, loss: 0.004913704004138708\n",
      "iteration 7290, loss: 0.004923510365188122\n",
      "iteration 7291, loss: 0.00515334215015173\n",
      "iteration 7292, loss: 0.004028713330626488\n",
      "iteration 7293, loss: 0.004167918115854263\n",
      "iteration 7294, loss: 0.003685904433950782\n",
      "iteration 7295, loss: 0.004694150295108557\n",
      "iteration 7296, loss: 0.005605516955256462\n",
      "iteration 7297, loss: 0.004417284391820431\n",
      "iteration 7298, loss: 0.005556396208703518\n",
      "iteration 7299, loss: 0.00453517772257328\n",
      "iteration 7300, loss: 0.00499392207711935\n",
      "iteration 7301, loss: 0.004738121759146452\n",
      "iteration 7302, loss: 0.0046136160381138325\n",
      "iteration 7303, loss: 0.0041099414229393005\n",
      "iteration 7304, loss: 0.004896421451121569\n",
      "iteration 7305, loss: 0.0044163912534713745\n",
      "iteration 7306, loss: 0.005024310667067766\n",
      "iteration 7307, loss: 0.005732756573706865\n",
      "iteration 7308, loss: 0.005476434249430895\n",
      "iteration 7309, loss: 0.003671620273962617\n",
      "iteration 7310, loss: 0.005739487707614899\n",
      "iteration 7311, loss: 0.004129198379814625\n",
      "iteration 7312, loss: 0.004330361261963844\n",
      "iteration 7313, loss: 0.004805782809853554\n",
      "iteration 7314, loss: 0.0035823362413793802\n",
      "iteration 7315, loss: 0.003951060585677624\n",
      "iteration 7316, loss: 0.004901873879134655\n",
      "iteration 7317, loss: 0.005008202977478504\n",
      "iteration 7318, loss: 0.004125373438000679\n",
      "iteration 7319, loss: 0.0052088224329054356\n",
      "iteration 7320, loss: 0.0037463963963091373\n",
      "iteration 7321, loss: 0.0050030723214149475\n",
      "iteration 7322, loss: 0.004757366143167019\n",
      "iteration 7323, loss: 0.0047493502497673035\n",
      "iteration 7324, loss: 0.004134263843297958\n",
      "iteration 7325, loss: 0.004864672664552927\n",
      "iteration 7326, loss: 0.004505055956542492\n",
      "iteration 7327, loss: 0.0051613361574709415\n",
      "iteration 7328, loss: 0.004065461456775665\n",
      "iteration 7329, loss: 0.004928699694573879\n",
      "iteration 7330, loss: 0.004386666230857372\n",
      "iteration 7331, loss: 0.004775091074407101\n",
      "iteration 7332, loss: 0.00405467115342617\n",
      "iteration 7333, loss: 0.004500402137637138\n",
      "iteration 7334, loss: 0.0049797771498560905\n",
      "iteration 7335, loss: 0.004193142056465149\n",
      "iteration 7336, loss: 0.005156155675649643\n",
      "iteration 7337, loss: 0.00391160836443305\n",
      "iteration 7338, loss: 0.004186902195215225\n",
      "iteration 7339, loss: 0.0048545245081186295\n",
      "iteration 7340, loss: 0.004588539712131023\n",
      "iteration 7341, loss: 0.004292311146855354\n",
      "iteration 7342, loss: 0.004150763154029846\n",
      "iteration 7343, loss: 0.004149365238845348\n",
      "iteration 7344, loss: 0.004787861369550228\n",
      "iteration 7345, loss: 0.004420173820108175\n",
      "iteration 7346, loss: 0.004043434746563435\n",
      "iteration 7347, loss: 0.004253340885043144\n",
      "iteration 7348, loss: 0.0036364507395774126\n",
      "iteration 7349, loss: 0.004658869467675686\n",
      "iteration 7350, loss: 0.004361825995147228\n",
      "iteration 7351, loss: 0.00436027068644762\n",
      "iteration 7352, loss: 0.004862356930971146\n",
      "iteration 7353, loss: 0.00374609581194818\n",
      "iteration 7354, loss: 0.0034629511646926403\n",
      "iteration 7355, loss: 0.004945280496031046\n",
      "iteration 7356, loss: 0.0045424941927194595\n",
      "iteration 7357, loss: 0.004662128631025553\n",
      "iteration 7358, loss: 0.0053496817126870155\n",
      "iteration 7359, loss: 0.004273653496056795\n",
      "iteration 7360, loss: 0.004079735837876797\n",
      "iteration 7361, loss: 0.004709290806204081\n",
      "iteration 7362, loss: 0.004216283559799194\n",
      "iteration 7363, loss: 0.004028345923870802\n",
      "iteration 7364, loss: 0.0051123760640621185\n",
      "iteration 7365, loss: 0.004989592358469963\n",
      "iteration 7366, loss: 0.004164981190115213\n",
      "iteration 7367, loss: 0.0050084833055734634\n",
      "iteration 7368, loss: 0.004079870879650116\n",
      "iteration 7369, loss: 0.0051499018445611\n",
      "iteration 7370, loss: 0.0037894027773290873\n",
      "iteration 7371, loss: 0.005157924257218838\n",
      "iteration 7372, loss: 0.004492675885558128\n",
      "iteration 7373, loss: 0.004819399677217007\n",
      "iteration 7374, loss: 0.004475769586861134\n",
      "iteration 7375, loss: 0.0044266884215176105\n",
      "iteration 7376, loss: 0.004130920860916376\n",
      "iteration 7377, loss: 0.004607866518199444\n",
      "iteration 7378, loss: 0.004537835251539946\n",
      "iteration 7379, loss: 0.00481064897030592\n",
      "iteration 7380, loss: 0.004519379697740078\n",
      "iteration 7381, loss: 0.004959377460181713\n",
      "iteration 7382, loss: 0.00498714204877615\n",
      "iteration 7383, loss: 0.0048705656081438065\n",
      "iteration 7384, loss: 0.005060197785496712\n",
      "iteration 7385, loss: 0.004648366943001747\n",
      "iteration 7386, loss: 0.004454148001968861\n",
      "iteration 7387, loss: 0.004150332883000374\n",
      "iteration 7388, loss: 0.004600582644343376\n",
      "iteration 7389, loss: 0.005406961310654879\n",
      "iteration 7390, loss: 0.004887400195002556\n",
      "iteration 7391, loss: 0.0038241632282733917\n",
      "iteration 7392, loss: 0.004474100656807423\n",
      "iteration 7393, loss: 0.005274524912238121\n",
      "iteration 7394, loss: 0.0053074657917022705\n",
      "iteration 7395, loss: 0.004342001862823963\n",
      "iteration 7396, loss: 0.0048486595042049885\n",
      "iteration 7397, loss: 0.005008796229958534\n",
      "iteration 7398, loss: 0.004438434261828661\n",
      "iteration 7399, loss: 0.004319184925407171\n",
      "iteration 7400, loss: 0.004413300193846226\n",
      "iteration 7401, loss: 0.004162551835179329\n",
      "iteration 7402, loss: 0.004821290262043476\n",
      "iteration 7403, loss: 0.004451149143278599\n",
      "iteration 7404, loss: 0.004004005808383226\n",
      "iteration 7405, loss: 0.0037938538007438183\n",
      "iteration 7406, loss: 0.0033824224956333637\n",
      "iteration 7407, loss: 0.004012014716863632\n",
      "iteration 7408, loss: 0.004544426687061787\n",
      "iteration 7409, loss: 0.004408465698361397\n",
      "iteration 7410, loss: 0.004736361093819141\n",
      "iteration 7411, loss: 0.004515291657298803\n",
      "iteration 7412, loss: 0.004958448000252247\n",
      "iteration 7413, loss: 0.004323228262364864\n",
      "iteration 7414, loss: 0.004859533626586199\n",
      "iteration 7415, loss: 0.003602168755605817\n",
      "iteration 7416, loss: 0.004777449183166027\n",
      "iteration 7417, loss: 0.004523248411715031\n",
      "iteration 7418, loss: 0.004387980792671442\n",
      "iteration 7419, loss: 0.004391815047711134\n",
      "iteration 7420, loss: 0.0039809588342905045\n",
      "iteration 7421, loss: 0.00435829721391201\n",
      "iteration 7422, loss: 0.0042703887447714806\n",
      "iteration 7423, loss: 0.003851243294775486\n",
      "iteration 7424, loss: 0.004137113690376282\n",
      "iteration 7425, loss: 0.005032903049141169\n",
      "iteration 7426, loss: 0.004361603409051895\n",
      "iteration 7427, loss: 0.004374012351036072\n",
      "iteration 7428, loss: 0.004678905010223389\n",
      "iteration 7429, loss: 0.004117483273148537\n",
      "iteration 7430, loss: 0.004077563062310219\n",
      "iteration 7431, loss: 0.0034001749008893967\n",
      "iteration 7432, loss: 0.0038900168146938086\n",
      "iteration 7433, loss: 0.005112778395414352\n",
      "iteration 7434, loss: 0.004412110894918442\n",
      "iteration 7435, loss: 0.004266249015927315\n",
      "iteration 7436, loss: 0.0052815512754023075\n",
      "iteration 7437, loss: 0.00457092234864831\n",
      "iteration 7438, loss: 0.004240402951836586\n",
      "iteration 7439, loss: 0.0038196679670363665\n",
      "iteration 7440, loss: 0.0041084885597229\n",
      "iteration 7441, loss: 0.004051675088703632\n",
      "iteration 7442, loss: 0.004188476130366325\n",
      "iteration 7443, loss: 0.0048868171870708466\n",
      "iteration 7444, loss: 0.004123231861740351\n",
      "iteration 7445, loss: 0.004332649055868387\n",
      "iteration 7446, loss: 0.004292274825274944\n",
      "iteration 7447, loss: 0.005405033007264137\n",
      "iteration 7448, loss: 0.004597566090524197\n",
      "iteration 7449, loss: 0.004428286105394363\n",
      "iteration 7450, loss: 0.003680400550365448\n",
      "iteration 7451, loss: 0.0036718423943966627\n",
      "iteration 7452, loss: 0.004510955885052681\n",
      "iteration 7453, loss: 0.005280601792037487\n",
      "iteration 7454, loss: 0.004483292810618877\n",
      "iteration 7455, loss: 0.004325897432863712\n",
      "iteration 7456, loss: 0.0038003441877663136\n",
      "iteration 7457, loss: 0.004122478421777487\n",
      "iteration 7458, loss: 0.005121221765875816\n",
      "iteration 7459, loss: 0.004553845152258873\n",
      "iteration 7460, loss: 0.004181961994618177\n",
      "iteration 7461, loss: 0.004179052542895079\n",
      "iteration 7462, loss: 0.0037110524717718363\n",
      "iteration 7463, loss: 0.004265096969902515\n",
      "iteration 7464, loss: 0.004269732628017664\n",
      "iteration 7465, loss: 0.0040321750566363335\n",
      "iteration 7466, loss: 0.003501273225992918\n",
      "iteration 7467, loss: 0.004362394101917744\n",
      "iteration 7468, loss: 0.005464595276862383\n",
      "iteration 7469, loss: 0.00461515411734581\n",
      "iteration 7470, loss: 0.00449004303663969\n",
      "iteration 7471, loss: 0.004300474189221859\n",
      "iteration 7472, loss: 0.004521542228758335\n",
      "iteration 7473, loss: 0.004600084386765957\n",
      "iteration 7474, loss: 0.003273064736276865\n",
      "iteration 7475, loss: 0.004551650956273079\n",
      "iteration 7476, loss: 0.004476954694837332\n",
      "iteration 7477, loss: 0.004240971989929676\n",
      "iteration 7478, loss: 0.006201458629220724\n",
      "iteration 7479, loss: 0.004714339040219784\n",
      "iteration 7480, loss: 0.005299735348671675\n",
      "iteration 7481, loss: 0.004508841782808304\n",
      "iteration 7482, loss: 0.0037430303636938334\n",
      "iteration 7483, loss: 0.004488677717745304\n",
      "iteration 7484, loss: 0.003757561556994915\n",
      "iteration 7485, loss: 0.004204612225294113\n",
      "iteration 7486, loss: 0.004218015819787979\n",
      "iteration 7487, loss: 0.004843981005251408\n",
      "iteration 7488, loss: 0.004026864655315876\n",
      "iteration 7489, loss: 0.004902766551822424\n",
      "iteration 7490, loss: 0.004571200348436832\n",
      "iteration 7491, loss: 0.004929724149405956\n",
      "iteration 7492, loss: 0.00439887959510088\n",
      "iteration 7493, loss: 0.004666883498430252\n",
      "iteration 7494, loss: 0.004193456843495369\n",
      "iteration 7495, loss: 0.003790436312556267\n",
      "iteration 7496, loss: 0.004327809438109398\n",
      "iteration 7497, loss: 0.004252886865288019\n",
      "iteration 7498, loss: 0.004710290115326643\n",
      "iteration 7499, loss: 0.004791718907654285\n",
      "iteration 7500, loss: 0.004633316304534674\n",
      "iteration 7501, loss: 0.004348369315266609\n",
      "iteration 7502, loss: 0.004465926438570023\n",
      "iteration 7503, loss: 0.004891634918749332\n",
      "iteration 7504, loss: 0.004311844706535339\n",
      "iteration 7505, loss: 0.004356236197054386\n",
      "iteration 7506, loss: 0.004629956558346748\n",
      "iteration 7507, loss: 0.004391100257635117\n",
      "iteration 7508, loss: 0.004328712821006775\n",
      "iteration 7509, loss: 0.004114278592169285\n",
      "iteration 7510, loss: 0.003918951377272606\n",
      "iteration 7511, loss: 0.004262891598045826\n",
      "iteration 7512, loss: 0.004481593146920204\n",
      "iteration 7513, loss: 0.0045778704807162285\n",
      "iteration 7514, loss: 0.005161106586456299\n",
      "iteration 7515, loss: 0.004578868858516216\n",
      "iteration 7516, loss: 0.003946837969124317\n",
      "iteration 7517, loss: 0.004771064966917038\n",
      "iteration 7518, loss: 0.004650292918086052\n",
      "iteration 7519, loss: 0.004988873843103647\n",
      "iteration 7520, loss: 0.004352646414190531\n",
      "iteration 7521, loss: 0.00430295942351222\n",
      "iteration 7522, loss: 0.0037998193874955177\n",
      "iteration 7523, loss: 0.003957411739975214\n",
      "iteration 7524, loss: 0.003322990844026208\n",
      "iteration 7525, loss: 0.0036913426592946053\n",
      "iteration 7526, loss: 0.004196753725409508\n",
      "iteration 7527, loss: 0.00445748632773757\n",
      "iteration 7528, loss: 0.003609031904488802\n",
      "iteration 7529, loss: 0.004111122339963913\n",
      "iteration 7530, loss: 0.00402932520955801\n",
      "iteration 7531, loss: 0.003637544810771942\n",
      "iteration 7532, loss: 0.004064654931426048\n",
      "iteration 7533, loss: 0.004118427634239197\n",
      "iteration 7534, loss: 0.003915740642696619\n",
      "iteration 7535, loss: 0.004382557235658169\n",
      "iteration 7536, loss: 0.004323246888816357\n",
      "iteration 7537, loss: 0.004434999078512192\n",
      "iteration 7538, loss: 0.004508153535425663\n",
      "iteration 7539, loss: 0.004340541083365679\n",
      "iteration 7540, loss: 0.005047149956226349\n",
      "iteration 7541, loss: 0.00395972328260541\n",
      "iteration 7542, loss: 0.004139058291912079\n",
      "iteration 7543, loss: 0.0044686696492135525\n",
      "iteration 7544, loss: 0.0041738832369446754\n",
      "iteration 7545, loss: 0.004366707988083363\n",
      "iteration 7546, loss: 0.004972527734935284\n",
      "iteration 7547, loss: 0.005345829762518406\n",
      "iteration 7548, loss: 0.004237458109855652\n",
      "iteration 7549, loss: 0.004554539453238249\n",
      "iteration 7550, loss: 0.003941596485674381\n",
      "iteration 7551, loss: 0.003647445933893323\n",
      "iteration 7552, loss: 0.004103453364223242\n",
      "iteration 7553, loss: 0.0038365619257092476\n",
      "iteration 7554, loss: 0.004016620572656393\n",
      "iteration 7555, loss: 0.003884204663336277\n",
      "iteration 7556, loss: 0.00411121966317296\n",
      "iteration 7557, loss: 0.0049148742109537125\n",
      "iteration 7558, loss: 0.00465861801058054\n",
      "iteration 7559, loss: 0.004392945673316717\n",
      "iteration 7560, loss: 0.0033544027246534824\n",
      "iteration 7561, loss: 0.0045954580418765545\n",
      "iteration 7562, loss: 0.004340358544141054\n",
      "iteration 7563, loss: 0.00490968395024538\n",
      "iteration 7564, loss: 0.004139631986618042\n",
      "iteration 7565, loss: 0.004984517581760883\n",
      "iteration 7566, loss: 0.004682701546698809\n",
      "iteration 7567, loss: 0.0039030122570693493\n",
      "iteration 7568, loss: 0.003985371440649033\n",
      "iteration 7569, loss: 0.0036366344429552555\n",
      "iteration 7570, loss: 0.004413668066263199\n",
      "iteration 7571, loss: 0.004471032880246639\n",
      "iteration 7572, loss: 0.0040644025430083275\n",
      "iteration 7573, loss: 0.004081494174897671\n",
      "iteration 7574, loss: 0.003952852915972471\n",
      "iteration 7575, loss: 0.0038378883618861437\n",
      "iteration 7576, loss: 0.003938143141567707\n",
      "iteration 7577, loss: 0.0046936883591115475\n",
      "iteration 7578, loss: 0.003969566896557808\n",
      "iteration 7579, loss: 0.003992016427218914\n",
      "iteration 7580, loss: 0.00495307520031929\n",
      "iteration 7581, loss: 0.00502217561006546\n",
      "iteration 7582, loss: 0.004084320273250341\n",
      "iteration 7583, loss: 0.004094325937330723\n",
      "iteration 7584, loss: 0.0034392517991364002\n",
      "iteration 7585, loss: 0.003685356816276908\n",
      "iteration 7586, loss: 0.00413884874433279\n",
      "iteration 7587, loss: 0.004153687506914139\n",
      "iteration 7588, loss: 0.004102720879018307\n",
      "iteration 7589, loss: 0.003725840710103512\n",
      "iteration 7590, loss: 0.004221782088279724\n",
      "iteration 7591, loss: 0.0051427241414785385\n",
      "iteration 7592, loss: 0.004115018527954817\n",
      "iteration 7593, loss: 0.0040432000532746315\n",
      "iteration 7594, loss: 0.004385497886687517\n",
      "iteration 7595, loss: 0.0044620768167078495\n",
      "iteration 7596, loss: 0.00421405304223299\n",
      "iteration 7597, loss: 0.003929504193365574\n",
      "iteration 7598, loss: 0.0034025590866804123\n",
      "iteration 7599, loss: 0.0032587675377726555\n",
      "iteration 7600, loss: 0.004469872452318668\n",
      "iteration 7601, loss: 0.00316390348598361\n",
      "iteration 7602, loss: 0.003594430396333337\n",
      "iteration 7603, loss: 0.004025415517389774\n",
      "iteration 7604, loss: 0.004557066131383181\n",
      "iteration 7605, loss: 0.0038461668882519007\n",
      "iteration 7606, loss: 0.0040792981162667274\n",
      "iteration 7607, loss: 0.004083147272467613\n",
      "iteration 7608, loss: 0.004523264244198799\n",
      "iteration 7609, loss: 0.0035506028216332197\n",
      "iteration 7610, loss: 0.004167282022535801\n",
      "iteration 7611, loss: 0.003932593390345573\n",
      "iteration 7612, loss: 0.004463012330234051\n",
      "iteration 7613, loss: 0.004277666099369526\n",
      "iteration 7614, loss: 0.003702716436237097\n",
      "iteration 7615, loss: 0.005189721938222647\n",
      "iteration 7616, loss: 0.004628852941095829\n",
      "iteration 7617, loss: 0.004809434525668621\n",
      "iteration 7618, loss: 0.0047337980940938\n",
      "iteration 7619, loss: 0.003860076889395714\n",
      "iteration 7620, loss: 0.0044524106197059155\n",
      "iteration 7621, loss: 0.005601747427135706\n",
      "iteration 7622, loss: 0.0037363790906965733\n",
      "iteration 7623, loss: 0.0035741045139729977\n",
      "iteration 7624, loss: 0.004385749809443951\n",
      "iteration 7625, loss: 0.0033786839339882135\n",
      "iteration 7626, loss: 0.003829718567430973\n",
      "iteration 7627, loss: 0.003680930007249117\n",
      "iteration 7628, loss: 0.0045636678114533424\n",
      "iteration 7629, loss: 0.0038276638370007277\n",
      "iteration 7630, loss: 0.003912805579602718\n",
      "iteration 7631, loss: 0.005269554443657398\n",
      "iteration 7632, loss: 0.003713754704222083\n",
      "iteration 7633, loss: 0.004562386777251959\n",
      "iteration 7634, loss: 0.004065824672579765\n",
      "iteration 7635, loss: 0.004249274265021086\n",
      "iteration 7636, loss: 0.004260879009962082\n",
      "iteration 7637, loss: 0.004112762399017811\n",
      "iteration 7638, loss: 0.003874281421303749\n",
      "iteration 7639, loss: 0.004404435865581036\n",
      "iteration 7640, loss: 0.00433080829679966\n",
      "iteration 7641, loss: 0.00351664493791759\n",
      "iteration 7642, loss: 0.004053894430398941\n",
      "iteration 7643, loss: 0.004304615780711174\n",
      "iteration 7644, loss: 0.004405468236654997\n",
      "iteration 7645, loss: 0.004625848028808832\n",
      "iteration 7646, loss: 0.004031481221318245\n",
      "iteration 7647, loss: 0.0038261679001152515\n",
      "iteration 7648, loss: 0.005080854520201683\n",
      "iteration 7649, loss: 0.004712846130132675\n",
      "iteration 7650, loss: 0.003688597586005926\n",
      "iteration 7651, loss: 0.0037741409614682198\n",
      "iteration 7652, loss: 0.00456855446100235\n",
      "iteration 7653, loss: 0.003961947280913591\n",
      "iteration 7654, loss: 0.004414730239659548\n",
      "iteration 7655, loss: 0.004522494040429592\n",
      "iteration 7656, loss: 0.004365433473140001\n",
      "iteration 7657, loss: 0.0046307360753417015\n",
      "iteration 7658, loss: 0.004478697665035725\n",
      "iteration 7659, loss: 0.0037373953964561224\n",
      "iteration 7660, loss: 0.003542858175933361\n",
      "iteration 7661, loss: 0.004068634007126093\n",
      "iteration 7662, loss: 0.004298428073525429\n",
      "iteration 7663, loss: 0.003840147517621517\n",
      "iteration 7664, loss: 0.005120255518704653\n",
      "iteration 7665, loss: 0.004218000452965498\n",
      "iteration 7666, loss: 0.003989997319877148\n",
      "iteration 7667, loss: 0.004205239005386829\n",
      "iteration 7668, loss: 0.0040433225221931934\n",
      "iteration 7669, loss: 0.0040062833577394485\n",
      "iteration 7670, loss: 0.004436306655406952\n",
      "iteration 7671, loss: 0.00423877639696002\n",
      "iteration 7672, loss: 0.004031401593238115\n",
      "iteration 7673, loss: 0.004214218817651272\n",
      "iteration 7674, loss: 0.003951499704271555\n",
      "iteration 7675, loss: 0.0042872242629528046\n",
      "iteration 7676, loss: 0.003915499430149794\n",
      "iteration 7677, loss: 0.003942498937249184\n",
      "iteration 7678, loss: 0.004187874961644411\n",
      "iteration 7679, loss: 0.003867432940751314\n",
      "iteration 7680, loss: 0.0037982063367962837\n",
      "iteration 7681, loss: 0.0038159708492457867\n",
      "iteration 7682, loss: 0.004416577983647585\n",
      "iteration 7683, loss: 0.004172455053776503\n",
      "iteration 7684, loss: 0.004076449200510979\n",
      "iteration 7685, loss: 0.004561050329357386\n",
      "iteration 7686, loss: 0.004767884500324726\n",
      "iteration 7687, loss: 0.004310826770961285\n",
      "iteration 7688, loss: 0.0042797839269042015\n",
      "iteration 7689, loss: 0.003907081671059132\n",
      "iteration 7690, loss: 0.003964551724493504\n",
      "iteration 7691, loss: 0.0037100985646247864\n",
      "iteration 7692, loss: 0.004537568427622318\n",
      "iteration 7693, loss: 0.0040101660415530205\n",
      "iteration 7694, loss: 0.0035595567896962166\n",
      "iteration 7695, loss: 0.0040409015491604805\n",
      "iteration 7696, loss: 0.0034975833259522915\n",
      "iteration 7697, loss: 0.004824075382202864\n",
      "iteration 7698, loss: 0.004445948638021946\n",
      "iteration 7699, loss: 0.004428677726536989\n",
      "iteration 7700, loss: 0.004125668667256832\n",
      "iteration 7701, loss: 0.004502664320170879\n",
      "iteration 7702, loss: 0.004474481102079153\n",
      "iteration 7703, loss: 0.003840568009763956\n",
      "iteration 7704, loss: 0.004134517163038254\n",
      "iteration 7705, loss: 0.004011263605207205\n",
      "iteration 7706, loss: 0.003879367373883724\n",
      "iteration 7707, loss: 0.003773279720917344\n",
      "iteration 7708, loss: 0.0047841062769293785\n",
      "iteration 7709, loss: 0.0035751350224018097\n",
      "iteration 7710, loss: 0.004701922181993723\n",
      "iteration 7711, loss: 0.00410377886146307\n",
      "iteration 7712, loss: 0.004018089734017849\n",
      "iteration 7713, loss: 0.00439263042062521\n",
      "iteration 7714, loss: 0.004343995824456215\n",
      "iteration 7715, loss: 0.0038141622208058834\n",
      "iteration 7716, loss: 0.004528548568487167\n",
      "iteration 7717, loss: 0.004137905780225992\n",
      "iteration 7718, loss: 0.003594248788431287\n",
      "iteration 7719, loss: 0.0038762949407100677\n",
      "iteration 7720, loss: 0.003928875084966421\n",
      "iteration 7721, loss: 0.0036816056817770004\n",
      "iteration 7722, loss: 0.003231385722756386\n",
      "iteration 7723, loss: 0.0039059892296791077\n",
      "iteration 7724, loss: 0.004051658324897289\n",
      "iteration 7725, loss: 0.0044210562482476234\n",
      "iteration 7726, loss: 0.004245367832481861\n",
      "iteration 7727, loss: 0.003925769589841366\n",
      "iteration 7728, loss: 0.004316671751439571\n",
      "iteration 7729, loss: 0.00404325919225812\n",
      "iteration 7730, loss: 0.004249987192451954\n",
      "iteration 7731, loss: 0.004145410843193531\n",
      "iteration 7732, loss: 0.004792781546711922\n",
      "iteration 7733, loss: 0.003589940257370472\n",
      "iteration 7734, loss: 0.003674630541354418\n",
      "iteration 7735, loss: 0.0034382320009171963\n",
      "iteration 7736, loss: 0.00420693214982748\n",
      "iteration 7737, loss: 0.003869612468406558\n",
      "iteration 7738, loss: 0.0041685039177536964\n",
      "iteration 7739, loss: 0.003414960578083992\n",
      "iteration 7740, loss: 0.003740331158041954\n",
      "iteration 7741, loss: 0.0037630763836205006\n",
      "iteration 7742, loss: 0.0036900825798511505\n",
      "iteration 7743, loss: 0.0039189038798213005\n",
      "iteration 7744, loss: 0.004862549714744091\n",
      "iteration 7745, loss: 0.0038069989532232285\n",
      "iteration 7746, loss: 0.0040712254121899605\n",
      "iteration 7747, loss: 0.004270624369382858\n",
      "iteration 7748, loss: 0.0035641128197312355\n",
      "iteration 7749, loss: 0.0038498472422361374\n",
      "iteration 7750, loss: 0.0044569456949830055\n",
      "iteration 7751, loss: 0.004927660338580608\n",
      "iteration 7752, loss: 0.004166590049862862\n",
      "iteration 7753, loss: 0.004256056621670723\n",
      "iteration 7754, loss: 0.004708963446319103\n",
      "iteration 7755, loss: 0.004385537467896938\n",
      "iteration 7756, loss: 0.004661327227950096\n",
      "iteration 7757, loss: 0.004223906435072422\n",
      "iteration 7758, loss: 0.004101553000509739\n",
      "iteration 7759, loss: 0.006033868063241243\n",
      "iteration 7760, loss: 0.004057751502841711\n",
      "iteration 7761, loss: 0.00427175173535943\n",
      "iteration 7762, loss: 0.004016016609966755\n",
      "iteration 7763, loss: 0.004384496249258518\n",
      "iteration 7764, loss: 0.004415260627865791\n",
      "iteration 7765, loss: 0.004051427356898785\n",
      "iteration 7766, loss: 0.00466882111504674\n",
      "iteration 7767, loss: 0.004170520696789026\n",
      "iteration 7768, loss: 0.00458983751013875\n",
      "iteration 7769, loss: 0.005957606714218855\n",
      "iteration 7770, loss: 0.004033144563436508\n",
      "iteration 7771, loss: 0.004618000239133835\n",
      "iteration 7772, loss: 0.003430246841162443\n",
      "iteration 7773, loss: 0.004349338822066784\n",
      "iteration 7774, loss: 0.005110419355332851\n",
      "iteration 7775, loss: 0.004201623611152172\n",
      "iteration 7776, loss: 0.005028838757425547\n",
      "iteration 7777, loss: 0.004474323242902756\n",
      "iteration 7778, loss: 0.004541554022580385\n",
      "iteration 7779, loss: 0.004157155752182007\n",
      "iteration 7780, loss: 0.0038384359795600176\n",
      "iteration 7781, loss: 0.00428328663110733\n",
      "iteration 7782, loss: 0.00451882928609848\n",
      "iteration 7783, loss: 0.0038479208014905453\n",
      "iteration 7784, loss: 0.00394336087629199\n",
      "iteration 7785, loss: 0.0044039161875844\n",
      "iteration 7786, loss: 0.003622217569500208\n",
      "iteration 7787, loss: 0.004523804411292076\n",
      "iteration 7788, loss: 0.005146372132003307\n",
      "iteration 7789, loss: 0.004296157509088516\n",
      "iteration 7790, loss: 0.004116266034543514\n",
      "iteration 7791, loss: 0.004353624768555164\n",
      "iteration 7792, loss: 0.003998591098934412\n",
      "iteration 7793, loss: 0.004355231299996376\n",
      "iteration 7794, loss: 0.00425342470407486\n",
      "iteration 7795, loss: 0.003839478362351656\n",
      "iteration 7796, loss: 0.003968994598835707\n",
      "iteration 7797, loss: 0.004003054462373257\n",
      "iteration 7798, loss: 0.004591739736497402\n",
      "iteration 7799, loss: 0.0038881590589880943\n",
      "iteration 7800, loss: 0.0037646100390702486\n",
      "iteration 7801, loss: 0.0044327289797365665\n",
      "iteration 7802, loss: 0.004094650968909264\n",
      "iteration 7803, loss: 0.004290634300559759\n",
      "iteration 7804, loss: 0.004306260962039232\n",
      "iteration 7805, loss: 0.004094618838280439\n",
      "iteration 7806, loss: 0.004147288389503956\n",
      "iteration 7807, loss: 0.004380646161735058\n",
      "iteration 7808, loss: 0.004016170743852854\n",
      "iteration 7809, loss: 0.004119168035686016\n",
      "iteration 7810, loss: 0.003571524051949382\n",
      "iteration 7811, loss: 0.004629284143447876\n",
      "iteration 7812, loss: 0.0038988073356449604\n",
      "iteration 7813, loss: 0.0041475845500826836\n",
      "iteration 7814, loss: 0.004602034576237202\n",
      "iteration 7815, loss: 0.003526347689330578\n",
      "iteration 7816, loss: 0.0040562511421740055\n",
      "iteration 7817, loss: 0.00508649880066514\n",
      "iteration 7818, loss: 0.004510160535573959\n",
      "iteration 7819, loss: 0.0040334658697247505\n",
      "iteration 7820, loss: 0.003898920025676489\n",
      "iteration 7821, loss: 0.004383101128041744\n",
      "iteration 7822, loss: 0.003755225334316492\n",
      "iteration 7823, loss: 0.004083908628672361\n",
      "iteration 7824, loss: 0.003836622228845954\n",
      "iteration 7825, loss: 0.0038638547994196415\n",
      "iteration 7826, loss: 0.0035904881078749895\n",
      "iteration 7827, loss: 0.0041047330014407635\n",
      "iteration 7828, loss: 0.004062758758664131\n",
      "iteration 7829, loss: 0.0038649479392915964\n",
      "iteration 7830, loss: 0.0033136592246592045\n",
      "iteration 7831, loss: 0.004419327713549137\n",
      "iteration 7832, loss: 0.0035046073608100414\n",
      "iteration 7833, loss: 0.003835237817838788\n",
      "iteration 7834, loss: 0.004347288981080055\n",
      "iteration 7835, loss: 0.004110810346901417\n",
      "iteration 7836, loss: 0.004475081339478493\n",
      "iteration 7837, loss: 0.004504363983869553\n",
      "iteration 7838, loss: 0.003653413150459528\n",
      "iteration 7839, loss: 0.003742497880011797\n",
      "iteration 7840, loss: 0.004267840646207333\n",
      "iteration 7841, loss: 0.00554992351680994\n",
      "iteration 7842, loss: 0.003855779767036438\n",
      "iteration 7843, loss: 0.005177722312510014\n",
      "iteration 7844, loss: 0.004689102992415428\n",
      "iteration 7845, loss: 0.003575471695512533\n",
      "iteration 7846, loss: 0.004179541952908039\n",
      "iteration 7847, loss: 0.003543773666024208\n",
      "iteration 7848, loss: 0.0038140793330967426\n",
      "iteration 7849, loss: 0.0038367430679500103\n",
      "iteration 7850, loss: 0.005163886584341526\n",
      "iteration 7851, loss: 0.004701193887740374\n",
      "iteration 7852, loss: 0.004250579513609409\n",
      "iteration 7853, loss: 0.004107254557311535\n",
      "iteration 7854, loss: 0.00449077133089304\n",
      "iteration 7855, loss: 0.0040235091000795364\n",
      "iteration 7856, loss: 0.004163460340350866\n",
      "iteration 7857, loss: 0.0032610390335321426\n",
      "iteration 7858, loss: 0.004783222451806068\n",
      "iteration 7859, loss: 0.004473506473004818\n",
      "iteration 7860, loss: 0.004046131856739521\n",
      "iteration 7861, loss: 0.003886864520609379\n",
      "iteration 7862, loss: 0.0041254726238548756\n",
      "iteration 7863, loss: 0.0038277790881693363\n",
      "iteration 7864, loss: 0.004351516254246235\n",
      "iteration 7865, loss: 0.004966758191585541\n",
      "iteration 7866, loss: 0.00425354577600956\n",
      "iteration 7867, loss: 0.004144941922277212\n",
      "iteration 7868, loss: 0.003024834208190441\n",
      "iteration 7869, loss: 0.0038719167932868004\n",
      "iteration 7870, loss: 0.0037347907200455666\n",
      "iteration 7871, loss: 0.004027417860925198\n",
      "iteration 7872, loss: 0.004248593933880329\n",
      "iteration 7873, loss: 0.00421547144651413\n",
      "iteration 7874, loss: 0.0043097687885165215\n",
      "iteration 7875, loss: 0.0032140847761183977\n",
      "iteration 7876, loss: 0.004098561592400074\n",
      "iteration 7877, loss: 0.003732270561158657\n",
      "iteration 7878, loss: 0.003520119469612837\n",
      "iteration 7879, loss: 0.0034153717570006847\n",
      "iteration 7880, loss: 0.004030301235616207\n",
      "iteration 7881, loss: 0.004086322616785765\n",
      "iteration 7882, loss: 0.003911683801561594\n",
      "iteration 7883, loss: 0.0037630919832736254\n",
      "iteration 7884, loss: 0.0044173430651426315\n",
      "iteration 7885, loss: 0.0042433287017047405\n",
      "iteration 7886, loss: 0.003861350240185857\n",
      "iteration 7887, loss: 0.003022843971848488\n",
      "iteration 7888, loss: 0.0038330683019012213\n",
      "iteration 7889, loss: 0.0037686177529394627\n",
      "iteration 7890, loss: 0.0034642850514501333\n",
      "iteration 7891, loss: 0.003929224330931902\n",
      "iteration 7892, loss: 0.0035322867333889008\n",
      "iteration 7893, loss: 0.00357840652577579\n",
      "iteration 7894, loss: 0.004138883203268051\n",
      "iteration 7895, loss: 0.0045144702307879925\n",
      "iteration 7896, loss: 0.004309108946472406\n",
      "iteration 7897, loss: 0.004436828196048737\n",
      "iteration 7898, loss: 0.005177999846637249\n",
      "iteration 7899, loss: 0.003896196372807026\n",
      "iteration 7900, loss: 0.003707495518028736\n",
      "iteration 7901, loss: 0.003689649049192667\n",
      "iteration 7902, loss: 0.0034889259841293097\n",
      "iteration 7903, loss: 0.0038924964610487223\n",
      "iteration 7904, loss: 0.003949323669075966\n",
      "iteration 7905, loss: 0.004413881339132786\n",
      "iteration 7906, loss: 0.004087802954018116\n",
      "iteration 7907, loss: 0.004258477129042149\n",
      "iteration 7908, loss: 0.0037081940099596977\n",
      "iteration 7909, loss: 0.003728201612830162\n",
      "iteration 7910, loss: 0.003740877378731966\n",
      "iteration 7911, loss: 0.003837400581687689\n",
      "iteration 7912, loss: 0.004110773093998432\n",
      "iteration 7913, loss: 0.003952317405492067\n",
      "iteration 7914, loss: 0.003508326830342412\n",
      "iteration 7915, loss: 0.0035445173271000385\n",
      "iteration 7916, loss: 0.003365927143022418\n",
      "iteration 7917, loss: 0.004159029573202133\n",
      "iteration 7918, loss: 0.0038408550899475813\n",
      "iteration 7919, loss: 0.00403056014329195\n",
      "iteration 7920, loss: 0.0034943390637636185\n",
      "iteration 7921, loss: 0.003795833559706807\n",
      "iteration 7922, loss: 0.0041513219475746155\n",
      "iteration 7923, loss: 0.004136264324188232\n",
      "iteration 7924, loss: 0.0037635094486176968\n",
      "iteration 7925, loss: 0.0045877061784267426\n",
      "iteration 7926, loss: 0.004431636072695255\n",
      "iteration 7927, loss: 0.003833331633359194\n",
      "iteration 7928, loss: 0.004262798000127077\n",
      "iteration 7929, loss: 0.004430174827575684\n",
      "iteration 7930, loss: 0.0036999762523919344\n",
      "iteration 7931, loss: 0.00382344052195549\n",
      "iteration 7932, loss: 0.003924384713172913\n",
      "iteration 7933, loss: 0.003636846784502268\n",
      "iteration 7934, loss: 0.0035847006365656853\n",
      "iteration 7935, loss: 0.004328704439103603\n",
      "iteration 7936, loss: 0.0036329124122858047\n",
      "iteration 7937, loss: 0.004535142797976732\n",
      "iteration 7938, loss: 0.004275832325220108\n",
      "iteration 7939, loss: 0.003954267129302025\n",
      "iteration 7940, loss: 0.004476215224713087\n",
      "iteration 7941, loss: 0.0036826059222221375\n",
      "iteration 7942, loss: 0.004156459588557482\n",
      "iteration 7943, loss: 0.004047202877700329\n",
      "iteration 7944, loss: 0.003938193432986736\n",
      "iteration 7945, loss: 0.004474123008549213\n",
      "iteration 7946, loss: 0.0038676150143146515\n",
      "iteration 7947, loss: 0.0032660742290318012\n",
      "iteration 7948, loss: 0.0041993409395217896\n",
      "iteration 7949, loss: 0.004040441941469908\n",
      "iteration 7950, loss: 0.004002337343990803\n",
      "iteration 7951, loss: 0.003341501113027334\n",
      "iteration 7952, loss: 0.00466803228482604\n",
      "iteration 7953, loss: 0.0037249987944960594\n",
      "iteration 7954, loss: 0.0038358215242624283\n",
      "iteration 7955, loss: 0.004614443983882666\n",
      "iteration 7956, loss: 0.003780543804168701\n",
      "iteration 7957, loss: 0.004294529557228088\n",
      "iteration 7958, loss: 0.0036910753697156906\n",
      "iteration 7959, loss: 0.003505727043375373\n",
      "iteration 7960, loss: 0.004585414659231901\n",
      "iteration 7961, loss: 0.004170415922999382\n",
      "iteration 7962, loss: 0.003846929408609867\n",
      "iteration 7963, loss: 0.005278825294226408\n",
      "iteration 7964, loss: 0.0034749486949294806\n",
      "iteration 7965, loss: 0.0038306210190057755\n",
      "iteration 7966, loss: 0.0042032599449157715\n",
      "iteration 7967, loss: 0.0035274431575089693\n",
      "iteration 7968, loss: 0.0041748275980353355\n",
      "iteration 7969, loss: 0.004378989338874817\n",
      "iteration 7970, loss: 0.0038188325706869364\n",
      "iteration 7971, loss: 0.0035185199230909348\n",
      "iteration 7972, loss: 0.003999239765107632\n",
      "iteration 7973, loss: 0.004087638109922409\n",
      "iteration 7974, loss: 0.00406071450561285\n",
      "iteration 7975, loss: 0.003146625589579344\n",
      "iteration 7976, loss: 0.0036233635619282722\n",
      "iteration 7977, loss: 0.0040559894405305386\n",
      "iteration 7978, loss: 0.0034925949294120073\n",
      "iteration 7979, loss: 0.0041527641005814075\n",
      "iteration 7980, loss: 0.004225530661642551\n",
      "iteration 7981, loss: 0.002546088071539998\n",
      "iteration 7982, loss: 0.0038658350240439177\n",
      "iteration 7983, loss: 0.004051336087286472\n",
      "iteration 7984, loss: 0.004200854804366827\n",
      "iteration 7985, loss: 0.0036058537662029266\n",
      "iteration 7986, loss: 0.003664128016680479\n",
      "iteration 7987, loss: 0.0037693437188863754\n",
      "iteration 7988, loss: 0.004119011107832193\n",
      "iteration 7989, loss: 0.0034551103599369526\n",
      "iteration 7990, loss: 0.004003050737082958\n",
      "iteration 7991, loss: 0.004197506234049797\n",
      "iteration 7992, loss: 0.0035345007199794054\n",
      "iteration 7993, loss: 0.0034161233343183994\n",
      "iteration 7994, loss: 0.003375046420842409\n",
      "iteration 7995, loss: 0.003463130909949541\n",
      "iteration 7996, loss: 0.00403070542961359\n",
      "iteration 7997, loss: 0.0037216381169855595\n",
      "iteration 7998, loss: 0.003601538948714733\n",
      "iteration 7999, loss: 0.0039686644449830055\n",
      "iteration 8000, loss: 0.0034600566141307354\n",
      "iteration 8001, loss: 0.004008207004517317\n",
      "iteration 8002, loss: 0.003582378150895238\n",
      "iteration 8003, loss: 0.0036465227603912354\n",
      "iteration 8004, loss: 0.0037529943510890007\n",
      "iteration 8005, loss: 0.0032964698038995266\n",
      "iteration 8006, loss: 0.003750233445316553\n",
      "iteration 8007, loss: 0.004019358195364475\n",
      "iteration 8008, loss: 0.004141567274928093\n",
      "iteration 8009, loss: 0.0036592683754861355\n",
      "iteration 8010, loss: 0.0036163469776511192\n",
      "iteration 8011, loss: 0.0039019205141812563\n",
      "iteration 8012, loss: 0.0039443206042051315\n",
      "iteration 8013, loss: 0.003122500376775861\n",
      "iteration 8014, loss: 0.003629101440310478\n",
      "iteration 8015, loss: 0.004096913151443005\n",
      "iteration 8016, loss: 0.004155260510742664\n",
      "iteration 8017, loss: 0.0040456862188875675\n",
      "iteration 8018, loss: 0.003222390543669462\n",
      "iteration 8019, loss: 0.004268841817975044\n",
      "iteration 8020, loss: 0.0036676572635769844\n",
      "iteration 8021, loss: 0.003982927650213242\n",
      "iteration 8022, loss: 0.004067656118422747\n",
      "iteration 8023, loss: 0.0035697927232831717\n",
      "iteration 8024, loss: 0.0045781489461660385\n",
      "iteration 8025, loss: 0.0038995901122689247\n",
      "iteration 8026, loss: 0.004059683531522751\n",
      "iteration 8027, loss: 0.0048413025215268135\n",
      "iteration 8028, loss: 0.0036414964124560356\n",
      "iteration 8029, loss: 0.00378143391571939\n",
      "iteration 8030, loss: 0.004731570370495319\n",
      "iteration 8031, loss: 0.0035343393683433533\n",
      "iteration 8032, loss: 0.0037125826347619295\n",
      "iteration 8033, loss: 0.005048966966569424\n",
      "iteration 8034, loss: 0.004299212712794542\n",
      "iteration 8035, loss: 0.004518754314631224\n",
      "iteration 8036, loss: 0.0032979710958898067\n",
      "iteration 8037, loss: 0.004123769234865904\n",
      "iteration 8038, loss: 0.004344075918197632\n",
      "iteration 8039, loss: 0.0043811919167637825\n",
      "iteration 8040, loss: 0.004144532606005669\n",
      "iteration 8041, loss: 0.0036502876318991184\n",
      "iteration 8042, loss: 0.004670001100748777\n",
      "iteration 8043, loss: 0.004691226873546839\n",
      "iteration 8044, loss: 0.003798697842285037\n",
      "iteration 8045, loss: 0.0037891846150159836\n",
      "iteration 8046, loss: 0.00426175631582737\n",
      "iteration 8047, loss: 0.004167931154370308\n",
      "iteration 8048, loss: 0.003914602100849152\n",
      "iteration 8049, loss: 0.00432173628360033\n",
      "iteration 8050, loss: 0.0046324580907821655\n",
      "iteration 8051, loss: 0.004553995095193386\n",
      "iteration 8052, loss: 0.004302652552723885\n",
      "iteration 8053, loss: 0.004153498448431492\n",
      "iteration 8054, loss: 0.003974699415266514\n",
      "iteration 8055, loss: 0.0041962722316384315\n",
      "iteration 8056, loss: 0.0037906598299741745\n",
      "iteration 8057, loss: 0.004635009914636612\n",
      "iteration 8058, loss: 0.003364007920026779\n",
      "iteration 8059, loss: 0.003964569419622421\n",
      "iteration 8060, loss: 0.0037739365361630917\n",
      "iteration 8061, loss: 0.004014347679913044\n",
      "iteration 8062, loss: 0.004179663956165314\n",
      "iteration 8063, loss: 0.003668484976515174\n",
      "iteration 8064, loss: 0.004039810970425606\n",
      "iteration 8065, loss: 0.0039659226313233376\n",
      "iteration 8066, loss: 0.0040451777167618275\n",
      "iteration 8067, loss: 0.004683998879045248\n",
      "iteration 8068, loss: 0.0037105665542185307\n",
      "iteration 8069, loss: 0.004969777539372444\n",
      "iteration 8070, loss: 0.0045022014528512955\n",
      "iteration 8071, loss: 0.004148432984948158\n",
      "iteration 8072, loss: 0.0032893086317926645\n",
      "iteration 8073, loss: 0.004412143025547266\n",
      "iteration 8074, loss: 0.0043332562781870365\n",
      "iteration 8075, loss: 0.004315500147640705\n",
      "iteration 8076, loss: 0.004298335872590542\n",
      "iteration 8077, loss: 0.004820073023438454\n",
      "iteration 8078, loss: 0.0037285303696990013\n",
      "iteration 8079, loss: 0.003772204276174307\n",
      "iteration 8080, loss: 0.0031770491041243076\n",
      "iteration 8081, loss: 0.004268765449523926\n",
      "iteration 8082, loss: 0.0042696669697761536\n",
      "iteration 8083, loss: 0.0038346003275364637\n",
      "iteration 8084, loss: 0.0032533295452594757\n",
      "iteration 8085, loss: 0.004651496186852455\n",
      "iteration 8086, loss: 0.003822611179202795\n",
      "iteration 8087, loss: 0.004395089577883482\n",
      "iteration 8088, loss: 0.003718462772667408\n",
      "iteration 8089, loss: 0.004223549738526344\n",
      "iteration 8090, loss: 0.0035723261535167694\n",
      "iteration 8091, loss: 0.003258528420701623\n",
      "iteration 8092, loss: 0.003878124989569187\n",
      "iteration 8093, loss: 0.0038180924020707607\n",
      "iteration 8094, loss: 0.004134964197874069\n",
      "iteration 8095, loss: 0.003682078095152974\n",
      "iteration 8096, loss: 0.004383061081171036\n",
      "iteration 8097, loss: 0.0037684310227632523\n",
      "iteration 8098, loss: 0.004447796382009983\n",
      "iteration 8099, loss: 0.0038885767571628094\n",
      "iteration 8100, loss: 0.003928472753614187\n",
      "iteration 8101, loss: 0.004275259096175432\n",
      "iteration 8102, loss: 0.0041938466019928455\n",
      "iteration 8103, loss: 0.0043103378266096115\n",
      "iteration 8104, loss: 0.004402238875627518\n",
      "iteration 8105, loss: 0.0035767843946814537\n",
      "iteration 8106, loss: 0.0038734024856239557\n",
      "iteration 8107, loss: 0.004100221209228039\n",
      "iteration 8108, loss: 0.003972217906266451\n",
      "iteration 8109, loss: 0.0038568484596908092\n",
      "iteration 8110, loss: 0.00393571425229311\n",
      "iteration 8111, loss: 0.004113044124096632\n",
      "iteration 8112, loss: 0.004839947912842035\n",
      "iteration 8113, loss: 0.004310740157961845\n",
      "iteration 8114, loss: 0.0037719893734902143\n",
      "iteration 8115, loss: 0.0037522127386182547\n",
      "iteration 8116, loss: 0.003385920077562332\n",
      "iteration 8117, loss: 0.0038309546653181314\n",
      "iteration 8118, loss: 0.0033472185023128986\n",
      "iteration 8119, loss: 0.003266866784542799\n",
      "iteration 8120, loss: 0.0033630244433879852\n",
      "iteration 8121, loss: 0.003992822486907244\n",
      "iteration 8122, loss: 0.003704636823385954\n",
      "iteration 8123, loss: 0.003413696074858308\n",
      "iteration 8124, loss: 0.004216874949634075\n",
      "iteration 8125, loss: 0.003623254131525755\n",
      "iteration 8126, loss: 0.003260863944888115\n",
      "iteration 8127, loss: 0.004442320205271244\n",
      "iteration 8128, loss: 0.003317879978567362\n",
      "iteration 8129, loss: 0.0036304625682532787\n",
      "iteration 8130, loss: 0.004153145477175713\n",
      "iteration 8131, loss: 0.0031551988795399666\n",
      "iteration 8132, loss: 0.0039678579196333885\n",
      "iteration 8133, loss: 0.003821302205324173\n",
      "iteration 8134, loss: 0.0038172630593180656\n",
      "iteration 8135, loss: 0.00388897443190217\n",
      "iteration 8136, loss: 0.004102079197764397\n",
      "iteration 8137, loss: 0.004263852722942829\n",
      "iteration 8138, loss: 0.004933524876832962\n",
      "iteration 8139, loss: 0.004450415261089802\n",
      "iteration 8140, loss: 0.003977891523391008\n",
      "iteration 8141, loss: 0.003854552749544382\n",
      "iteration 8142, loss: 0.004378902725875378\n",
      "iteration 8143, loss: 0.00378967123106122\n",
      "iteration 8144, loss: 0.0032284564804285765\n",
      "iteration 8145, loss: 0.003226241795346141\n",
      "iteration 8146, loss: 0.004706524312496185\n",
      "iteration 8147, loss: 0.002930098446086049\n",
      "iteration 8148, loss: 0.003773182863369584\n",
      "iteration 8149, loss: 0.003465353511273861\n",
      "iteration 8150, loss: 0.004115276038646698\n",
      "iteration 8151, loss: 0.00409814715385437\n",
      "iteration 8152, loss: 0.0044863587245345116\n",
      "iteration 8153, loss: 0.003800273174419999\n",
      "iteration 8154, loss: 0.004199127666652203\n",
      "iteration 8155, loss: 0.003602940123528242\n",
      "iteration 8156, loss: 0.004011981189250946\n",
      "iteration 8157, loss: 0.0037201656959950924\n",
      "iteration 8158, loss: 0.003987153992056847\n",
      "iteration 8159, loss: 0.004455344285815954\n",
      "iteration 8160, loss: 0.004222184419631958\n",
      "iteration 8161, loss: 0.004248740151524544\n",
      "iteration 8162, loss: 0.0036991145461797714\n",
      "iteration 8163, loss: 0.004494870081543922\n",
      "iteration 8164, loss: 0.004569890908896923\n",
      "iteration 8165, loss: 0.0032606443855911493\n",
      "iteration 8166, loss: 0.004633510950952768\n",
      "iteration 8167, loss: 0.0036510038189589977\n",
      "iteration 8168, loss: 0.003997518215328455\n",
      "iteration 8169, loss: 0.0034357672557234764\n",
      "iteration 8170, loss: 0.0038668764755129814\n",
      "iteration 8171, loss: 0.003983491566032171\n",
      "iteration 8172, loss: 0.004210047423839569\n",
      "iteration 8173, loss: 0.004030182957649231\n",
      "iteration 8174, loss: 0.0043340642005205154\n",
      "iteration 8175, loss: 0.00419588340446353\n",
      "iteration 8176, loss: 0.0032830494455993176\n",
      "iteration 8177, loss: 0.0036118188872933388\n",
      "iteration 8178, loss: 0.003531151218339801\n",
      "iteration 8179, loss: 0.003704660339280963\n",
      "iteration 8180, loss: 0.0036120922304689884\n",
      "iteration 8181, loss: 0.003210489172488451\n",
      "iteration 8182, loss: 0.003260477911680937\n",
      "iteration 8183, loss: 0.0036420589312911034\n",
      "iteration 8184, loss: 0.00348112266510725\n",
      "iteration 8185, loss: 0.0035727866925299168\n",
      "iteration 8186, loss: 0.00317057641223073\n",
      "iteration 8187, loss: 0.0031317714601755142\n",
      "iteration 8188, loss: 0.004071488976478577\n",
      "iteration 8189, loss: 0.003195481840521097\n",
      "iteration 8190, loss: 0.003776990342885256\n",
      "iteration 8191, loss: 0.003691393416374922\n",
      "iteration 8192, loss: 0.003402452217414975\n",
      "iteration 8193, loss: 0.0032705713529139757\n",
      "iteration 8194, loss: 0.0037053637206554413\n",
      "iteration 8195, loss: 0.003383946605026722\n",
      "iteration 8196, loss: 0.0029943378176540136\n",
      "iteration 8197, loss: 0.003346147947013378\n",
      "iteration 8198, loss: 0.004495345521718264\n",
      "iteration 8199, loss: 0.005020122043788433\n",
      "iteration 8200, loss: 0.0033972682431340218\n",
      "iteration 8201, loss: 0.004448784980922937\n",
      "iteration 8202, loss: 0.0032376525923609734\n",
      "iteration 8203, loss: 0.0038837925530970097\n",
      "iteration 8204, loss: 0.004174596630036831\n",
      "iteration 8205, loss: 0.003840581513941288\n",
      "iteration 8206, loss: 0.003373838495463133\n",
      "iteration 8207, loss: 0.003490858245640993\n",
      "iteration 8208, loss: 0.004669814370572567\n",
      "iteration 8209, loss: 0.004214374348521233\n",
      "iteration 8210, loss: 0.003659705398604274\n",
      "iteration 8211, loss: 0.00342969736084342\n",
      "iteration 8212, loss: 0.0036881547421216965\n",
      "iteration 8213, loss: 0.003852273803204298\n",
      "iteration 8214, loss: 0.003868164960294962\n",
      "iteration 8215, loss: 0.004054938908666372\n",
      "iteration 8216, loss: 0.004479581955820322\n",
      "iteration 8217, loss: 0.003477086080238223\n",
      "iteration 8218, loss: 0.0046600173227488995\n",
      "iteration 8219, loss: 0.0036282441578805447\n",
      "iteration 8220, loss: 0.00350125296972692\n",
      "iteration 8221, loss: 0.004419784992933273\n",
      "iteration 8222, loss: 0.003044665791094303\n",
      "iteration 8223, loss: 0.003981352783739567\n",
      "iteration 8224, loss: 0.004598512314260006\n",
      "iteration 8225, loss: 0.003970785066485405\n",
      "iteration 8226, loss: 0.003944572061300278\n",
      "iteration 8227, loss: 0.003866636659950018\n",
      "iteration 8228, loss: 0.004269334953278303\n",
      "iteration 8229, loss: 0.0033022421412169933\n",
      "iteration 8230, loss: 0.003181176260113716\n",
      "iteration 8231, loss: 0.003527148626744747\n",
      "iteration 8232, loss: 0.0044905440881848335\n",
      "iteration 8233, loss: 0.003255384974181652\n",
      "iteration 8234, loss: 0.0036055275704711676\n",
      "iteration 8235, loss: 0.0036669261753559113\n",
      "iteration 8236, loss: 0.0037151577416807413\n",
      "iteration 8237, loss: 0.0036165104247629642\n",
      "iteration 8238, loss: 0.004315764643251896\n",
      "iteration 8239, loss: 0.0036585847847163677\n",
      "iteration 8240, loss: 0.0033379693049937487\n",
      "iteration 8241, loss: 0.0039793141186237335\n",
      "iteration 8242, loss: 0.003719919826835394\n",
      "iteration 8243, loss: 0.004421841353178024\n",
      "iteration 8244, loss: 0.00414802273735404\n",
      "iteration 8245, loss: 0.0042726462706923485\n",
      "iteration 8246, loss: 0.0033277615439146757\n",
      "iteration 8247, loss: 0.003952887374907732\n",
      "iteration 8248, loss: 0.0033358584623783827\n",
      "iteration 8249, loss: 0.0036395250353962183\n",
      "iteration 8250, loss: 0.003792361356317997\n",
      "iteration 8251, loss: 0.003632699605077505\n",
      "iteration 8252, loss: 0.004186395555734634\n",
      "iteration 8253, loss: 0.004063555039465427\n",
      "iteration 8254, loss: 0.00410128477960825\n",
      "iteration 8255, loss: 0.003778473474085331\n",
      "iteration 8256, loss: 0.0036966525949537754\n",
      "iteration 8257, loss: 0.0043510026298463345\n",
      "iteration 8258, loss: 0.0037798460107296705\n",
      "iteration 8259, loss: 0.003685615723952651\n",
      "iteration 8260, loss: 0.0035132807679474354\n",
      "iteration 8261, loss: 0.003558433149009943\n",
      "iteration 8262, loss: 0.0037088494282215834\n",
      "iteration 8263, loss: 0.0038029965944588184\n",
      "iteration 8264, loss: 0.0034242989495396614\n",
      "iteration 8265, loss: 0.0036165453493595123\n",
      "iteration 8266, loss: 0.00400138832628727\n",
      "iteration 8267, loss: 0.00453649926930666\n",
      "iteration 8268, loss: 0.0034239664673805237\n",
      "iteration 8269, loss: 0.004483741708099842\n",
      "iteration 8270, loss: 0.0036564762704074383\n",
      "iteration 8271, loss: 0.003187797265127301\n",
      "iteration 8272, loss: 0.0037922165356576443\n",
      "iteration 8273, loss: 0.0035621155984699726\n",
      "iteration 8274, loss: 0.0040250178426504135\n",
      "iteration 8275, loss: 0.003704376984387636\n",
      "iteration 8276, loss: 0.0035963640548288822\n",
      "iteration 8277, loss: 0.0036085841711610556\n",
      "iteration 8278, loss: 0.003912640735507011\n",
      "iteration 8279, loss: 0.0038304852787405252\n",
      "iteration 8280, loss: 0.0035665957257151604\n",
      "iteration 8281, loss: 0.004100731573998928\n",
      "iteration 8282, loss: 0.0034758164547383785\n",
      "iteration 8283, loss: 0.0036086593754589558\n",
      "iteration 8284, loss: 0.00333376694470644\n",
      "iteration 8285, loss: 0.003216506913304329\n",
      "iteration 8286, loss: 0.0035064001567661762\n",
      "iteration 8287, loss: 0.003991189878433943\n",
      "iteration 8288, loss: 0.0037368261255323887\n",
      "iteration 8289, loss: 0.0041485256515443325\n",
      "iteration 8290, loss: 0.003858678974211216\n",
      "iteration 8291, loss: 0.005262735299766064\n",
      "iteration 8292, loss: 0.0037731477059423923\n",
      "iteration 8293, loss: 0.003810760797932744\n",
      "iteration 8294, loss: 0.0037484760396182537\n",
      "iteration 8295, loss: 0.004007189534604549\n",
      "iteration 8296, loss: 0.004216562956571579\n",
      "iteration 8297, loss: 0.0039972662925720215\n",
      "iteration 8298, loss: 0.0037642468232661486\n",
      "iteration 8299, loss: 0.0034256340004503727\n",
      "iteration 8300, loss: 0.003898932598531246\n",
      "iteration 8301, loss: 0.003525413339957595\n",
      "iteration 8302, loss: 0.0032045040279626846\n",
      "iteration 8303, loss: 0.0037465086206793785\n",
      "iteration 8304, loss: 0.003821330377832055\n",
      "iteration 8305, loss: 0.0034928005188703537\n",
      "iteration 8306, loss: 0.0037858663126826286\n",
      "iteration 8307, loss: 0.004248242359608412\n",
      "iteration 8308, loss: 0.004136189818382263\n",
      "iteration 8309, loss: 0.003664286807179451\n",
      "iteration 8310, loss: 0.004601884633302689\n",
      "iteration 8311, loss: 0.0035995161160826683\n",
      "iteration 8312, loss: 0.0035969200544059277\n",
      "iteration 8313, loss: 0.003669348545372486\n",
      "iteration 8314, loss: 0.0029372433200478554\n",
      "iteration 8315, loss: 0.003475781297311187\n",
      "iteration 8316, loss: 0.00346096302382648\n",
      "iteration 8317, loss: 0.0033577848225831985\n",
      "iteration 8318, loss: 0.0034401912707835436\n",
      "iteration 8319, loss: 0.004441205877810717\n",
      "iteration 8320, loss: 0.0037044701166450977\n",
      "iteration 8321, loss: 0.0036118021234869957\n",
      "iteration 8322, loss: 0.0036251123528927565\n",
      "iteration 8323, loss: 0.003773484379053116\n",
      "iteration 8324, loss: 0.004145334474742413\n",
      "iteration 8325, loss: 0.003212603274732828\n",
      "iteration 8326, loss: 0.003862147917971015\n",
      "iteration 8327, loss: 0.0037637120112776756\n",
      "iteration 8328, loss: 0.0037811181973665953\n",
      "iteration 8329, loss: 0.00380006805062294\n",
      "iteration 8330, loss: 0.003288961248472333\n",
      "iteration 8331, loss: 0.003827522974461317\n",
      "iteration 8332, loss: 0.0037112003192305565\n",
      "iteration 8333, loss: 0.004135634750127792\n",
      "iteration 8334, loss: 0.004099543672055006\n",
      "iteration 8335, loss: 0.0035749361850321293\n",
      "iteration 8336, loss: 0.003703540889546275\n",
      "iteration 8337, loss: 0.003071218030527234\n",
      "iteration 8338, loss: 0.004002399742603302\n",
      "iteration 8339, loss: 0.0035696914419531822\n",
      "iteration 8340, loss: 0.0036265752278268337\n",
      "iteration 8341, loss: 0.003376137465238571\n",
      "iteration 8342, loss: 0.003979022614657879\n",
      "iteration 8343, loss: 0.0037399763241410255\n",
      "iteration 8344, loss: 0.0035642762668430805\n",
      "iteration 8345, loss: 0.0035181096754968166\n",
      "iteration 8346, loss: 0.0036540767177939415\n",
      "iteration 8347, loss: 0.004070580005645752\n",
      "iteration 8348, loss: 0.004018258769065142\n",
      "iteration 8349, loss: 0.0037375660613179207\n",
      "iteration 8350, loss: 0.003412828082218766\n",
      "iteration 8351, loss: 0.003889147425070405\n",
      "iteration 8352, loss: 0.002858124440535903\n",
      "iteration 8353, loss: 0.003910626284778118\n",
      "iteration 8354, loss: 0.0034039206802845\n",
      "iteration 8355, loss: 0.0033971501979976892\n",
      "iteration 8356, loss: 0.0040100738406181335\n",
      "iteration 8357, loss: 0.004311977420002222\n",
      "iteration 8358, loss: 0.003785676322877407\n",
      "iteration 8359, loss: 0.004004130605608225\n",
      "iteration 8360, loss: 0.0033655071165412664\n",
      "iteration 8361, loss: 0.0037589631974697113\n",
      "iteration 8362, loss: 0.0035920245572924614\n",
      "iteration 8363, loss: 0.004145767539739609\n",
      "iteration 8364, loss: 0.0033217836171388626\n",
      "iteration 8365, loss: 0.00359937222674489\n",
      "iteration 8366, loss: 0.0038058594800531864\n",
      "iteration 8367, loss: 0.003468912560492754\n",
      "iteration 8368, loss: 0.00442859111353755\n",
      "iteration 8369, loss: 0.0036133863031864166\n",
      "iteration 8370, loss: 0.004143172409385443\n",
      "iteration 8371, loss: 0.003933875821530819\n",
      "iteration 8372, loss: 0.0035731762181967497\n",
      "iteration 8373, loss: 0.0037485710345208645\n",
      "iteration 8374, loss: 0.004212519619613886\n",
      "iteration 8375, loss: 0.004597187507897615\n",
      "iteration 8376, loss: 0.0035928573925048113\n",
      "iteration 8377, loss: 0.003877308452501893\n",
      "iteration 8378, loss: 0.0033972179517149925\n",
      "iteration 8379, loss: 0.0034635397605597973\n",
      "iteration 8380, loss: 0.004123303573578596\n",
      "iteration 8381, loss: 0.0035021696239709854\n",
      "iteration 8382, loss: 0.0040915231220424175\n",
      "iteration 8383, loss: 0.002836039289832115\n",
      "iteration 8384, loss: 0.004292911849915981\n",
      "iteration 8385, loss: 0.0038419081829488277\n",
      "iteration 8386, loss: 0.0033192213159054518\n",
      "iteration 8387, loss: 0.0034974971786141396\n",
      "iteration 8388, loss: 0.003768324153497815\n",
      "iteration 8389, loss: 0.003567643230780959\n",
      "iteration 8390, loss: 0.0035564873833209276\n",
      "iteration 8391, loss: 0.003635045140981674\n",
      "iteration 8392, loss: 0.003251755842939019\n",
      "iteration 8393, loss: 0.003896672511473298\n",
      "iteration 8394, loss: 0.003490068018436432\n",
      "iteration 8395, loss: 0.0031533860601484776\n",
      "iteration 8396, loss: 0.003549151588231325\n",
      "iteration 8397, loss: 0.003703854978084564\n",
      "iteration 8398, loss: 0.003306065686047077\n",
      "iteration 8399, loss: 0.003537634387612343\n",
      "iteration 8400, loss: 0.0033842362463474274\n",
      "iteration 8401, loss: 0.0037310642655938864\n",
      "iteration 8402, loss: 0.0035932855680584908\n",
      "iteration 8403, loss: 0.0035175466910004616\n",
      "iteration 8404, loss: 0.0035340548492968082\n",
      "iteration 8405, loss: 0.004119374789297581\n",
      "iteration 8406, loss: 0.003714856691658497\n",
      "iteration 8407, loss: 0.0033205770887434483\n",
      "iteration 8408, loss: 0.0033579564187675714\n",
      "iteration 8409, loss: 0.003409340512007475\n",
      "iteration 8410, loss: 0.003269950160756707\n",
      "iteration 8411, loss: 0.003431593533605337\n",
      "iteration 8412, loss: 0.0036307782866060734\n",
      "iteration 8413, loss: 0.0033741318620741367\n",
      "iteration 8414, loss: 0.003237216267734766\n",
      "iteration 8415, loss: 0.0034986906684935093\n",
      "iteration 8416, loss: 0.0033134727273136377\n",
      "iteration 8417, loss: 0.003387455828487873\n",
      "iteration 8418, loss: 0.0033363376278430223\n",
      "iteration 8419, loss: 0.003627061378210783\n",
      "iteration 8420, loss: 0.0038810421247035265\n",
      "iteration 8421, loss: 0.0035097943618893623\n",
      "iteration 8422, loss: 0.0031433352269232273\n",
      "iteration 8423, loss: 0.004049691837280989\n",
      "iteration 8424, loss: 0.0033955296967178583\n",
      "iteration 8425, loss: 0.004559633322060108\n",
      "iteration 8426, loss: 0.003593506757169962\n",
      "iteration 8427, loss: 0.0038989693857729435\n",
      "iteration 8428, loss: 0.0032276432029902935\n",
      "iteration 8429, loss: 0.003688783384859562\n",
      "iteration 8430, loss: 0.0039109643548727036\n",
      "iteration 8431, loss: 0.003023269586265087\n",
      "iteration 8432, loss: 0.0040442366153001785\n",
      "iteration 8433, loss: 0.003763119922950864\n",
      "iteration 8434, loss: 0.003420657943934202\n",
      "iteration 8435, loss: 0.003750396426767111\n",
      "iteration 8436, loss: 0.003929817117750645\n",
      "iteration 8437, loss: 0.004104643128812313\n",
      "iteration 8438, loss: 0.003044357057660818\n",
      "iteration 8439, loss: 0.0034097894094884396\n",
      "iteration 8440, loss: 0.0033900716807693243\n",
      "iteration 8441, loss: 0.003234740812331438\n",
      "iteration 8442, loss: 0.003988401964306831\n",
      "iteration 8443, loss: 0.003168606199324131\n",
      "iteration 8444, loss: 0.0033096973784267902\n",
      "iteration 8445, loss: 0.0035522538237273693\n",
      "iteration 8446, loss: 0.0037361562717705965\n",
      "iteration 8447, loss: 0.0037680433597415686\n",
      "iteration 8448, loss: 0.003398392116650939\n",
      "iteration 8449, loss: 0.0029355292208492756\n",
      "iteration 8450, loss: 0.003885791637003422\n",
      "iteration 8451, loss: 0.0035041985101997852\n",
      "iteration 8452, loss: 0.0033651934936642647\n",
      "iteration 8453, loss: 0.0035202335566282272\n",
      "iteration 8454, loss: 0.003509640460833907\n",
      "iteration 8455, loss: 0.003891734639182687\n",
      "iteration 8456, loss: 0.0033551810774952173\n",
      "iteration 8457, loss: 0.0032054963521659374\n",
      "iteration 8458, loss: 0.004040814936161041\n",
      "iteration 8459, loss: 0.003386140102520585\n",
      "iteration 8460, loss: 0.0041128238663077354\n",
      "iteration 8461, loss: 0.0037765372544527054\n",
      "iteration 8462, loss: 0.0034065605141222477\n",
      "iteration 8463, loss: 0.004221043083816767\n",
      "iteration 8464, loss: 0.0033145598135888577\n",
      "iteration 8465, loss: 0.004580985754728317\n",
      "iteration 8466, loss: 0.0037352247163653374\n",
      "iteration 8467, loss: 0.0034042238257825375\n",
      "iteration 8468, loss: 0.003217131830751896\n",
      "iteration 8469, loss: 0.0035237278789281845\n",
      "iteration 8470, loss: 0.003920270130038261\n",
      "iteration 8471, loss: 0.0038158318493515253\n",
      "iteration 8472, loss: 0.0035004583187401295\n",
      "iteration 8473, loss: 0.0037667048163712025\n",
      "iteration 8474, loss: 0.003920889925211668\n",
      "iteration 8475, loss: 0.0030875778757035732\n",
      "iteration 8476, loss: 0.004074107855558395\n",
      "iteration 8477, loss: 0.003548604901880026\n",
      "iteration 8478, loss: 0.00411757780238986\n",
      "iteration 8479, loss: 0.003750323783606291\n",
      "iteration 8480, loss: 0.0034334412775933743\n",
      "iteration 8481, loss: 0.004176917020231485\n",
      "iteration 8482, loss: 0.004127263557165861\n",
      "iteration 8483, loss: 0.004026171751320362\n",
      "iteration 8484, loss: 0.0034840404987335205\n",
      "iteration 8485, loss: 0.0033231358975172043\n",
      "iteration 8486, loss: 0.004356679040938616\n",
      "iteration 8487, loss: 0.003455623984336853\n",
      "iteration 8488, loss: 0.0043219588696956635\n",
      "iteration 8489, loss: 0.0032836496829986572\n",
      "iteration 8490, loss: 0.003998352214694023\n",
      "iteration 8491, loss: 0.0036102759186178446\n",
      "iteration 8492, loss: 0.0037362067960202694\n",
      "iteration 8493, loss: 0.0032719578593969345\n",
      "iteration 8494, loss: 0.0037023751065135\n",
      "iteration 8495, loss: 0.0035288219805806875\n",
      "iteration 8496, loss: 0.003788189496845007\n",
      "iteration 8497, loss: 0.003405627328902483\n",
      "iteration 8498, loss: 0.003428118070587516\n",
      "iteration 8499, loss: 0.003810460679233074\n",
      "iteration 8500, loss: 0.0037302838172763586\n",
      "iteration 8501, loss: 0.003614356741309166\n",
      "iteration 8502, loss: 0.004183377604931593\n",
      "iteration 8503, loss: 0.0034821918234229088\n",
      "iteration 8504, loss: 0.002866084687411785\n",
      "iteration 8505, loss: 0.003989159129559994\n",
      "iteration 8506, loss: 0.0041177039965987206\n",
      "iteration 8507, loss: 0.0039008604362607002\n",
      "iteration 8508, loss: 0.0036944961175322533\n",
      "iteration 8509, loss: 0.003238972509279847\n",
      "iteration 8510, loss: 0.004124988801777363\n",
      "iteration 8511, loss: 0.003650086233392358\n",
      "iteration 8512, loss: 0.0029780296608805656\n",
      "iteration 8513, loss: 0.0036162007600069046\n",
      "iteration 8514, loss: 0.0038521746173501015\n",
      "iteration 8515, loss: 0.003484321292489767\n",
      "iteration 8516, loss: 0.003939558286219835\n",
      "iteration 8517, loss: 0.00353005388751626\n",
      "iteration 8518, loss: 0.003733433084562421\n",
      "iteration 8519, loss: 0.0029604285955429077\n",
      "iteration 8520, loss: 0.0036715082824230194\n",
      "iteration 8521, loss: 0.0033425581641495228\n",
      "iteration 8522, loss: 0.0037974584847688675\n",
      "iteration 8523, loss: 0.0035090879537165165\n",
      "iteration 8524, loss: 0.0033401628024876118\n",
      "iteration 8525, loss: 0.0036408761516213417\n",
      "iteration 8526, loss: 0.0038916501216590405\n",
      "iteration 8527, loss: 0.0033236800227314234\n",
      "iteration 8528, loss: 0.003226296044886112\n",
      "iteration 8529, loss: 0.0038116932846605778\n",
      "iteration 8530, loss: 0.003161303699016571\n",
      "iteration 8531, loss: 0.0042920783162117004\n",
      "iteration 8532, loss: 0.003586596343666315\n",
      "iteration 8533, loss: 0.003513469360768795\n",
      "iteration 8534, loss: 0.0035636965185403824\n",
      "iteration 8535, loss: 0.0036939317360520363\n",
      "iteration 8536, loss: 0.004233249928802252\n",
      "iteration 8537, loss: 0.0034517664462327957\n",
      "iteration 8538, loss: 0.0038582661654800177\n",
      "iteration 8539, loss: 0.0035471199080348015\n",
      "iteration 8540, loss: 0.004230203106999397\n",
      "iteration 8541, loss: 0.0035847751423716545\n",
      "iteration 8542, loss: 0.0035407557152211666\n",
      "iteration 8543, loss: 0.004178847651928663\n",
      "iteration 8544, loss: 0.004040609113872051\n",
      "iteration 8545, loss: 0.003643672214820981\n",
      "iteration 8546, loss: 0.003771133255213499\n",
      "iteration 8547, loss: 0.003073207102715969\n",
      "iteration 8548, loss: 0.003927803132683039\n",
      "iteration 8549, loss: 0.0038753331173211336\n",
      "iteration 8550, loss: 0.0037657273933291435\n",
      "iteration 8551, loss: 0.0038361377082765102\n",
      "iteration 8552, loss: 0.0039040199480950832\n",
      "iteration 8553, loss: 0.003310262691229582\n",
      "iteration 8554, loss: 0.003817360382527113\n",
      "iteration 8555, loss: 0.003925801254808903\n",
      "iteration 8556, loss: 0.0032495716586709023\n",
      "iteration 8557, loss: 0.0037564984522759914\n",
      "iteration 8558, loss: 0.003930755890905857\n",
      "iteration 8559, loss: 0.0042334310710430145\n",
      "iteration 8560, loss: 0.003138831350952387\n",
      "iteration 8561, loss: 0.004001191817224026\n",
      "iteration 8562, loss: 0.004283269867300987\n",
      "iteration 8563, loss: 0.003852300578728318\n",
      "iteration 8564, loss: 0.0039019486866891384\n",
      "iteration 8565, loss: 0.0035290003288537264\n",
      "iteration 8566, loss: 0.003793880343437195\n",
      "iteration 8567, loss: 0.0037093362770974636\n",
      "iteration 8568, loss: 0.003227190114557743\n",
      "iteration 8569, loss: 0.00363597902469337\n",
      "iteration 8570, loss: 0.0035096732899546623\n",
      "iteration 8571, loss: 0.003593338653445244\n",
      "iteration 8572, loss: 0.0035521313548088074\n",
      "iteration 8573, loss: 0.0032453560270369053\n",
      "iteration 8574, loss: 0.0037140855565667152\n",
      "iteration 8575, loss: 0.0032367692328989506\n",
      "iteration 8576, loss: 0.0029383660294115543\n",
      "iteration 8577, loss: 0.0037987777031958103\n",
      "iteration 8578, loss: 0.004096164833754301\n",
      "iteration 8579, loss: 0.0036897817626595497\n",
      "iteration 8580, loss: 0.0034501247573643923\n",
      "iteration 8581, loss: 0.003997954074293375\n",
      "iteration 8582, loss: 0.00346506224013865\n",
      "iteration 8583, loss: 0.0038051577284932137\n",
      "iteration 8584, loss: 0.003314375411719084\n",
      "iteration 8585, loss: 0.004019224084913731\n",
      "iteration 8586, loss: 0.0036653103306889534\n",
      "iteration 8587, loss: 0.0038494234904646873\n",
      "iteration 8588, loss: 0.003467773087322712\n",
      "iteration 8589, loss: 0.003684690222144127\n",
      "iteration 8590, loss: 0.0032434589229524136\n",
      "iteration 8591, loss: 0.0033888875041157007\n",
      "iteration 8592, loss: 0.003397169057279825\n",
      "iteration 8593, loss: 0.0031731680501252413\n",
      "iteration 8594, loss: 0.0033592607360333204\n",
      "iteration 8595, loss: 0.0030657623428851366\n",
      "iteration 8596, loss: 0.003864283673465252\n",
      "iteration 8597, loss: 0.003689210396260023\n",
      "iteration 8598, loss: 0.003622719319537282\n",
      "iteration 8599, loss: 0.0033088577911257744\n",
      "iteration 8600, loss: 0.0032092875335365534\n",
      "iteration 8601, loss: 0.0035602941643446684\n",
      "iteration 8602, loss: 0.003321742406114936\n",
      "iteration 8603, loss: 0.0029512448236346245\n",
      "iteration 8604, loss: 0.0036362470127642155\n",
      "iteration 8605, loss: 0.003398820059373975\n",
      "iteration 8606, loss: 0.0035815280862152576\n",
      "iteration 8607, loss: 0.0040618181228637695\n",
      "iteration 8608, loss: 0.003523054998368025\n",
      "iteration 8609, loss: 0.0039023403078317642\n",
      "iteration 8610, loss: 0.003147504758089781\n",
      "iteration 8611, loss: 0.0043164268136024475\n",
      "iteration 8612, loss: 0.0037751011550426483\n",
      "iteration 8613, loss: 0.00314119178801775\n",
      "iteration 8614, loss: 0.0043703727424144745\n",
      "iteration 8615, loss: 0.0038915975019335747\n",
      "iteration 8616, loss: 0.0035736714489758015\n",
      "iteration 8617, loss: 0.00391582353040576\n",
      "iteration 8618, loss: 0.0035346723161637783\n",
      "iteration 8619, loss: 0.0032760901376605034\n",
      "iteration 8620, loss: 0.0038171098567545414\n",
      "iteration 8621, loss: 0.003395962994545698\n",
      "iteration 8622, loss: 0.0042081791907548904\n",
      "iteration 8623, loss: 0.0032871332950890064\n",
      "iteration 8624, loss: 0.0032486070413142443\n",
      "iteration 8625, loss: 0.0034654499031603336\n",
      "iteration 8626, loss: 0.003465411253273487\n",
      "iteration 8627, loss: 0.0036081247963011265\n",
      "iteration 8628, loss: 0.003080956172198057\n",
      "iteration 8629, loss: 0.0030104578472673893\n",
      "iteration 8630, loss: 0.0035597647074609995\n",
      "iteration 8631, loss: 0.003805294632911682\n",
      "iteration 8632, loss: 0.0036735981702804565\n",
      "iteration 8633, loss: 0.0034248752053827047\n",
      "iteration 8634, loss: 0.0037495605647563934\n",
      "iteration 8635, loss: 0.003500499529764056\n",
      "iteration 8636, loss: 0.003430705051869154\n",
      "iteration 8637, loss: 0.003441872540861368\n",
      "iteration 8638, loss: 0.00422588549554348\n",
      "iteration 8639, loss: 0.004043622873723507\n",
      "iteration 8640, loss: 0.0032962565310299397\n",
      "iteration 8641, loss: 0.003496984951198101\n",
      "iteration 8642, loss: 0.003569460241124034\n",
      "iteration 8643, loss: 0.0036227144300937653\n",
      "iteration 8644, loss: 0.0030994557309895754\n",
      "iteration 8645, loss: 0.0032114542555063963\n",
      "iteration 8646, loss: 0.003700000001117587\n",
      "iteration 8647, loss: 0.003534833900630474\n",
      "iteration 8648, loss: 0.0035621756687760353\n",
      "iteration 8649, loss: 0.00358314230106771\n",
      "iteration 8650, loss: 0.004593224264681339\n",
      "iteration 8651, loss: 0.003537809941917658\n",
      "iteration 8652, loss: 0.00327250175178051\n",
      "iteration 8653, loss: 0.003755544312298298\n",
      "iteration 8654, loss: 0.0037293427158147097\n",
      "iteration 8655, loss: 0.004309189971536398\n",
      "iteration 8656, loss: 0.0039016129449009895\n",
      "iteration 8657, loss: 0.0032291566021740437\n",
      "iteration 8658, loss: 0.003657541936263442\n",
      "iteration 8659, loss: 0.0031679519452154636\n",
      "iteration 8660, loss: 0.003529755398631096\n",
      "iteration 8661, loss: 0.00363306631334126\n",
      "iteration 8662, loss: 0.0035938324872404337\n",
      "iteration 8663, loss: 0.003278191667050123\n",
      "iteration 8664, loss: 0.0033572912216186523\n",
      "iteration 8665, loss: 0.003417541738599539\n",
      "iteration 8666, loss: 0.0034743596334010363\n",
      "iteration 8667, loss: 0.0033431188203394413\n",
      "iteration 8668, loss: 0.0035072024911642075\n",
      "iteration 8669, loss: 0.0032744240015745163\n",
      "iteration 8670, loss: 0.0035406951792538166\n",
      "iteration 8671, loss: 0.0043959240429103374\n",
      "iteration 8672, loss: 0.0029898458160459995\n",
      "iteration 8673, loss: 0.0030942480079829693\n",
      "iteration 8674, loss: 0.0032078407239168882\n",
      "iteration 8675, loss: 0.004455034155398607\n",
      "iteration 8676, loss: 0.0038343321066349745\n",
      "iteration 8677, loss: 0.003686795011162758\n",
      "iteration 8678, loss: 0.003269319422543049\n",
      "iteration 8679, loss: 0.003445165930315852\n",
      "iteration 8680, loss: 0.0034535687882453203\n",
      "iteration 8681, loss: 0.003611538093537092\n",
      "iteration 8682, loss: 0.003368464997038245\n",
      "iteration 8683, loss: 0.0035036723129451275\n",
      "iteration 8684, loss: 0.00360096781514585\n",
      "iteration 8685, loss: 0.002918223850429058\n",
      "iteration 8686, loss: 0.004040225874632597\n",
      "iteration 8687, loss: 0.00376194529235363\n",
      "iteration 8688, loss: 0.003536034608259797\n",
      "iteration 8689, loss: 0.0036023843567818403\n",
      "iteration 8690, loss: 0.0035922552924603224\n",
      "iteration 8691, loss: 0.0029083178378641605\n",
      "iteration 8692, loss: 0.0038776337169110775\n",
      "iteration 8693, loss: 0.003169413423165679\n",
      "iteration 8694, loss: 0.003237141063436866\n",
      "iteration 8695, loss: 0.0033112000674009323\n",
      "iteration 8696, loss: 0.003576265648007393\n",
      "iteration 8697, loss: 0.0036106526385992765\n",
      "iteration 8698, loss: 0.003602423705160618\n",
      "iteration 8699, loss: 0.00309172086417675\n",
      "iteration 8700, loss: 0.0036315249744802713\n",
      "iteration 8701, loss: 0.002821662463247776\n",
      "iteration 8702, loss: 0.0030979011207818985\n",
      "iteration 8703, loss: 0.003548423293977976\n",
      "iteration 8704, loss: 0.0032379478216171265\n",
      "iteration 8705, loss: 0.003516258206218481\n",
      "iteration 8706, loss: 0.004022669047117233\n",
      "iteration 8707, loss: 0.0035189390182495117\n",
      "iteration 8708, loss: 0.003896249458193779\n",
      "iteration 8709, loss: 0.0032416866160929203\n",
      "iteration 8710, loss: 0.0034232072066515684\n",
      "iteration 8711, loss: 0.0030837743543088436\n",
      "iteration 8712, loss: 0.003626992693170905\n",
      "iteration 8713, loss: 0.004231607541441917\n",
      "iteration 8714, loss: 0.0027329172007739544\n",
      "iteration 8715, loss: 0.002987057203426957\n",
      "iteration 8716, loss: 0.003351965919137001\n",
      "iteration 8717, loss: 0.003547820495441556\n",
      "iteration 8718, loss: 0.0036028502508997917\n",
      "iteration 8719, loss: 0.0030578088480979204\n",
      "iteration 8720, loss: 0.003558605443686247\n",
      "iteration 8721, loss: 0.003528307192027569\n",
      "iteration 8722, loss: 0.0041492534801363945\n",
      "iteration 8723, loss: 0.0027377246879041195\n",
      "iteration 8724, loss: 0.003354668151587248\n",
      "iteration 8725, loss: 0.0036929668858647346\n",
      "iteration 8726, loss: 0.003455841215327382\n",
      "iteration 8727, loss: 0.0033508813939988613\n",
      "iteration 8728, loss: 0.003822892904281616\n",
      "iteration 8729, loss: 0.0034583949018269777\n",
      "iteration 8730, loss: 0.0030652463901787996\n",
      "iteration 8731, loss: 0.0035291952081024647\n",
      "iteration 8732, loss: 0.0033287974074482918\n",
      "iteration 8733, loss: 0.0034795557148754597\n",
      "iteration 8734, loss: 0.0031144064851105213\n",
      "iteration 8735, loss: 0.0037677367217838764\n",
      "iteration 8736, loss: 0.0036108470521867275\n",
      "iteration 8737, loss: 0.0028824107721447945\n",
      "iteration 8738, loss: 0.0031116525642573833\n",
      "iteration 8739, loss: 0.003122622612863779\n",
      "iteration 8740, loss: 0.0035025994293391705\n",
      "iteration 8741, loss: 0.004078911617398262\n",
      "iteration 8742, loss: 0.003153085708618164\n",
      "iteration 8743, loss: 0.0033359741792082787\n",
      "iteration 8744, loss: 0.004233117215335369\n",
      "iteration 8745, loss: 0.004640023689717054\n",
      "iteration 8746, loss: 0.0034438353031873703\n",
      "iteration 8747, loss: 0.0030803903937339783\n",
      "iteration 8748, loss: 0.0034357805270701647\n",
      "iteration 8749, loss: 0.0032281295862048864\n",
      "iteration 8750, loss: 0.003402869449928403\n",
      "iteration 8751, loss: 0.003652253421023488\n",
      "iteration 8752, loss: 0.004077309742569923\n",
      "iteration 8753, loss: 0.0034820481669157743\n",
      "iteration 8754, loss: 0.0032610767520964146\n",
      "iteration 8755, loss: 0.003535728668794036\n",
      "iteration 8756, loss: 0.0032518128864467144\n",
      "iteration 8757, loss: 0.0033257859759032726\n",
      "iteration 8758, loss: 0.0035980555694550276\n",
      "iteration 8759, loss: 0.0034952133428305387\n",
      "iteration 8760, loss: 0.0035111382603645325\n",
      "iteration 8761, loss: 0.0034557494800537825\n",
      "iteration 8762, loss: 0.0035127676092088223\n",
      "iteration 8763, loss: 0.004075101111084223\n",
      "iteration 8764, loss: 0.002714046509936452\n",
      "iteration 8765, loss: 0.0027461445424705744\n",
      "iteration 8766, loss: 0.0031419391743838787\n",
      "iteration 8767, loss: 0.0036171777173876762\n",
      "iteration 8768, loss: 0.0031919730827212334\n",
      "iteration 8769, loss: 0.0036825155839323997\n",
      "iteration 8770, loss: 0.004154200199991465\n",
      "iteration 8771, loss: 0.0033748408313840628\n",
      "iteration 8772, loss: 0.00424696272239089\n",
      "iteration 8773, loss: 0.0032926597632467747\n",
      "iteration 8774, loss: 0.0033381725661456585\n",
      "iteration 8775, loss: 0.003836721647530794\n",
      "iteration 8776, loss: 0.003468952141702175\n",
      "iteration 8777, loss: 0.0032729683443903923\n",
      "iteration 8778, loss: 0.003704408183693886\n",
      "iteration 8779, loss: 0.003509177826344967\n",
      "iteration 8780, loss: 0.0033485786989331245\n",
      "iteration 8781, loss: 0.003564679529517889\n",
      "iteration 8782, loss: 0.0036824005655944347\n",
      "iteration 8783, loss: 0.0037533389404416084\n",
      "iteration 8784, loss: 0.003288235515356064\n",
      "iteration 8785, loss: 0.0034527666866779327\n",
      "iteration 8786, loss: 0.0031549495179206133\n",
      "iteration 8787, loss: 0.003315115347504616\n",
      "iteration 8788, loss: 0.0027497666887938976\n",
      "iteration 8789, loss: 0.004462067503482103\n",
      "iteration 8790, loss: 0.0037522942293435335\n",
      "iteration 8791, loss: 0.0038116525392979383\n",
      "iteration 8792, loss: 0.0034746944438666105\n",
      "iteration 8793, loss: 0.0030362936668097973\n",
      "iteration 8794, loss: 0.0032943044789135456\n",
      "iteration 8795, loss: 0.002745815785601735\n",
      "iteration 8796, loss: 0.0034367176704108715\n",
      "iteration 8797, loss: 0.0033191158436238766\n",
      "iteration 8798, loss: 0.0037759291008114815\n",
      "iteration 8799, loss: 0.0033927808981388807\n",
      "iteration 8800, loss: 0.0032236496917903423\n",
      "iteration 8801, loss: 0.002957718912512064\n",
      "iteration 8802, loss: 0.003716722596436739\n",
      "iteration 8803, loss: 0.0032804638613015413\n",
      "iteration 8804, loss: 0.00328234164044261\n",
      "iteration 8805, loss: 0.0035522659309208393\n",
      "iteration 8806, loss: 0.0034483452327549458\n",
      "iteration 8807, loss: 0.0032193721272051334\n",
      "iteration 8808, loss: 0.0036724701058119535\n",
      "iteration 8809, loss: 0.003685224801301956\n",
      "iteration 8810, loss: 0.003283299971371889\n",
      "iteration 8811, loss: 0.0030748266726732254\n",
      "iteration 8812, loss: 0.0030236844904720783\n",
      "iteration 8813, loss: 0.0031687018927186728\n",
      "iteration 8814, loss: 0.003463913919404149\n",
      "iteration 8815, loss: 0.003107347059994936\n",
      "iteration 8816, loss: 0.003345807082951069\n",
      "iteration 8817, loss: 0.0025602970272302628\n",
      "iteration 8818, loss: 0.004178470000624657\n",
      "iteration 8819, loss: 0.003432912053540349\n",
      "iteration 8820, loss: 0.002841297071427107\n",
      "iteration 8821, loss: 0.0031064199283719063\n",
      "iteration 8822, loss: 0.003711783792823553\n",
      "iteration 8823, loss: 0.0033344514667987823\n",
      "iteration 8824, loss: 0.002946295775473118\n",
      "iteration 8825, loss: 0.0034221364185214043\n",
      "iteration 8826, loss: 0.003397910622879863\n",
      "iteration 8827, loss: 0.003503353800624609\n",
      "iteration 8828, loss: 0.0036064255982637405\n",
      "iteration 8829, loss: 0.0038307670038193464\n",
      "iteration 8830, loss: 0.0032995743677020073\n",
      "iteration 8831, loss: 0.0036581573076546192\n",
      "iteration 8832, loss: 0.0036658630706369877\n",
      "iteration 8833, loss: 0.0032705124467611313\n",
      "iteration 8834, loss: 0.0028725923039019108\n",
      "iteration 8835, loss: 0.0036232424899935722\n",
      "iteration 8836, loss: 0.004543784074485302\n",
      "iteration 8837, loss: 0.0030999057926237583\n",
      "iteration 8838, loss: 0.003497330006211996\n",
      "iteration 8839, loss: 0.00366544583812356\n",
      "iteration 8840, loss: 0.0030635944567620754\n",
      "iteration 8841, loss: 0.003568581072613597\n",
      "iteration 8842, loss: 0.0028874892741441727\n",
      "iteration 8843, loss: 0.002792414976283908\n",
      "iteration 8844, loss: 0.0030147377401590347\n",
      "iteration 8845, loss: 0.00309180561453104\n",
      "iteration 8846, loss: 0.0030803417321294546\n",
      "iteration 8847, loss: 0.0029530867468565702\n",
      "iteration 8848, loss: 0.0035290485247969627\n",
      "iteration 8849, loss: 0.0031014590058475733\n",
      "iteration 8850, loss: 0.0030658633913844824\n",
      "iteration 8851, loss: 0.0028010800015181303\n",
      "iteration 8852, loss: 0.003444833680987358\n",
      "iteration 8853, loss: 0.0029687765054404736\n",
      "iteration 8854, loss: 0.0030951269436627626\n",
      "iteration 8855, loss: 0.0034978194162249565\n",
      "iteration 8856, loss: 0.0030170606914907694\n",
      "iteration 8857, loss: 0.0031730416230857372\n",
      "iteration 8858, loss: 0.003016284666955471\n",
      "iteration 8859, loss: 0.003184010274708271\n",
      "iteration 8860, loss: 0.0027743694372475147\n",
      "iteration 8861, loss: 0.003262944519519806\n",
      "iteration 8862, loss: 0.0033904630690813065\n",
      "iteration 8863, loss: 0.0035907127894461155\n",
      "iteration 8864, loss: 0.00447314465418458\n",
      "iteration 8865, loss: 0.003993989899754524\n",
      "iteration 8866, loss: 0.003631599945947528\n",
      "iteration 8867, loss: 0.0035916061606258154\n",
      "iteration 8868, loss: 0.0037077232263982296\n",
      "iteration 8869, loss: 0.0033803172409534454\n",
      "iteration 8870, loss: 0.0028649084270000458\n",
      "iteration 8871, loss: 0.0036230580881237984\n",
      "iteration 8872, loss: 0.0029469167347997427\n",
      "iteration 8873, loss: 0.003689201083034277\n",
      "iteration 8874, loss: 0.0033998219296336174\n",
      "iteration 8875, loss: 0.0034784702584147453\n",
      "iteration 8876, loss: 0.003365148324519396\n",
      "iteration 8877, loss: 0.0033409465104341507\n",
      "iteration 8878, loss: 0.0029990484472364187\n",
      "iteration 8879, loss: 0.0029078894294798374\n",
      "iteration 8880, loss: 0.003242586273699999\n",
      "iteration 8881, loss: 0.003297354793176055\n",
      "iteration 8882, loss: 0.003291995031759143\n",
      "iteration 8883, loss: 0.003099698107689619\n",
      "iteration 8884, loss: 0.0031400988809764385\n",
      "iteration 8885, loss: 0.0033845757134258747\n",
      "iteration 8886, loss: 0.0035682618618011475\n",
      "iteration 8887, loss: 0.00301090395078063\n",
      "iteration 8888, loss: 0.0035818777978420258\n",
      "iteration 8889, loss: 0.003909931518137455\n",
      "iteration 8890, loss: 0.0028664269484579563\n",
      "iteration 8891, loss: 0.003340670373290777\n",
      "iteration 8892, loss: 0.003343385411426425\n",
      "iteration 8893, loss: 0.0037868923973292112\n",
      "iteration 8894, loss: 0.003513867035508156\n",
      "iteration 8895, loss: 0.00318208709359169\n",
      "iteration 8896, loss: 0.0032526140566915274\n",
      "iteration 8897, loss: 0.0032271603122353554\n",
      "iteration 8898, loss: 0.0037210192531347275\n",
      "iteration 8899, loss: 0.0033413569908589125\n",
      "iteration 8900, loss: 0.004220105707645416\n",
      "iteration 8901, loss: 0.00327314087189734\n",
      "iteration 8902, loss: 0.0033582933247089386\n",
      "iteration 8903, loss: 0.003405041992664337\n",
      "iteration 8904, loss: 0.00379013828933239\n",
      "iteration 8905, loss: 0.0032547288574278355\n",
      "iteration 8906, loss: 0.0031457971781492233\n",
      "iteration 8907, loss: 0.003250929992645979\n",
      "iteration 8908, loss: 0.0030495550017803907\n",
      "iteration 8909, loss: 0.00302308052778244\n",
      "iteration 8910, loss: 0.003446855116635561\n",
      "iteration 8911, loss: 0.003608786268159747\n",
      "iteration 8912, loss: 0.0037064473144710064\n",
      "iteration 8913, loss: 0.003258615732192993\n",
      "iteration 8914, loss: 0.003419203218072653\n",
      "iteration 8915, loss: 0.003241285216063261\n",
      "iteration 8916, loss: 0.003435659222304821\n",
      "iteration 8917, loss: 0.003956454806029797\n",
      "iteration 8918, loss: 0.003973846323788166\n",
      "iteration 8919, loss: 0.003353287698701024\n",
      "iteration 8920, loss: 0.0029275468550622463\n",
      "iteration 8921, loss: 0.0032258592545986176\n",
      "iteration 8922, loss: 0.0029668202623724937\n",
      "iteration 8923, loss: 0.003171927062794566\n",
      "iteration 8924, loss: 0.003911119885742664\n",
      "iteration 8925, loss: 0.003512405091896653\n",
      "iteration 8926, loss: 0.0037768413312733173\n",
      "iteration 8927, loss: 0.0037090377882122993\n",
      "iteration 8928, loss: 0.0035360075999051332\n",
      "iteration 8929, loss: 0.0036617631558328867\n",
      "iteration 8930, loss: 0.0037385078612715006\n",
      "iteration 8931, loss: 0.003594778710976243\n",
      "iteration 8932, loss: 0.0036139250732958317\n",
      "iteration 8933, loss: 0.0031350187491625547\n",
      "iteration 8934, loss: 0.0035142963752150536\n",
      "iteration 8935, loss: 0.004153856076300144\n",
      "iteration 8936, loss: 0.0038529508747160435\n",
      "iteration 8937, loss: 0.0035606068558990955\n",
      "iteration 8938, loss: 0.002856879960745573\n",
      "iteration 8939, loss: 0.003573889611288905\n",
      "iteration 8940, loss: 0.0036293589510023594\n",
      "iteration 8941, loss: 0.0041206893511116505\n",
      "iteration 8942, loss: 0.0034723368007689714\n",
      "iteration 8943, loss: 0.003214270109310746\n",
      "iteration 8944, loss: 0.0033152508549392223\n",
      "iteration 8945, loss: 0.0038868708070367575\n",
      "iteration 8946, loss: 0.003289888147264719\n",
      "iteration 8947, loss: 0.0034837243147194386\n",
      "iteration 8948, loss: 0.0032473006285727024\n",
      "iteration 8949, loss: 0.0033268535044044256\n",
      "iteration 8950, loss: 0.0031707212328910828\n",
      "iteration 8951, loss: 0.004031924065202475\n",
      "iteration 8952, loss: 0.003813244868069887\n",
      "iteration 8953, loss: 0.003431192133575678\n",
      "iteration 8954, loss: 0.003162966575473547\n",
      "iteration 8955, loss: 0.003431924618780613\n",
      "iteration 8956, loss: 0.0034557408653199673\n",
      "iteration 8957, loss: 0.00358587340451777\n",
      "iteration 8958, loss: 0.003179308259859681\n",
      "iteration 8959, loss: 0.0027179024182260036\n",
      "iteration 8960, loss: 0.0028575407341122627\n",
      "iteration 8961, loss: 0.0030602761544287205\n",
      "iteration 8962, loss: 0.0030952184461057186\n",
      "iteration 8963, loss: 0.003095718100667\n",
      "iteration 8964, loss: 0.003462419845163822\n",
      "iteration 8965, loss: 0.0037163798697292805\n",
      "iteration 8966, loss: 0.003023047000169754\n",
      "iteration 8967, loss: 0.003320694901049137\n",
      "iteration 8968, loss: 0.003616865025833249\n",
      "iteration 8969, loss: 0.0035119112581014633\n",
      "iteration 8970, loss: 0.003976764623075724\n",
      "iteration 8971, loss: 0.0037618691567331553\n",
      "iteration 8972, loss: 0.0038648650515824556\n",
      "iteration 8973, loss: 0.003724199254065752\n",
      "iteration 8974, loss: 0.002927688416093588\n",
      "iteration 8975, loss: 0.0032506189309060574\n",
      "iteration 8976, loss: 0.0026557703968137503\n",
      "iteration 8977, loss: 0.0034146164543926716\n",
      "iteration 8978, loss: 0.003424876369535923\n",
      "iteration 8979, loss: 0.0040292697958648205\n",
      "iteration 8980, loss: 0.003170734504237771\n",
      "iteration 8981, loss: 0.0036632088012993336\n",
      "iteration 8982, loss: 0.003380376845598221\n",
      "iteration 8983, loss: 0.002914693672209978\n",
      "iteration 8984, loss: 0.0036366134881973267\n",
      "iteration 8985, loss: 0.003470496740192175\n",
      "iteration 8986, loss: 0.003979217726737261\n",
      "iteration 8987, loss: 0.003566211322322488\n",
      "iteration 8988, loss: 0.00296748336404562\n",
      "iteration 8989, loss: 0.0032038665376603603\n",
      "iteration 8990, loss: 0.003502077190205455\n",
      "iteration 8991, loss: 0.0031819853466004133\n",
      "iteration 8992, loss: 0.0030871424823999405\n",
      "iteration 8993, loss: 0.0034265760332345963\n",
      "iteration 8994, loss: 0.0035503837279975414\n",
      "iteration 8995, loss: 0.003128558862954378\n",
      "iteration 8996, loss: 0.0025833500549197197\n",
      "iteration 8997, loss: 0.003494644071906805\n",
      "iteration 8998, loss: 0.003174247918650508\n",
      "iteration 8999, loss: 0.0035980562679469585\n",
      "iteration 9000, loss: 0.00349115626886487\n",
      "iteration 9001, loss: 0.0030874896328896284\n",
      "iteration 9002, loss: 0.00434844009578228\n",
      "iteration 9003, loss: 0.0034137628972530365\n",
      "iteration 9004, loss: 0.0035424837842583656\n",
      "iteration 9005, loss: 0.003914244472980499\n",
      "iteration 9006, loss: 0.003557551419362426\n",
      "iteration 9007, loss: 0.003177414182573557\n",
      "iteration 9008, loss: 0.003081939648836851\n",
      "iteration 9009, loss: 0.003542844671756029\n",
      "iteration 9010, loss: 0.0029429891146719456\n",
      "iteration 9011, loss: 0.003224363084882498\n",
      "iteration 9012, loss: 0.0032744191121309996\n",
      "iteration 9013, loss: 0.003864568192511797\n",
      "iteration 9014, loss: 0.003072898369282484\n",
      "iteration 9015, loss: 0.00296721700578928\n",
      "iteration 9016, loss: 0.002705439692363143\n",
      "iteration 9017, loss: 0.0029321371112018824\n",
      "iteration 9018, loss: 0.0036090838257223368\n",
      "iteration 9019, loss: 0.0030063388403505087\n",
      "iteration 9020, loss: 0.0032739639282226562\n",
      "iteration 9021, loss: 0.0029898611828684807\n",
      "iteration 9022, loss: 0.0028414628468453884\n",
      "iteration 9023, loss: 0.003366274293512106\n",
      "iteration 9024, loss: 0.002830319106578827\n",
      "iteration 9025, loss: 0.0035534282214939594\n",
      "iteration 9026, loss: 0.0031303190626204014\n",
      "iteration 9027, loss: 0.0026266633067280054\n",
      "iteration 9028, loss: 0.0036003771238029003\n",
      "iteration 9029, loss: 0.0026551797054708004\n",
      "iteration 9030, loss: 0.0031204496044665575\n",
      "iteration 9031, loss: 0.003458950435742736\n",
      "iteration 9032, loss: 0.0031602594535797834\n",
      "iteration 9033, loss: 0.0030004805885255337\n",
      "iteration 9034, loss: 0.0036221169866621494\n",
      "iteration 9035, loss: 0.0034242768306285143\n",
      "iteration 9036, loss: 0.0033852485939860344\n",
      "iteration 9037, loss: 0.002910908544436097\n",
      "iteration 9038, loss: 0.0027389847673475742\n",
      "iteration 9039, loss: 0.00375939579680562\n",
      "iteration 9040, loss: 0.0034126960672438145\n",
      "iteration 9041, loss: 0.002909022383391857\n",
      "iteration 9042, loss: 0.003475738223642111\n",
      "iteration 9043, loss: 0.0036709755659103394\n",
      "iteration 9044, loss: 0.002977299503982067\n",
      "iteration 9045, loss: 0.0034225117415189743\n",
      "iteration 9046, loss: 0.0037631597369909286\n",
      "iteration 9047, loss: 0.0029799004551023245\n",
      "iteration 9048, loss: 0.003467623610049486\n",
      "iteration 9049, loss: 0.0040901824831962585\n",
      "iteration 9050, loss: 0.0030418920796364546\n",
      "iteration 9051, loss: 0.00349593092687428\n",
      "iteration 9052, loss: 0.0026198923587799072\n",
      "iteration 9053, loss: 0.003341028932482004\n",
      "iteration 9054, loss: 0.0026101861149072647\n",
      "iteration 9055, loss: 0.003611281281337142\n",
      "iteration 9056, loss: 0.003060506656765938\n",
      "iteration 9057, loss: 0.0029172045178711414\n",
      "iteration 9058, loss: 0.003445496316999197\n",
      "iteration 9059, loss: 0.0032162610441446304\n",
      "iteration 9060, loss: 0.003276378847658634\n",
      "iteration 9061, loss: 0.0036345701664686203\n",
      "iteration 9062, loss: 0.003222182858735323\n",
      "iteration 9063, loss: 0.003957725130021572\n",
      "iteration 9064, loss: 0.0036041808780282736\n",
      "iteration 9065, loss: 0.003955889493227005\n",
      "iteration 9066, loss: 0.0037413022946566343\n",
      "iteration 9067, loss: 0.0034341434948146343\n",
      "iteration 9068, loss: 0.003690688405185938\n",
      "iteration 9069, loss: 0.002897961065173149\n",
      "iteration 9070, loss: 0.003957333043217659\n",
      "iteration 9071, loss: 0.0038495163898915052\n",
      "iteration 9072, loss: 0.0031356203835457563\n",
      "iteration 9073, loss: 0.003014966379851103\n",
      "iteration 9074, loss: 0.003560704179108143\n",
      "iteration 9075, loss: 0.003550453344359994\n",
      "iteration 9076, loss: 0.003846776206046343\n",
      "iteration 9077, loss: 0.0031130691058933735\n",
      "iteration 9078, loss: 0.0030608740635216236\n",
      "iteration 9079, loss: 0.002711421810090542\n",
      "iteration 9080, loss: 0.003128492273390293\n",
      "iteration 9081, loss: 0.003624852281063795\n",
      "iteration 9082, loss: 0.0032050032168626785\n",
      "iteration 9083, loss: 0.002732206368818879\n",
      "iteration 9084, loss: 0.0036963066086173058\n",
      "iteration 9085, loss: 0.0029568462632596493\n",
      "iteration 9086, loss: 0.003012108150869608\n",
      "iteration 9087, loss: 0.002857465296983719\n",
      "iteration 9088, loss: 0.002734115347266197\n",
      "iteration 9089, loss: 0.003207552246749401\n",
      "iteration 9090, loss: 0.003218086902052164\n",
      "iteration 9091, loss: 0.0027744662947952747\n",
      "iteration 9092, loss: 0.00304154260084033\n",
      "iteration 9093, loss: 0.0032404838129878044\n",
      "iteration 9094, loss: 0.0024679163470864296\n",
      "iteration 9095, loss: 0.003198503516614437\n",
      "iteration 9096, loss: 0.002803218550980091\n",
      "iteration 9097, loss: 0.002871497068554163\n",
      "iteration 9098, loss: 0.0029660759028047323\n",
      "iteration 9099, loss: 0.0028594075702130795\n",
      "iteration 9100, loss: 0.003116467734798789\n",
      "iteration 9101, loss: 0.003349726554006338\n",
      "iteration 9102, loss: 0.002998911775648594\n",
      "iteration 9103, loss: 0.0035915246699005365\n",
      "iteration 9104, loss: 0.002720802091062069\n",
      "iteration 9105, loss: 0.0033202734775841236\n",
      "iteration 9106, loss: 0.003292408771812916\n",
      "iteration 9107, loss: 0.003189906943589449\n",
      "iteration 9108, loss: 0.0027595844585448503\n",
      "iteration 9109, loss: 0.002846300136297941\n",
      "iteration 9110, loss: 0.003203683765605092\n",
      "iteration 9111, loss: 0.0031639081425964832\n",
      "iteration 9112, loss: 0.0033003310672938824\n",
      "iteration 9113, loss: 0.0032672886736691\n",
      "iteration 9114, loss: 0.003166983835399151\n",
      "iteration 9115, loss: 0.0033195491414517164\n",
      "iteration 9116, loss: 0.0034077377058565617\n",
      "iteration 9117, loss: 0.002888668794184923\n",
      "iteration 9118, loss: 0.0035983817651867867\n",
      "iteration 9119, loss: 0.0033150375820696354\n",
      "iteration 9120, loss: 0.0032498305663466454\n",
      "iteration 9121, loss: 0.0029641087166965008\n",
      "iteration 9122, loss: 0.0031413710676133633\n",
      "iteration 9123, loss: 0.0032289675436913967\n",
      "iteration 9124, loss: 0.0032114009372889996\n",
      "iteration 9125, loss: 0.0031690383329987526\n",
      "iteration 9126, loss: 0.003092366736382246\n",
      "iteration 9127, loss: 0.0029037855565547943\n",
      "iteration 9128, loss: 0.00332986144348979\n",
      "iteration 9129, loss: 0.0029431194998323917\n",
      "iteration 9130, loss: 0.003405994502827525\n",
      "iteration 9131, loss: 0.00316154258325696\n",
      "iteration 9132, loss: 0.0033083695452660322\n",
      "iteration 9133, loss: 0.0028616986237466335\n",
      "iteration 9134, loss: 0.003268531756475568\n",
      "iteration 9135, loss: 0.0028713149949908257\n",
      "iteration 9136, loss: 0.00253515993244946\n",
      "iteration 9137, loss: 0.003053566673770547\n",
      "iteration 9138, loss: 0.003448712406679988\n",
      "iteration 9139, loss: 0.0032461511436849833\n",
      "iteration 9140, loss: 0.00294396816752851\n",
      "iteration 9141, loss: 0.0037651401944458485\n",
      "iteration 9142, loss: 0.002987015526741743\n",
      "iteration 9143, loss: 0.00297848554328084\n",
      "iteration 9144, loss: 0.003391302889212966\n",
      "iteration 9145, loss: 0.003380967304110527\n",
      "iteration 9146, loss: 0.0036510764621198177\n",
      "iteration 9147, loss: 0.003109995275735855\n",
      "iteration 9148, loss: 0.003001120872795582\n",
      "iteration 9149, loss: 0.002958756871521473\n",
      "iteration 9150, loss: 0.003021943848580122\n",
      "iteration 9151, loss: 0.0028940830379724503\n",
      "iteration 9152, loss: 0.003090799553319812\n",
      "iteration 9153, loss: 0.004104582592844963\n",
      "iteration 9154, loss: 0.0031962599605321884\n",
      "iteration 9155, loss: 0.0029452296439558268\n",
      "iteration 9156, loss: 0.0034154169261455536\n",
      "iteration 9157, loss: 0.002982855774462223\n",
      "iteration 9158, loss: 0.0026030614972114563\n",
      "iteration 9159, loss: 0.003068450838327408\n",
      "iteration 9160, loss: 0.0030020475387573242\n",
      "iteration 9161, loss: 0.003091098740696907\n",
      "iteration 9162, loss: 0.003217204473912716\n",
      "iteration 9163, loss: 0.0035256370902061462\n",
      "iteration 9164, loss: 0.003122792113572359\n",
      "iteration 9165, loss: 0.0035220100544393063\n",
      "iteration 9166, loss: 0.0030589380767196417\n",
      "iteration 9167, loss: 0.003887548577040434\n",
      "iteration 9168, loss: 0.003306714352220297\n",
      "iteration 9169, loss: 0.0033504310995340347\n",
      "iteration 9170, loss: 0.0032559800893068314\n",
      "iteration 9171, loss: 0.00335961882956326\n",
      "iteration 9172, loss: 0.0031811140943318605\n",
      "iteration 9173, loss: 0.0028448631055653095\n",
      "iteration 9174, loss: 0.003372421022504568\n",
      "iteration 9175, loss: 0.003975040279328823\n",
      "iteration 9176, loss: 0.0035514812916517258\n",
      "iteration 9177, loss: 0.00374583899974823\n",
      "iteration 9178, loss: 0.0041257222183048725\n",
      "iteration 9179, loss: 0.002900740597397089\n",
      "iteration 9180, loss: 0.0031483066268265247\n",
      "iteration 9181, loss: 0.0037231463938951492\n",
      "iteration 9182, loss: 0.0029985764995217323\n",
      "iteration 9183, loss: 0.0031729470938444138\n",
      "iteration 9184, loss: 0.003674057312309742\n",
      "iteration 9185, loss: 0.0036003664135932922\n",
      "iteration 9186, loss: 0.0031186803244054317\n",
      "iteration 9187, loss: 0.0032917228527367115\n",
      "iteration 9188, loss: 0.0029867812991142273\n",
      "iteration 9189, loss: 0.0038301933091133833\n",
      "iteration 9190, loss: 0.003397631226107478\n",
      "iteration 9191, loss: 0.003631402738392353\n",
      "iteration 9192, loss: 0.003295247908681631\n",
      "iteration 9193, loss: 0.003871134016662836\n",
      "iteration 9194, loss: 0.0026209871284663677\n",
      "iteration 9195, loss: 0.00296286353841424\n",
      "iteration 9196, loss: 0.003555078525096178\n",
      "iteration 9197, loss: 0.0027656941674649715\n",
      "iteration 9198, loss: 0.0029318397864699364\n",
      "iteration 9199, loss: 0.0028291670605540276\n",
      "iteration 9200, loss: 0.0029511130414903164\n",
      "iteration 9201, loss: 0.003478399943560362\n",
      "iteration 9202, loss: 0.004023060202598572\n",
      "iteration 9203, loss: 0.00334837450645864\n",
      "iteration 9204, loss: 0.003723735688254237\n",
      "iteration 9205, loss: 0.003914909902960062\n",
      "iteration 9206, loss: 0.003187483176589012\n",
      "iteration 9207, loss: 0.0038814228028059006\n",
      "iteration 9208, loss: 0.003297759685665369\n",
      "iteration 9209, loss: 0.0030636033043265343\n",
      "iteration 9210, loss: 0.00323777599260211\n",
      "iteration 9211, loss: 0.0031386888585984707\n",
      "iteration 9212, loss: 0.0037340782582759857\n",
      "iteration 9213, loss: 0.003202777588739991\n",
      "iteration 9214, loss: 0.003242526203393936\n",
      "iteration 9215, loss: 0.0031838349532335997\n",
      "iteration 9216, loss: 0.002997520612552762\n",
      "iteration 9217, loss: 0.0028938441537320614\n",
      "iteration 9218, loss: 0.003443199209868908\n",
      "iteration 9219, loss: 0.002973228693008423\n",
      "iteration 9220, loss: 0.003774478333070874\n",
      "iteration 9221, loss: 0.0028465099167078733\n",
      "iteration 9222, loss: 0.003898646915331483\n",
      "iteration 9223, loss: 0.003000219352543354\n",
      "iteration 9224, loss: 0.0031112143769860268\n",
      "iteration 9225, loss: 0.0026055490598082542\n",
      "iteration 9226, loss: 0.0035834326408803463\n",
      "iteration 9227, loss: 0.0030115668196231127\n",
      "iteration 9228, loss: 0.002830262528732419\n",
      "iteration 9229, loss: 0.002453476656228304\n",
      "iteration 9230, loss: 0.003249088302254677\n",
      "iteration 9231, loss: 0.003020521253347397\n",
      "iteration 9232, loss: 0.003313208231702447\n",
      "iteration 9233, loss: 0.0025948642287403345\n",
      "iteration 9234, loss: 0.0033875005319714546\n",
      "iteration 9235, loss: 0.0034465475473552942\n",
      "iteration 9236, loss: 0.0030987956561148167\n",
      "iteration 9237, loss: 0.004252669867128134\n",
      "iteration 9238, loss: 0.0032523926347494125\n",
      "iteration 9239, loss: 0.003323232289403677\n",
      "iteration 9240, loss: 0.0037459093146026134\n",
      "iteration 9241, loss: 0.003204264910891652\n",
      "iteration 9242, loss: 0.0029158424586057663\n",
      "iteration 9243, loss: 0.0031171285081654787\n",
      "iteration 9244, loss: 0.003991252742707729\n",
      "iteration 9245, loss: 0.0032623866572976112\n",
      "iteration 9246, loss: 0.0029474373441189528\n",
      "iteration 9247, loss: 0.003712163306772709\n",
      "iteration 9248, loss: 0.0036099348217248917\n",
      "iteration 9249, loss: 0.0029355385340750217\n",
      "iteration 9250, loss: 0.0032668132334947586\n",
      "iteration 9251, loss: 0.003668139223009348\n",
      "iteration 9252, loss: 0.002889720257371664\n",
      "iteration 9253, loss: 0.0030856868252158165\n",
      "iteration 9254, loss: 0.0035691724624484777\n",
      "iteration 9255, loss: 0.003093327395617962\n",
      "iteration 9256, loss: 0.003600964555516839\n",
      "iteration 9257, loss: 0.0036579486913979053\n",
      "iteration 9258, loss: 0.0034402788151055574\n",
      "iteration 9259, loss: 0.0033927385229617357\n",
      "iteration 9260, loss: 0.0027628724928945303\n",
      "iteration 9261, loss: 0.0024977221619337797\n",
      "iteration 9262, loss: 0.003122310619801283\n",
      "iteration 9263, loss: 0.0033145355992019176\n",
      "iteration 9264, loss: 0.002703204518184066\n",
      "iteration 9265, loss: 0.003989347722381353\n",
      "iteration 9266, loss: 0.002958399709314108\n",
      "iteration 9267, loss: 0.0030870395712554455\n",
      "iteration 9268, loss: 0.00410676933825016\n",
      "iteration 9269, loss: 0.0029409117996692657\n",
      "iteration 9270, loss: 0.0031436497811228037\n",
      "iteration 9271, loss: 0.003372778417542577\n",
      "iteration 9272, loss: 0.0033129979856312275\n",
      "iteration 9273, loss: 0.0032487076241523027\n",
      "iteration 9274, loss: 0.003494633361697197\n",
      "iteration 9275, loss: 0.0034911867696791887\n",
      "iteration 9276, loss: 0.0027608717791736126\n",
      "iteration 9277, loss: 0.003431718796491623\n",
      "iteration 9278, loss: 0.0029549100436270237\n",
      "iteration 9279, loss: 0.002655233256518841\n",
      "iteration 9280, loss: 0.002520932350307703\n",
      "iteration 9281, loss: 0.002676909090951085\n",
      "iteration 9282, loss: 0.003359487047418952\n",
      "iteration 9283, loss: 0.002893908182159066\n",
      "iteration 9284, loss: 0.0031641956884413958\n",
      "iteration 9285, loss: 0.002929026260972023\n",
      "iteration 9286, loss: 0.003237561322748661\n",
      "iteration 9287, loss: 0.003076562425121665\n",
      "iteration 9288, loss: 0.0032668050844222307\n",
      "iteration 9289, loss: 0.0034608112182468176\n",
      "iteration 9290, loss: 0.003214634256437421\n",
      "iteration 9291, loss: 0.0028545407112687826\n",
      "iteration 9292, loss: 0.0035639568231999874\n",
      "iteration 9293, loss: 0.002858766121789813\n",
      "iteration 9294, loss: 0.0029986861627548933\n",
      "iteration 9295, loss: 0.0031051822006702423\n",
      "iteration 9296, loss: 0.002927177818492055\n",
      "iteration 9297, loss: 0.0028218189254403114\n",
      "iteration 9298, loss: 0.0030296738259494305\n",
      "iteration 9299, loss: 0.0032126647420227528\n",
      "iteration 9300, loss: 0.0026104473508894444\n",
      "iteration 9301, loss: 0.0027763494290411472\n",
      "iteration 9302, loss: 0.002466573379933834\n",
      "iteration 9303, loss: 0.0034358857665210962\n",
      "iteration 9304, loss: 0.0027642366476356983\n",
      "iteration 9305, loss: 0.0034559385385364294\n",
      "iteration 9306, loss: 0.002828048076480627\n",
      "iteration 9307, loss: 0.0029278909787535667\n",
      "iteration 9308, loss: 0.0030091777443885803\n",
      "iteration 9309, loss: 0.002642539795488119\n",
      "iteration 9310, loss: 0.0025671455077826977\n",
      "iteration 9311, loss: 0.0034207154531031847\n",
      "iteration 9312, loss: 0.0027336138300597668\n",
      "iteration 9313, loss: 0.0031683165580034256\n",
      "iteration 9314, loss: 0.0034221031237393618\n",
      "iteration 9315, loss: 0.0027664033696055412\n",
      "iteration 9316, loss: 0.002972701098769903\n",
      "iteration 9317, loss: 0.0030255294404923916\n",
      "iteration 9318, loss: 0.0025894565042108297\n",
      "iteration 9319, loss: 0.003379252040758729\n",
      "iteration 9320, loss: 0.0028856731951236725\n",
      "iteration 9321, loss: 0.003459192579612136\n",
      "iteration 9322, loss: 0.0035961552057415247\n",
      "iteration 9323, loss: 0.0033957131672650576\n",
      "iteration 9324, loss: 0.0031238491646945477\n",
      "iteration 9325, loss: 0.003058966249227524\n",
      "iteration 9326, loss: 0.003413309808820486\n",
      "iteration 9327, loss: 0.002952516544610262\n",
      "iteration 9328, loss: 0.0033691949211061\n",
      "iteration 9329, loss: 0.003572955261915922\n",
      "iteration 9330, loss: 0.002837755484506488\n",
      "iteration 9331, loss: 0.003171838354319334\n",
      "iteration 9332, loss: 0.0029345147777348757\n",
      "iteration 9333, loss: 0.003807211294770241\n",
      "iteration 9334, loss: 0.002965301275253296\n",
      "iteration 9335, loss: 0.0030347579158842564\n",
      "iteration 9336, loss: 0.002954701194539666\n",
      "iteration 9337, loss: 0.0032660842407494783\n",
      "iteration 9338, loss: 0.0034916407894343138\n",
      "iteration 9339, loss: 0.0030787368305027485\n",
      "iteration 9340, loss: 0.004168281797319651\n",
      "iteration 9341, loss: 0.0033750550355762243\n",
      "iteration 9342, loss: 0.0034979397896677256\n",
      "iteration 9343, loss: 0.0030399905517697334\n",
      "iteration 9344, loss: 0.0038515711203217506\n",
      "iteration 9345, loss: 0.0030685467645525932\n",
      "iteration 9346, loss: 0.0029311804100871086\n",
      "iteration 9347, loss: 0.00283880066126585\n",
      "iteration 9348, loss: 0.003493014257401228\n",
      "iteration 9349, loss: 0.0031753312796354294\n",
      "iteration 9350, loss: 0.002824694849550724\n",
      "iteration 9351, loss: 0.0031091636046767235\n",
      "iteration 9352, loss: 0.003279812168329954\n",
      "iteration 9353, loss: 0.002274110447615385\n",
      "iteration 9354, loss: 0.0027806274592876434\n",
      "iteration 9355, loss: 0.003194165416061878\n",
      "iteration 9356, loss: 0.003101858077570796\n",
      "iteration 9357, loss: 0.0030720809008926153\n",
      "iteration 9358, loss: 0.002482128795236349\n",
      "iteration 9359, loss: 0.0029202685691416264\n",
      "iteration 9360, loss: 0.0024397766683250666\n",
      "iteration 9361, loss: 0.0030617776792496443\n",
      "iteration 9362, loss: 0.0028669782914221287\n",
      "iteration 9363, loss: 0.003049684688448906\n",
      "iteration 9364, loss: 0.0030646505765616894\n",
      "iteration 9365, loss: 0.0029343008063733578\n",
      "iteration 9366, loss: 0.003731864271685481\n",
      "iteration 9367, loss: 0.0024011442437767982\n",
      "iteration 9368, loss: 0.0034442974720150232\n",
      "iteration 9369, loss: 0.0029067015275359154\n",
      "iteration 9370, loss: 0.0032332418486475945\n",
      "iteration 9371, loss: 0.003095815423876047\n",
      "iteration 9372, loss: 0.003376849927008152\n",
      "iteration 9373, loss: 0.003322307951748371\n",
      "iteration 9374, loss: 0.0031405636109411716\n",
      "iteration 9375, loss: 0.003049129620194435\n",
      "iteration 9376, loss: 0.0030612098053097725\n",
      "iteration 9377, loss: 0.0028397575952112675\n",
      "iteration 9378, loss: 0.002987693529576063\n",
      "iteration 9379, loss: 0.0027528414502739906\n",
      "iteration 9380, loss: 0.0033417989034205675\n",
      "iteration 9381, loss: 0.002964714774861932\n",
      "iteration 9382, loss: 0.002895804587751627\n",
      "iteration 9383, loss: 0.0033629583194851875\n",
      "iteration 9384, loss: 0.0030119512230157852\n",
      "iteration 9385, loss: 0.003312331158667803\n",
      "iteration 9386, loss: 0.0033987697679549456\n",
      "iteration 9387, loss: 0.0028714158106595278\n",
      "iteration 9388, loss: 0.0031256412621587515\n",
      "iteration 9389, loss: 0.0028962302021682262\n",
      "iteration 9390, loss: 0.0031625248957425356\n",
      "iteration 9391, loss: 0.003012796863913536\n",
      "iteration 9392, loss: 0.003255716757848859\n",
      "iteration 9393, loss: 0.002628883346915245\n",
      "iteration 9394, loss: 0.003489239141345024\n",
      "iteration 9395, loss: 0.002922173123806715\n",
      "iteration 9396, loss: 0.002729746513068676\n",
      "iteration 9397, loss: 0.002587669063359499\n",
      "iteration 9398, loss: 0.003465430811047554\n",
      "iteration 9399, loss: 0.0031320173293352127\n",
      "iteration 9400, loss: 0.0036154300905764103\n",
      "iteration 9401, loss: 0.003653363324701786\n",
      "iteration 9402, loss: 0.0029073869809508324\n",
      "iteration 9403, loss: 0.0032804454676806927\n",
      "iteration 9404, loss: 0.0034903085324913263\n",
      "iteration 9405, loss: 0.0028273460920900106\n",
      "iteration 9406, loss: 0.003205211367458105\n",
      "iteration 9407, loss: 0.003432803088799119\n",
      "iteration 9408, loss: 0.002861510030925274\n",
      "iteration 9409, loss: 0.002975259907543659\n",
      "iteration 9410, loss: 0.003425188362598419\n",
      "iteration 9411, loss: 0.0029950272291898727\n",
      "iteration 9412, loss: 0.003346056677401066\n",
      "iteration 9413, loss: 0.0026766154915094376\n",
      "iteration 9414, loss: 0.0030772732570767403\n",
      "iteration 9415, loss: 0.0038038170896470547\n",
      "iteration 9416, loss: 0.002820533001795411\n",
      "iteration 9417, loss: 0.0028110172133892775\n",
      "iteration 9418, loss: 0.0031847930513322353\n",
      "iteration 9419, loss: 0.002866039052605629\n",
      "iteration 9420, loss: 0.0035829797852784395\n",
      "iteration 9421, loss: 0.003271518275141716\n",
      "iteration 9422, loss: 0.003281882032752037\n",
      "iteration 9423, loss: 0.002918784972280264\n",
      "iteration 9424, loss: 0.003314022207632661\n",
      "iteration 9425, loss: 0.0034613367170095444\n",
      "iteration 9426, loss: 0.0028212436009198427\n",
      "iteration 9427, loss: 0.0036983750760555267\n",
      "iteration 9428, loss: 0.0034044920466840267\n",
      "iteration 9429, loss: 0.0039357272908091545\n",
      "iteration 9430, loss: 0.004095122218132019\n",
      "iteration 9431, loss: 0.002729116240516305\n",
      "iteration 9432, loss: 0.002661050297319889\n",
      "iteration 9433, loss: 0.0028471616096794605\n",
      "iteration 9434, loss: 0.0032784794457256794\n",
      "iteration 9435, loss: 0.003844437887892127\n",
      "iteration 9436, loss: 0.0035071137826889753\n",
      "iteration 9437, loss: 0.0035750933457165956\n",
      "iteration 9438, loss: 0.0029118265956640244\n",
      "iteration 9439, loss: 0.002726786071434617\n",
      "iteration 9440, loss: 0.003253502771258354\n",
      "iteration 9441, loss: 0.0031065631192177534\n",
      "iteration 9442, loss: 0.0031167331617325544\n",
      "iteration 9443, loss: 0.002846031915396452\n",
      "iteration 9444, loss: 0.003355985274538398\n",
      "iteration 9445, loss: 0.003158695762977004\n",
      "iteration 9446, loss: 0.0028980073984712362\n",
      "iteration 9447, loss: 0.0029529016464948654\n",
      "iteration 9448, loss: 0.00327056716196239\n",
      "iteration 9449, loss: 0.0024707966949790716\n",
      "iteration 9450, loss: 0.003096039406955242\n",
      "iteration 9451, loss: 0.0034066149964928627\n",
      "iteration 9452, loss: 0.003096052911132574\n",
      "iteration 9453, loss: 0.0029187726322561502\n",
      "iteration 9454, loss: 0.0028928196988999844\n",
      "iteration 9455, loss: 0.003873625071719289\n",
      "iteration 9456, loss: 0.003275626804679632\n",
      "iteration 9457, loss: 0.003165853675454855\n",
      "iteration 9458, loss: 0.0029070903547108173\n",
      "iteration 9459, loss: 0.0032016984187066555\n",
      "iteration 9460, loss: 0.003712674370035529\n",
      "iteration 9461, loss: 0.002894999925047159\n",
      "iteration 9462, loss: 0.0033890630584210157\n",
      "iteration 9463, loss: 0.0030878549441695213\n",
      "iteration 9464, loss: 0.0037260865792632103\n",
      "iteration 9465, loss: 0.003524093423038721\n",
      "iteration 9466, loss: 0.0031872515100985765\n",
      "iteration 9467, loss: 0.0026759342290461063\n",
      "iteration 9468, loss: 0.0028775720857083797\n",
      "iteration 9469, loss: 0.0025884241331368685\n",
      "iteration 9470, loss: 0.003782673040404916\n",
      "iteration 9471, loss: 0.0031202996615320444\n",
      "iteration 9472, loss: 0.0031870612874627113\n",
      "iteration 9473, loss: 0.002864185255020857\n",
      "iteration 9474, loss: 0.0033241421915590763\n",
      "iteration 9475, loss: 0.0029138089157640934\n",
      "iteration 9476, loss: 0.003229002933949232\n",
      "iteration 9477, loss: 0.003173230914399028\n",
      "iteration 9478, loss: 0.002522233873605728\n",
      "iteration 9479, loss: 0.0032684209290891886\n",
      "iteration 9480, loss: 0.0024130712263286114\n",
      "iteration 9481, loss: 0.0029206397011876106\n",
      "iteration 9482, loss: 0.003058185800909996\n",
      "iteration 9483, loss: 0.0032186175230890512\n",
      "iteration 9484, loss: 0.003670420963317156\n",
      "iteration 9485, loss: 0.0035086784046143293\n",
      "iteration 9486, loss: 0.0024927002377808094\n",
      "iteration 9487, loss: 0.0035571143962442875\n",
      "iteration 9488, loss: 0.002642870880663395\n",
      "iteration 9489, loss: 0.003383681643754244\n",
      "iteration 9490, loss: 0.0026262239553034306\n",
      "iteration 9491, loss: 0.002980749122798443\n",
      "iteration 9492, loss: 0.0028122272342443466\n",
      "iteration 9493, loss: 0.003208350855857134\n",
      "iteration 9494, loss: 0.0030165293719619513\n",
      "iteration 9495, loss: 0.0025903047062456608\n",
      "iteration 9496, loss: 0.002700288314372301\n",
      "iteration 9497, loss: 0.0032055899500846863\n",
      "iteration 9498, loss: 0.0033180543687194586\n",
      "iteration 9499, loss: 0.0030996580608189106\n",
      "iteration 9500, loss: 0.003863639198243618\n",
      "iteration 9501, loss: 0.0033947143238037825\n",
      "iteration 9502, loss: 0.003460322041064501\n",
      "iteration 9503, loss: 0.0035780686885118484\n",
      "iteration 9504, loss: 0.0029308591037988663\n",
      "iteration 9505, loss: 0.0033273515291512012\n",
      "iteration 9506, loss: 0.00342166842892766\n",
      "iteration 9507, loss: 0.0033119518775492907\n",
      "iteration 9508, loss: 0.003075494198128581\n",
      "iteration 9509, loss: 0.0030127749778330326\n",
      "iteration 9510, loss: 0.0029215612448751926\n",
      "iteration 9511, loss: 0.0035334331914782524\n",
      "iteration 9512, loss: 0.003014281392097473\n",
      "iteration 9513, loss: 0.0032431557774543762\n",
      "iteration 9514, loss: 0.0036997210700064898\n",
      "iteration 9515, loss: 0.0033106724731624126\n",
      "iteration 9516, loss: 0.0031190281733870506\n",
      "iteration 9517, loss: 0.003307740204036236\n",
      "iteration 9518, loss: 0.002453516237437725\n",
      "iteration 9519, loss: 0.002638538833707571\n",
      "iteration 9520, loss: 0.003142023691907525\n",
      "iteration 9521, loss: 0.0030523892492055893\n",
      "iteration 9522, loss: 0.003374477382749319\n",
      "iteration 9523, loss: 0.003156064311042428\n",
      "iteration 9524, loss: 0.00342019135132432\n",
      "iteration 9525, loss: 0.002972529735416174\n",
      "iteration 9526, loss: 0.0028421280439943075\n",
      "iteration 9527, loss: 0.003175274468958378\n",
      "iteration 9528, loss: 0.002988007850944996\n",
      "iteration 9529, loss: 0.0032687033526599407\n",
      "iteration 9530, loss: 0.0026397141627967358\n",
      "iteration 9531, loss: 0.002729109488427639\n",
      "iteration 9532, loss: 0.003239668672904372\n",
      "iteration 9533, loss: 0.003367555793374777\n",
      "iteration 9534, loss: 0.0024997785221785307\n",
      "iteration 9535, loss: 0.003027119440957904\n",
      "iteration 9536, loss: 0.002828299067914486\n",
      "iteration 9537, loss: 0.002944330917671323\n",
      "iteration 9538, loss: 0.0032933892216533422\n",
      "iteration 9539, loss: 0.002760833129286766\n",
      "iteration 9540, loss: 0.00324472738429904\n",
      "iteration 9541, loss: 0.0032057571224868298\n",
      "iteration 9542, loss: 0.00304081616923213\n",
      "iteration 9543, loss: 0.003140239743515849\n",
      "iteration 9544, loss: 0.002557906322181225\n",
      "iteration 9545, loss: 0.0031502172350883484\n",
      "iteration 9546, loss: 0.00280084740370512\n",
      "iteration 9547, loss: 0.0028174519538879395\n",
      "iteration 9548, loss: 0.003648159559816122\n",
      "iteration 9549, loss: 0.0027800691314041615\n",
      "iteration 9550, loss: 0.0033454247750341892\n",
      "iteration 9551, loss: 0.0027536158449947834\n",
      "iteration 9552, loss: 0.003071411047130823\n",
      "iteration 9553, loss: 0.002835486549884081\n",
      "iteration 9554, loss: 0.003785855369642377\n",
      "iteration 9555, loss: 0.0033573275431990623\n",
      "iteration 9556, loss: 0.004084108397364616\n",
      "iteration 9557, loss: 0.003112809732556343\n",
      "iteration 9558, loss: 0.003076839493587613\n",
      "iteration 9559, loss: 0.0036343014799058437\n",
      "iteration 9560, loss: 0.0031970550771802664\n",
      "iteration 9561, loss: 0.002970166504383087\n",
      "iteration 9562, loss: 0.003961647395044565\n",
      "iteration 9563, loss: 0.0030954598914831877\n",
      "iteration 9564, loss: 0.003381247166544199\n",
      "iteration 9565, loss: 0.0025708130560815334\n",
      "iteration 9566, loss: 0.003614999819546938\n",
      "iteration 9567, loss: 0.003304417012259364\n",
      "iteration 9568, loss: 0.0038019847124814987\n",
      "iteration 9569, loss: 0.0035294261761009693\n",
      "iteration 9570, loss: 0.0026232018135488033\n",
      "iteration 9571, loss: 0.0033675432205200195\n",
      "iteration 9572, loss: 0.0028140477370470762\n",
      "iteration 9573, loss: 0.0033237612806260586\n",
      "iteration 9574, loss: 0.00313907815143466\n",
      "iteration 9575, loss: 0.0028713284991681576\n",
      "iteration 9576, loss: 0.00269067264162004\n",
      "iteration 9577, loss: 0.0031798663549125195\n",
      "iteration 9578, loss: 0.003264801111072302\n",
      "iteration 9579, loss: 0.0030549478251487017\n",
      "iteration 9580, loss: 0.0029316043946892023\n",
      "iteration 9581, loss: 0.0032607021275907755\n",
      "iteration 9582, loss: 0.0026138194371014833\n",
      "iteration 9583, loss: 0.0028205292765051126\n",
      "iteration 9584, loss: 0.002919529564678669\n",
      "iteration 9585, loss: 0.0021659708581864834\n",
      "iteration 9586, loss: 0.00327952834777534\n",
      "iteration 9587, loss: 0.0031506745144724846\n",
      "iteration 9588, loss: 0.002673500217497349\n",
      "iteration 9589, loss: 0.003255173098295927\n",
      "iteration 9590, loss: 0.0034019872546195984\n",
      "iteration 9591, loss: 0.0031879248563200235\n",
      "iteration 9592, loss: 0.0032904804684221745\n",
      "iteration 9593, loss: 0.0028880052268505096\n",
      "iteration 9594, loss: 0.0031208579894155264\n",
      "iteration 9595, loss: 0.0027880375273525715\n",
      "iteration 9596, loss: 0.0030826532747596502\n",
      "iteration 9597, loss: 0.0031366762705147266\n",
      "iteration 9598, loss: 0.002818106207996607\n",
      "iteration 9599, loss: 0.0028082765638828278\n",
      "iteration 9600, loss: 0.002885077614337206\n",
      "iteration 9601, loss: 0.003370618913322687\n",
      "iteration 9602, loss: 0.0031802221201360226\n",
      "iteration 9603, loss: 0.0028954867739230394\n",
      "iteration 9604, loss: 0.0031700069084763527\n",
      "iteration 9605, loss: 0.0031944019719958305\n",
      "iteration 9606, loss: 0.003141505178064108\n",
      "iteration 9607, loss: 0.0030964165925979614\n",
      "iteration 9608, loss: 0.003379285801202059\n",
      "iteration 9609, loss: 0.0030638601165264845\n",
      "iteration 9610, loss: 0.0034078070893883705\n",
      "iteration 9611, loss: 0.003177389269694686\n",
      "iteration 9612, loss: 0.0025607042480260134\n",
      "iteration 9613, loss: 0.0037118184845894575\n",
      "iteration 9614, loss: 0.003124846611171961\n",
      "iteration 9615, loss: 0.0028589467983692884\n",
      "iteration 9616, loss: 0.0032666241750121117\n",
      "iteration 9617, loss: 0.0028550513088703156\n",
      "iteration 9618, loss: 0.0025649461895227432\n",
      "iteration 9619, loss: 0.0028008664958178997\n",
      "iteration 9620, loss: 0.0033127418719232082\n",
      "iteration 9621, loss: 0.0032350593246519566\n",
      "iteration 9622, loss: 0.0032618471886962652\n",
      "iteration 9623, loss: 0.0037552188150584698\n",
      "iteration 9624, loss: 0.0027367370203137398\n",
      "iteration 9625, loss: 0.0027405056171119213\n",
      "iteration 9626, loss: 0.0032775411382317543\n",
      "iteration 9627, loss: 0.0026811473071575165\n",
      "iteration 9628, loss: 0.002880997955799103\n",
      "iteration 9629, loss: 0.00344076962210238\n",
      "iteration 9630, loss: 0.002918010577559471\n",
      "iteration 9631, loss: 0.003287993371486664\n",
      "iteration 9632, loss: 0.0034004198387265205\n",
      "iteration 9633, loss: 0.002705180551856756\n",
      "iteration 9634, loss: 0.003289385000243783\n",
      "iteration 9635, loss: 0.0031413615215569735\n",
      "iteration 9636, loss: 0.0026856143958866596\n",
      "iteration 9637, loss: 0.0027893208898603916\n",
      "iteration 9638, loss: 0.002993994625285268\n",
      "iteration 9639, loss: 0.00283749564550817\n",
      "iteration 9640, loss: 0.0027266223914921284\n",
      "iteration 9641, loss: 0.0024686777032911777\n",
      "iteration 9642, loss: 0.003958504647016525\n",
      "iteration 9643, loss: 0.0027733249589800835\n",
      "iteration 9644, loss: 0.0028932220302522182\n",
      "iteration 9645, loss: 0.0030843617860227823\n",
      "iteration 9646, loss: 0.0030377148650586605\n",
      "iteration 9647, loss: 0.0026139658875763416\n",
      "iteration 9648, loss: 0.0037516485899686813\n",
      "iteration 9649, loss: 0.0031921048648655415\n",
      "iteration 9650, loss: 0.003432492259889841\n",
      "iteration 9651, loss: 0.0027042701840400696\n",
      "iteration 9652, loss: 0.0029415348544716835\n",
      "iteration 9653, loss: 0.0030787966679781675\n",
      "iteration 9654, loss: 0.0027246964164078236\n",
      "iteration 9655, loss: 0.003627808066084981\n",
      "iteration 9656, loss: 0.0031869658268988132\n",
      "iteration 9657, loss: 0.0034458984155207872\n",
      "iteration 9658, loss: 0.0029950481839478016\n",
      "iteration 9659, loss: 0.003607819089666009\n",
      "iteration 9660, loss: 0.003424364374950528\n",
      "iteration 9661, loss: 0.002947945147752762\n",
      "iteration 9662, loss: 0.0027400595135986805\n",
      "iteration 9663, loss: 0.002290165750309825\n",
      "iteration 9664, loss: 0.002285010414198041\n",
      "iteration 9665, loss: 0.003288720268756151\n",
      "iteration 9666, loss: 0.002720993012189865\n",
      "iteration 9667, loss: 0.002617910970002413\n",
      "iteration 9668, loss: 0.002812473103404045\n",
      "iteration 9669, loss: 0.0029327417723834515\n",
      "iteration 9670, loss: 0.003138377331197262\n",
      "iteration 9671, loss: 0.002650018548592925\n",
      "iteration 9672, loss: 0.0029694302938878536\n",
      "iteration 9673, loss: 0.003065150696784258\n",
      "iteration 9674, loss: 0.002760624513030052\n",
      "iteration 9675, loss: 0.0028649780433624983\n",
      "iteration 9676, loss: 0.0023090888280421495\n",
      "iteration 9677, loss: 0.002774759428575635\n",
      "iteration 9678, loss: 0.002672638278454542\n",
      "iteration 9679, loss: 0.0032095913775265217\n",
      "iteration 9680, loss: 0.0026431865990161896\n",
      "iteration 9681, loss: 0.002584736328572035\n",
      "iteration 9682, loss: 0.0027360725216567516\n",
      "iteration 9683, loss: 0.0028407713398337364\n",
      "iteration 9684, loss: 0.0024161466863006353\n",
      "iteration 9685, loss: 0.003407296258956194\n",
      "iteration 9686, loss: 0.0033212502021342516\n",
      "iteration 9687, loss: 0.003176601603627205\n",
      "iteration 9688, loss: 0.002821162808686495\n",
      "iteration 9689, loss: 0.0022824937477707863\n",
      "iteration 9690, loss: 0.0022041676566004753\n",
      "iteration 9691, loss: 0.0028761804569512606\n",
      "iteration 9692, loss: 0.002795922104269266\n",
      "iteration 9693, loss: 0.002898899372667074\n",
      "iteration 9694, loss: 0.002948442008346319\n",
      "iteration 9695, loss: 0.0028298813849687576\n",
      "iteration 9696, loss: 0.0027419973630458117\n",
      "iteration 9697, loss: 0.0024698597844690084\n",
      "iteration 9698, loss: 0.003269147826358676\n",
      "iteration 9699, loss: 0.002906753681600094\n",
      "iteration 9700, loss: 0.0031107873655855656\n",
      "iteration 9701, loss: 0.0030916323885321617\n",
      "iteration 9702, loss: 0.003050933126360178\n",
      "iteration 9703, loss: 0.00411988515406847\n",
      "iteration 9704, loss: 0.002979039680212736\n",
      "iteration 9705, loss: 0.0027600720059126616\n",
      "iteration 9706, loss: 0.0037254951894283295\n",
      "iteration 9707, loss: 0.002388800261542201\n",
      "iteration 9708, loss: 0.0035315623972564936\n",
      "iteration 9709, loss: 0.002955159405246377\n",
      "iteration 9710, loss: 0.002946957014501095\n",
      "iteration 9711, loss: 0.002771999454125762\n",
      "iteration 9712, loss: 0.0030788364820182323\n",
      "iteration 9713, loss: 0.0032851053401827812\n",
      "iteration 9714, loss: 0.002863888395950198\n",
      "iteration 9715, loss: 0.002851859200745821\n",
      "iteration 9716, loss: 0.0030748401768505573\n",
      "iteration 9717, loss: 0.0028471534606069326\n",
      "iteration 9718, loss: 0.0035370723344385624\n",
      "iteration 9719, loss: 0.0029639289714396\n",
      "iteration 9720, loss: 0.0027573518455028534\n",
      "iteration 9721, loss: 0.0028881256002932787\n",
      "iteration 9722, loss: 0.0030821007676422596\n",
      "iteration 9723, loss: 0.00280224671587348\n",
      "iteration 9724, loss: 0.003251034300774336\n",
      "iteration 9725, loss: 0.002670922316610813\n",
      "iteration 9726, loss: 0.002657400444149971\n",
      "iteration 9727, loss: 0.0027656140737235546\n",
      "iteration 9728, loss: 0.002461508149281144\n",
      "iteration 9729, loss: 0.0026262979954481125\n",
      "iteration 9730, loss: 0.002858011983335018\n",
      "iteration 9731, loss: 0.002927228342741728\n",
      "iteration 9732, loss: 0.0027036783285439014\n",
      "iteration 9733, loss: 0.00334535026922822\n",
      "iteration 9734, loss: 0.0030296321492642164\n",
      "iteration 9735, loss: 0.0029590546619147062\n",
      "iteration 9736, loss: 0.003095055930316448\n",
      "iteration 9737, loss: 0.003455066354945302\n",
      "iteration 9738, loss: 0.0030645872466266155\n",
      "iteration 9739, loss: 0.002654760843142867\n",
      "iteration 9740, loss: 0.0029133358038961887\n",
      "iteration 9741, loss: 0.0026175077073276043\n",
      "iteration 9742, loss: 0.0026686531491577625\n",
      "iteration 9743, loss: 0.0031117252074182034\n",
      "iteration 9744, loss: 0.0032909270375967026\n",
      "iteration 9745, loss: 0.002580885775387287\n",
      "iteration 9746, loss: 0.0029358339961618185\n",
      "iteration 9747, loss: 0.003021403681486845\n",
      "iteration 9748, loss: 0.0035916240885853767\n",
      "iteration 9749, loss: 0.002473781118169427\n",
      "iteration 9750, loss: 0.002753883134573698\n",
      "iteration 9751, loss: 0.003361132927238941\n",
      "iteration 9752, loss: 0.0030749544966965914\n",
      "iteration 9753, loss: 0.0026267128996551037\n",
      "iteration 9754, loss: 0.00365445576608181\n",
      "iteration 9755, loss: 0.0027195229195058346\n",
      "iteration 9756, loss: 0.003419792279601097\n",
      "iteration 9757, loss: 0.0029705404303967953\n",
      "iteration 9758, loss: 0.0028565688990056515\n",
      "iteration 9759, loss: 0.0028552168514579535\n",
      "iteration 9760, loss: 0.0029268518555909395\n",
      "iteration 9761, loss: 0.003111491445451975\n",
      "iteration 9762, loss: 0.0024495625402778387\n",
      "iteration 9763, loss: 0.0027223839424550533\n",
      "iteration 9764, loss: 0.0028181071393191814\n",
      "iteration 9765, loss: 0.0033668833784759045\n",
      "iteration 9766, loss: 0.002805595053359866\n",
      "iteration 9767, loss: 0.0025801111478358507\n",
      "iteration 9768, loss: 0.0025227542500942945\n",
      "iteration 9769, loss: 0.0026656240224838257\n",
      "iteration 9770, loss: 0.0024768561124801636\n",
      "iteration 9771, loss: 0.0026632167864590883\n",
      "iteration 9772, loss: 0.002828397788107395\n",
      "iteration 9773, loss: 0.003091223072260618\n",
      "iteration 9774, loss: 0.0023737316951155663\n",
      "iteration 9775, loss: 0.002701259683817625\n",
      "iteration 9776, loss: 0.0031399247236549854\n",
      "iteration 9777, loss: 0.002843767637386918\n",
      "iteration 9778, loss: 0.0033427351154386997\n",
      "iteration 9779, loss: 0.0027565122582018375\n",
      "iteration 9780, loss: 0.0020949579775333405\n",
      "iteration 9781, loss: 0.002342756139114499\n",
      "iteration 9782, loss: 0.002658720128238201\n",
      "iteration 9783, loss: 0.0028616238851100206\n",
      "iteration 9784, loss: 0.003302180441096425\n",
      "iteration 9785, loss: 0.0029754680581390858\n",
      "iteration 9786, loss: 0.0029374894220381975\n",
      "iteration 9787, loss: 0.00294252997264266\n",
      "iteration 9788, loss: 0.00260139349848032\n",
      "iteration 9789, loss: 0.0026725642383098602\n",
      "iteration 9790, loss: 0.00235431338660419\n",
      "iteration 9791, loss: 0.0034270931500941515\n",
      "iteration 9792, loss: 0.002589721931144595\n",
      "iteration 9793, loss: 0.002617735182866454\n",
      "iteration 9794, loss: 0.003062410745769739\n",
      "iteration 9795, loss: 0.00282172579318285\n",
      "iteration 9796, loss: 0.0034107379615306854\n",
      "iteration 9797, loss: 0.003157195635139942\n",
      "iteration 9798, loss: 0.002346967812627554\n",
      "iteration 9799, loss: 0.0030934608075767756\n",
      "iteration 9800, loss: 0.0030591553077101707\n",
      "iteration 9801, loss: 0.003190478775650263\n",
      "iteration 9802, loss: 0.002753056585788727\n",
      "iteration 9803, loss: 0.0032696055714040995\n",
      "iteration 9804, loss: 0.0028068488463759422\n",
      "iteration 9805, loss: 0.002870060969144106\n",
      "iteration 9806, loss: 0.003217979334294796\n",
      "iteration 9807, loss: 0.0033748112618923187\n",
      "iteration 9808, loss: 0.00286086555570364\n",
      "iteration 9809, loss: 0.0028147862758487463\n",
      "iteration 9810, loss: 0.0023125135339796543\n",
      "iteration 9811, loss: 0.0026239899452775717\n",
      "iteration 9812, loss: 0.00312733999453485\n",
      "iteration 9813, loss: 0.002708690706640482\n",
      "iteration 9814, loss: 0.0031854088883847\n",
      "iteration 9815, loss: 0.003135808277875185\n",
      "iteration 9816, loss: 0.0023319763131439686\n",
      "iteration 9817, loss: 0.0025207893922924995\n",
      "iteration 9818, loss: 0.002844443079084158\n",
      "iteration 9819, loss: 0.0033742599189281464\n",
      "iteration 9820, loss: 0.0030124466866254807\n",
      "iteration 9821, loss: 0.0023499333765357733\n",
      "iteration 9822, loss: 0.002329593524336815\n",
      "iteration 9823, loss: 0.002863495144993067\n",
      "iteration 9824, loss: 0.00315473647788167\n",
      "iteration 9825, loss: 0.0026383220683783293\n",
      "iteration 9826, loss: 0.002529798774048686\n",
      "iteration 9827, loss: 0.0030649970285594463\n",
      "iteration 9828, loss: 0.002718665637075901\n",
      "iteration 9829, loss: 0.0023835322353988886\n",
      "iteration 9830, loss: 0.0028360614087432623\n",
      "iteration 9831, loss: 0.0029337997548282146\n",
      "iteration 9832, loss: 0.0031523830257356167\n",
      "iteration 9833, loss: 0.003194413147866726\n",
      "iteration 9834, loss: 0.002697175834327936\n",
      "iteration 9835, loss: 0.0025793728418648243\n",
      "iteration 9836, loss: 0.0026565054431557655\n",
      "iteration 9837, loss: 0.002852597739547491\n",
      "iteration 9838, loss: 0.0029629566706717014\n",
      "iteration 9839, loss: 0.002477062400430441\n",
      "iteration 9840, loss: 0.0029139313846826553\n",
      "iteration 9841, loss: 0.0025099606718868017\n",
      "iteration 9842, loss: 0.0027079638093709946\n",
      "iteration 9843, loss: 0.0028580508660525084\n",
      "iteration 9844, loss: 0.002817924600094557\n",
      "iteration 9845, loss: 0.0030293078161776066\n",
      "iteration 9846, loss: 0.0026284230407327414\n",
      "iteration 9847, loss: 0.0027255420573055744\n",
      "iteration 9848, loss: 0.002889266237616539\n",
      "iteration 9849, loss: 0.002987678861245513\n",
      "iteration 9850, loss: 0.002736604306846857\n",
      "iteration 9851, loss: 0.0029491542372852564\n",
      "iteration 9852, loss: 0.00322518777102232\n",
      "iteration 9853, loss: 0.0031029172241687775\n",
      "iteration 9854, loss: 0.0029992051422595978\n",
      "iteration 9855, loss: 0.002675015013664961\n",
      "iteration 9856, loss: 0.0030356922652572393\n",
      "iteration 9857, loss: 0.0024078055284917355\n",
      "iteration 9858, loss: 0.002825740724802017\n",
      "iteration 9859, loss: 0.0030030254274606705\n",
      "iteration 9860, loss: 0.00263963732868433\n",
      "iteration 9861, loss: 0.0028590320143848658\n",
      "iteration 9862, loss: 0.002366137458011508\n",
      "iteration 9863, loss: 0.002688606968149543\n",
      "iteration 9864, loss: 0.00300407106988132\n",
      "iteration 9865, loss: 0.0028247302398085594\n",
      "iteration 9866, loss: 0.0027338070794939995\n",
      "iteration 9867, loss: 0.0029179593548178673\n",
      "iteration 9868, loss: 0.0027364827692508698\n",
      "iteration 9869, loss: 0.0032949221786111593\n",
      "iteration 9870, loss: 0.0027908675838261843\n",
      "iteration 9871, loss: 0.0032776864245533943\n",
      "iteration 9872, loss: 0.003107030875980854\n",
      "iteration 9873, loss: 0.0026084790006279945\n",
      "iteration 9874, loss: 0.0028154761530458927\n",
      "iteration 9875, loss: 0.0029504033736884594\n",
      "iteration 9876, loss: 0.0030264570377767086\n",
      "iteration 9877, loss: 0.0026697651483118534\n",
      "iteration 9878, loss: 0.002977356081828475\n",
      "iteration 9879, loss: 0.0028599712532013655\n",
      "iteration 9880, loss: 0.002292575314640999\n",
      "iteration 9881, loss: 0.0030299292411655188\n",
      "iteration 9882, loss: 0.0029306975193321705\n",
      "iteration 9883, loss: 0.002880545100197196\n",
      "iteration 9884, loss: 0.003266700077801943\n",
      "iteration 9885, loss: 0.0027065223548561335\n",
      "iteration 9886, loss: 0.00290672411210835\n",
      "iteration 9887, loss: 0.0028138214256614447\n",
      "iteration 9888, loss: 0.0028366497717797756\n",
      "iteration 9889, loss: 0.0035256275441497564\n",
      "iteration 9890, loss: 0.0027312752790749073\n",
      "iteration 9891, loss: 0.002918334910646081\n",
      "iteration 9892, loss: 0.0030240602791309357\n",
      "iteration 9893, loss: 0.0023040303494781256\n",
      "iteration 9894, loss: 0.0027866773307323456\n",
      "iteration 9895, loss: 0.0032592900097370148\n",
      "iteration 9896, loss: 0.0024997000582516193\n",
      "iteration 9897, loss: 0.0029842453077435493\n",
      "iteration 9898, loss: 0.002665539039298892\n",
      "iteration 9899, loss: 0.0030604819767177105\n",
      "iteration 9900, loss: 0.0029788219835609198\n",
      "iteration 9901, loss: 0.0030317995697259903\n",
      "iteration 9902, loss: 0.0032707578502595425\n",
      "iteration 9903, loss: 0.00270022451877594\n",
      "iteration 9904, loss: 0.0027946652844548225\n",
      "iteration 9905, loss: 0.002466989681124687\n",
      "iteration 9906, loss: 0.002602814696729183\n",
      "iteration 9907, loss: 0.0027341058012098074\n",
      "iteration 9908, loss: 0.0025272569619119167\n",
      "iteration 9909, loss: 0.0022295163944363594\n",
      "iteration 9910, loss: 0.002571721561253071\n",
      "iteration 9911, loss: 0.002780660754069686\n",
      "iteration 9912, loss: 0.002780040493234992\n",
      "iteration 9913, loss: 0.0030012018978595734\n",
      "iteration 9914, loss: 0.003043167758733034\n",
      "iteration 9915, loss: 0.0026265601627528667\n",
      "iteration 9916, loss: 0.0028462170157581568\n",
      "iteration 9917, loss: 0.002927304245531559\n",
      "iteration 9918, loss: 0.0025955557357519865\n",
      "iteration 9919, loss: 0.0032181248534470797\n",
      "iteration 9920, loss: 0.0033153346739709377\n",
      "iteration 9921, loss: 0.0026876255869865417\n",
      "iteration 9922, loss: 0.002562766894698143\n",
      "iteration 9923, loss: 0.0029576166998595\n",
      "iteration 9924, loss: 0.002853209851309657\n",
      "iteration 9925, loss: 0.0027238596230745316\n",
      "iteration 9926, loss: 0.002591185038909316\n",
      "iteration 9927, loss: 0.002920582890510559\n",
      "iteration 9928, loss: 0.003076899331063032\n",
      "iteration 9929, loss: 0.002945818705484271\n",
      "iteration 9930, loss: 0.0031121443025767803\n",
      "iteration 9931, loss: 0.002676651580259204\n",
      "iteration 9932, loss: 0.0034395060501992702\n",
      "iteration 9933, loss: 0.0026719113811850548\n",
      "iteration 9934, loss: 0.003174074459820986\n",
      "iteration 9935, loss: 0.003762081265449524\n",
      "iteration 9936, loss: 0.002972237765789032\n",
      "iteration 9937, loss: 0.003006588201969862\n",
      "iteration 9938, loss: 0.0028209646698087454\n",
      "iteration 9939, loss: 0.002714413683861494\n",
      "iteration 9940, loss: 0.0032640118151903152\n",
      "iteration 9941, loss: 0.003094721119850874\n",
      "iteration 9942, loss: 0.0027524512261152267\n",
      "iteration 9943, loss: 0.0033293040469288826\n",
      "iteration 9944, loss: 0.0031006806530058384\n",
      "iteration 9945, loss: 0.002685661893337965\n",
      "iteration 9946, loss: 0.003034687601029873\n",
      "iteration 9947, loss: 0.0027148660738021135\n",
      "iteration 9948, loss: 0.003165855770930648\n",
      "iteration 9949, loss: 0.0025939620099961758\n",
      "iteration 9950, loss: 0.0034146863035857677\n",
      "iteration 9951, loss: 0.0031886533834040165\n",
      "iteration 9952, loss: 0.0028143348172307014\n",
      "iteration 9953, loss: 0.0031168898567557335\n",
      "iteration 9954, loss: 0.0026952363550662994\n",
      "iteration 9955, loss: 0.002892536111176014\n",
      "iteration 9956, loss: 0.00255393935367465\n",
      "iteration 9957, loss: 0.0031131936702877283\n",
      "iteration 9958, loss: 0.0033022137358784676\n",
      "iteration 9959, loss: 0.002896606922149658\n",
      "iteration 9960, loss: 0.002611614763736725\n",
      "iteration 9961, loss: 0.0034033223055303097\n",
      "iteration 9962, loss: 0.0029204010497778654\n",
      "iteration 9963, loss: 0.002767406404018402\n",
      "iteration 9964, loss: 0.0033196161966770887\n",
      "iteration 9965, loss: 0.002612804528325796\n",
      "iteration 9966, loss: 0.002823134884238243\n",
      "iteration 9967, loss: 0.0029230755753815174\n",
      "iteration 9968, loss: 0.002989825326949358\n",
      "iteration 9969, loss: 0.0026449752040207386\n",
      "iteration 9970, loss: 0.0029233188834041357\n",
      "iteration 9971, loss: 0.003399317618459463\n",
      "iteration 9972, loss: 0.002820624504238367\n",
      "iteration 9973, loss: 0.0027285730466246605\n",
      "iteration 9974, loss: 0.00266252551227808\n",
      "iteration 9975, loss: 0.002691403031349182\n",
      "iteration 9976, loss: 0.00266710901632905\n",
      "iteration 9977, loss: 0.0033232830464839935\n",
      "iteration 9978, loss: 0.002562809269875288\n",
      "iteration 9979, loss: 0.0033188783563673496\n",
      "iteration 9980, loss: 0.0027718115597963333\n",
      "iteration 9981, loss: 0.0025846383068710566\n",
      "iteration 9982, loss: 0.002883000299334526\n",
      "iteration 9983, loss: 0.0026428254786878824\n",
      "iteration 9984, loss: 0.0030303550884127617\n",
      "iteration 9985, loss: 0.0035245707258582115\n",
      "iteration 9986, loss: 0.0025436347350478172\n",
      "iteration 9987, loss: 0.0026290074456483126\n",
      "iteration 9988, loss: 0.0026068317238241434\n",
      "iteration 9989, loss: 0.0030581403989344835\n",
      "iteration 9990, loss: 0.0026464432012289762\n",
      "iteration 9991, loss: 0.002847882453352213\n",
      "iteration 9992, loss: 0.0028510564006865025\n",
      "iteration 9993, loss: 0.002911289921030402\n",
      "iteration 9994, loss: 0.0025610672309994698\n",
      "iteration 9995, loss: 0.0030051106587052345\n",
      "iteration 9996, loss: 0.0025624281261116266\n",
      "iteration 9997, loss: 0.0035292827524244785\n",
      "iteration 9998, loss: 0.002900893334299326\n",
      "iteration 9999, loss: 0.0025786173064261675\n",
      "iteration 10000, loss: 0.0030614538118243217\n",
      "iteration 10001, loss: 0.0034137319307774305\n",
      "iteration 10002, loss: 0.0027090166695415974\n",
      "iteration 10003, loss: 0.0025895109865814447\n",
      "iteration 10004, loss: 0.0029617862310260534\n",
      "iteration 10005, loss: 0.0029444615356624126\n",
      "iteration 10006, loss: 0.002464115619659424\n",
      "iteration 10007, loss: 0.0036883633583784103\n",
      "iteration 10008, loss: 0.002761446638032794\n",
      "iteration 10009, loss: 0.003482274478301406\n",
      "iteration 10010, loss: 0.002600237261503935\n",
      "iteration 10011, loss: 0.002772998996078968\n",
      "iteration 10012, loss: 0.0027015393134206533\n",
      "iteration 10013, loss: 0.002701627090573311\n",
      "iteration 10014, loss: 0.0029582721181213856\n",
      "iteration 10015, loss: 0.003116662846878171\n",
      "iteration 10016, loss: 0.0029098743107169867\n",
      "iteration 10017, loss: 0.002985491883009672\n",
      "iteration 10018, loss: 0.002794995903968811\n",
      "iteration 10019, loss: 0.0025840546004474163\n",
      "iteration 10020, loss: 0.003091964405030012\n",
      "iteration 10021, loss: 0.0026539750397205353\n",
      "iteration 10022, loss: 0.0027241427451372147\n",
      "iteration 10023, loss: 0.002818751148879528\n",
      "iteration 10024, loss: 0.002776494948193431\n",
      "iteration 10025, loss: 0.0028394663240760565\n",
      "iteration 10026, loss: 0.0028766801115125418\n",
      "iteration 10027, loss: 0.0027955607511103153\n",
      "iteration 10028, loss: 0.0021791276521980762\n",
      "iteration 10029, loss: 0.002834460698068142\n",
      "iteration 10030, loss: 0.0027161743491888046\n",
      "iteration 10031, loss: 0.0032095995265990496\n",
      "iteration 10032, loss: 0.0020169317722320557\n",
      "iteration 10033, loss: 0.0030475223902612925\n",
      "iteration 10034, loss: 0.002727885264903307\n",
      "iteration 10035, loss: 0.0025117192417383194\n",
      "iteration 10036, loss: 0.002790407743304968\n",
      "iteration 10037, loss: 0.0024918015114963055\n",
      "iteration 10038, loss: 0.0027332690078765154\n",
      "iteration 10039, loss: 0.002927943132817745\n",
      "iteration 10040, loss: 0.0022546674590557814\n",
      "iteration 10041, loss: 0.0029929284937679768\n",
      "iteration 10042, loss: 0.002791823586449027\n",
      "iteration 10043, loss: 0.0027089626528322697\n",
      "iteration 10044, loss: 0.0029253738466650248\n",
      "iteration 10045, loss: 0.00277200760319829\n",
      "iteration 10046, loss: 0.003016857896000147\n",
      "iteration 10047, loss: 0.002807959448546171\n",
      "iteration 10048, loss: 0.0023872898891568184\n",
      "iteration 10049, loss: 0.0030724559910595417\n",
      "iteration 10050, loss: 0.0023106893058866262\n",
      "iteration 10051, loss: 0.003228183602914214\n",
      "iteration 10052, loss: 0.002732706954702735\n",
      "iteration 10053, loss: 0.002274341881275177\n",
      "iteration 10054, loss: 0.0028385259211063385\n",
      "iteration 10055, loss: 0.002657987643033266\n",
      "iteration 10056, loss: 0.003288300707936287\n",
      "iteration 10057, loss: 0.002212190069258213\n",
      "iteration 10058, loss: 0.002621373161673546\n",
      "iteration 10059, loss: 0.0032010425347834826\n",
      "iteration 10060, loss: 0.002770246472209692\n",
      "iteration 10061, loss: 0.0024994027335196733\n",
      "iteration 10062, loss: 0.0024973382242023945\n",
      "iteration 10063, loss: 0.0027933483943343163\n",
      "iteration 10064, loss: 0.002950012916699052\n",
      "iteration 10065, loss: 0.0025284281000494957\n",
      "iteration 10066, loss: 0.0031149405986070633\n",
      "iteration 10067, loss: 0.0027562654577195644\n",
      "iteration 10068, loss: 0.0028829218354076147\n",
      "iteration 10069, loss: 0.002756658475846052\n",
      "iteration 10070, loss: 0.003158924635499716\n",
      "iteration 10071, loss: 0.002739191288128495\n",
      "iteration 10072, loss: 0.002730170963332057\n",
      "iteration 10073, loss: 0.0025902155321091413\n",
      "iteration 10074, loss: 0.0029556648805737495\n",
      "iteration 10075, loss: 0.002798720495775342\n",
      "iteration 10076, loss: 0.0021688202396035194\n",
      "iteration 10077, loss: 0.002749568549916148\n",
      "iteration 10078, loss: 0.0030982999596744776\n",
      "iteration 10079, loss: 0.0025371513329446316\n",
      "iteration 10080, loss: 0.003015755442902446\n",
      "iteration 10081, loss: 0.0024562813341617584\n",
      "iteration 10082, loss: 0.002862163819372654\n",
      "iteration 10083, loss: 0.002803354524075985\n",
      "iteration 10084, loss: 0.0024508736096322536\n",
      "iteration 10085, loss: 0.002685694955289364\n",
      "iteration 10086, loss: 0.0026952160988003016\n",
      "iteration 10087, loss: 0.0028191604651510715\n",
      "iteration 10088, loss: 0.0027350084856152534\n",
      "iteration 10089, loss: 0.0029488177970051765\n",
      "iteration 10090, loss: 0.003367796540260315\n",
      "iteration 10091, loss: 0.0023234139662235975\n",
      "iteration 10092, loss: 0.002985283499583602\n",
      "iteration 10093, loss: 0.0029832033906131983\n",
      "iteration 10094, loss: 0.0022445505019277334\n",
      "iteration 10095, loss: 0.0024274426978081465\n",
      "iteration 10096, loss: 0.0024778160732239485\n",
      "iteration 10097, loss: 0.0025636719074100256\n",
      "iteration 10098, loss: 0.0026755458675324917\n",
      "iteration 10099, loss: 0.0024091885425150394\n",
      "iteration 10100, loss: 0.0023339653853327036\n",
      "iteration 10101, loss: 0.002966672647744417\n",
      "iteration 10102, loss: 0.002379440004006028\n",
      "iteration 10103, loss: 0.003271819092333317\n",
      "iteration 10104, loss: 0.003051686566323042\n",
      "iteration 10105, loss: 0.0027499357238411903\n",
      "iteration 10106, loss: 0.0027980171144008636\n",
      "iteration 10107, loss: 0.0031780938152223825\n",
      "iteration 10108, loss: 0.0027839108370244503\n",
      "iteration 10109, loss: 0.0026056827045977116\n",
      "iteration 10110, loss: 0.002436598762869835\n",
      "iteration 10111, loss: 0.002352297306060791\n",
      "iteration 10112, loss: 0.0024084090255200863\n",
      "iteration 10113, loss: 0.0021877039689570665\n",
      "iteration 10114, loss: 0.003258026670664549\n",
      "iteration 10115, loss: 0.002414480783045292\n",
      "iteration 10116, loss: 0.0034718955866992474\n",
      "iteration 10117, loss: 0.0024483948945999146\n",
      "iteration 10118, loss: 0.0023706567008048296\n",
      "iteration 10119, loss: 0.0024448037147521973\n",
      "iteration 10120, loss: 0.002627146663144231\n",
      "iteration 10121, loss: 0.002641160972416401\n",
      "iteration 10122, loss: 0.0030572041869163513\n",
      "iteration 10123, loss: 0.0024739070795476437\n",
      "iteration 10124, loss: 0.002294043079018593\n",
      "iteration 10125, loss: 0.0031206689309328794\n",
      "iteration 10126, loss: 0.0027151782996952534\n",
      "iteration 10127, loss: 0.0026954086497426033\n",
      "iteration 10128, loss: 0.0025838729925453663\n",
      "iteration 10129, loss: 0.002849223092198372\n",
      "iteration 10130, loss: 0.0023573830258101225\n",
      "iteration 10131, loss: 0.0025280406698584557\n",
      "iteration 10132, loss: 0.002915282268077135\n",
      "iteration 10133, loss: 0.0030864146538078785\n",
      "iteration 10134, loss: 0.0021864555310457945\n",
      "iteration 10135, loss: 0.002295461017638445\n",
      "iteration 10136, loss: 0.0026897736825048923\n",
      "iteration 10137, loss: 0.002521003596484661\n",
      "iteration 10138, loss: 0.0033197563607245684\n",
      "iteration 10139, loss: 0.002406980376690626\n",
      "iteration 10140, loss: 0.0028048157691955566\n",
      "iteration 10141, loss: 0.0028147539123892784\n",
      "iteration 10142, loss: 0.003288747277110815\n",
      "iteration 10143, loss: 0.0029230571817606688\n",
      "iteration 10144, loss: 0.002892534714192152\n",
      "iteration 10145, loss: 0.0036310716532170773\n",
      "iteration 10146, loss: 0.0030837454833090305\n",
      "iteration 10147, loss: 0.003380361245945096\n",
      "iteration 10148, loss: 0.00328870490193367\n",
      "iteration 10149, loss: 0.0027099931612610817\n",
      "iteration 10150, loss: 0.003094466170296073\n",
      "iteration 10151, loss: 0.002502170391380787\n",
      "iteration 10152, loss: 0.0028782193548977375\n",
      "iteration 10153, loss: 0.00256706727668643\n",
      "iteration 10154, loss: 0.0033081299625337124\n",
      "iteration 10155, loss: 0.0035730102099478245\n",
      "iteration 10156, loss: 0.003517338540405035\n",
      "iteration 10157, loss: 0.0028182677924633026\n",
      "iteration 10158, loss: 0.0029465975239872932\n",
      "iteration 10159, loss: 0.002831164048984647\n",
      "iteration 10160, loss: 0.0030720678623765707\n",
      "iteration 10161, loss: 0.00300045870244503\n",
      "iteration 10162, loss: 0.0033760196529328823\n",
      "iteration 10163, loss: 0.0024474700912833214\n",
      "iteration 10164, loss: 0.00302370497956872\n",
      "iteration 10165, loss: 0.002681603655219078\n",
      "iteration 10166, loss: 0.0023244761396199465\n",
      "iteration 10167, loss: 0.0023984843865036964\n",
      "iteration 10168, loss: 0.003014788730069995\n",
      "iteration 10169, loss: 0.0032160114496946335\n",
      "iteration 10170, loss: 0.0028907351661473513\n",
      "iteration 10171, loss: 0.0029700975865125656\n",
      "iteration 10172, loss: 0.002620154991745949\n",
      "iteration 10173, loss: 0.002826548181474209\n",
      "iteration 10174, loss: 0.002674088580533862\n",
      "iteration 10175, loss: 0.0029288935475051403\n",
      "iteration 10176, loss: 0.002779829315841198\n",
      "iteration 10177, loss: 0.003313628491014242\n",
      "iteration 10178, loss: 0.0028602785896509886\n",
      "iteration 10179, loss: 0.002673158422112465\n",
      "iteration 10180, loss: 0.0027134225238114595\n",
      "iteration 10181, loss: 0.0031163946259766817\n",
      "iteration 10182, loss: 0.0028671487234532833\n",
      "iteration 10183, loss: 0.0027093198150396347\n",
      "iteration 10184, loss: 0.002556008519604802\n",
      "iteration 10185, loss: 0.002254864200949669\n",
      "iteration 10186, loss: 0.0028823688626289368\n",
      "iteration 10187, loss: 0.003192365635186434\n",
      "iteration 10188, loss: 0.0026114238426089287\n",
      "iteration 10189, loss: 0.0032515027560293674\n",
      "iteration 10190, loss: 0.003218383062630892\n",
      "iteration 10191, loss: 0.0035172775387763977\n",
      "iteration 10192, loss: 0.0025760929565876722\n",
      "iteration 10193, loss: 0.0022141295485198498\n",
      "iteration 10194, loss: 0.0025311154313385487\n",
      "iteration 10195, loss: 0.0028672516345977783\n",
      "iteration 10196, loss: 0.0028602383099496365\n",
      "iteration 10197, loss: 0.002980410121381283\n",
      "iteration 10198, loss: 0.0027046033646911383\n",
      "iteration 10199, loss: 0.0035588343162089586\n",
      "iteration 10200, loss: 0.00252663460560143\n",
      "iteration 10201, loss: 0.002532741753384471\n",
      "iteration 10202, loss: 0.0028325626626610756\n",
      "iteration 10203, loss: 0.002213037107139826\n",
      "iteration 10204, loss: 0.0028683440759778023\n",
      "iteration 10205, loss: 0.0025305424351245165\n",
      "iteration 10206, loss: 0.00273389951325953\n",
      "iteration 10207, loss: 0.0022617438808083534\n",
      "iteration 10208, loss: 0.002920357510447502\n",
      "iteration 10209, loss: 0.002818408887833357\n",
      "iteration 10210, loss: 0.002389023080468178\n",
      "iteration 10211, loss: 0.0025000504683703184\n",
      "iteration 10212, loss: 0.0024376772344112396\n",
      "iteration 10213, loss: 0.0034821806475520134\n",
      "iteration 10214, loss: 0.002635868964716792\n",
      "iteration 10215, loss: 0.0026454702019691467\n",
      "iteration 10216, loss: 0.003192294156178832\n",
      "iteration 10217, loss: 0.0029200888238847256\n",
      "iteration 10218, loss: 0.0025409632362425327\n",
      "iteration 10219, loss: 0.003408596385270357\n",
      "iteration 10220, loss: 0.002528397599235177\n",
      "iteration 10221, loss: 0.0031588845886290073\n",
      "iteration 10222, loss: 0.0028859833255410194\n",
      "iteration 10223, loss: 0.0026302915066480637\n",
      "iteration 10224, loss: 0.0026401104405522346\n",
      "iteration 10225, loss: 0.002519066445529461\n",
      "iteration 10226, loss: 0.002910010749474168\n",
      "iteration 10227, loss: 0.002954249968752265\n",
      "iteration 10228, loss: 0.002772761508822441\n",
      "iteration 10229, loss: 0.002516132080927491\n",
      "iteration 10230, loss: 0.0029539787210524082\n",
      "iteration 10231, loss: 0.0021766982972621918\n",
      "iteration 10232, loss: 0.0031708483584225178\n",
      "iteration 10233, loss: 0.0024099666625261307\n",
      "iteration 10234, loss: 0.002677325392141938\n",
      "iteration 10235, loss: 0.0024011859204620123\n",
      "iteration 10236, loss: 0.003409321652725339\n",
      "iteration 10237, loss: 0.0028525213710963726\n",
      "iteration 10238, loss: 0.002801322378218174\n",
      "iteration 10239, loss: 0.002801168244332075\n",
      "iteration 10240, loss: 0.0023934836499392986\n",
      "iteration 10241, loss: 0.0032310939859598875\n",
      "iteration 10242, loss: 0.0026580370031297207\n",
      "iteration 10243, loss: 0.002666396088898182\n",
      "iteration 10244, loss: 0.0026045413687825203\n",
      "iteration 10245, loss: 0.0025107390247285366\n",
      "iteration 10246, loss: 0.002917638747021556\n",
      "iteration 10247, loss: 0.0030143936164677143\n",
      "iteration 10248, loss: 0.003107033669948578\n",
      "iteration 10249, loss: 0.002839181572198868\n",
      "iteration 10250, loss: 0.0025054055731743574\n",
      "iteration 10251, loss: 0.002632539253681898\n",
      "iteration 10252, loss: 0.0025671827606856823\n",
      "iteration 10253, loss: 0.0029383115470409393\n",
      "iteration 10254, loss: 0.002748054452240467\n",
      "iteration 10255, loss: 0.0024431534111499786\n",
      "iteration 10256, loss: 0.002797118853777647\n",
      "iteration 10257, loss: 0.0025726230815052986\n",
      "iteration 10258, loss: 0.002870631869882345\n",
      "iteration 10259, loss: 0.002483856165781617\n",
      "iteration 10260, loss: 0.0027544344775378704\n",
      "iteration 10261, loss: 0.0026094738859683275\n",
      "iteration 10262, loss: 0.0028324269223958254\n",
      "iteration 10263, loss: 0.0023757275193929672\n",
      "iteration 10264, loss: 0.002918404759839177\n",
      "iteration 10265, loss: 0.002832471625879407\n",
      "iteration 10266, loss: 0.0029448794666677713\n",
      "iteration 10267, loss: 0.0020783203653991222\n",
      "iteration 10268, loss: 0.0023612554650753736\n",
      "iteration 10269, loss: 0.0028078178875148296\n",
      "iteration 10270, loss: 0.0028547500260174274\n",
      "iteration 10271, loss: 0.003046390600502491\n",
      "iteration 10272, loss: 0.0029607953038066626\n",
      "iteration 10273, loss: 0.0028404872864484787\n",
      "iteration 10274, loss: 0.0028959561605006456\n",
      "iteration 10275, loss: 0.0025829894002527\n",
      "iteration 10276, loss: 0.0029027252458035946\n",
      "iteration 10277, loss: 0.002721143886446953\n",
      "iteration 10278, loss: 0.0030259983614087105\n",
      "iteration 10279, loss: 0.0021227989345788956\n",
      "iteration 10280, loss: 0.0025399415753781796\n",
      "iteration 10281, loss: 0.00246499665081501\n",
      "iteration 10282, loss: 0.002531282603740692\n",
      "iteration 10283, loss: 0.0024322790559381247\n",
      "iteration 10284, loss: 0.002170354127883911\n",
      "iteration 10285, loss: 0.0024543385952711105\n",
      "iteration 10286, loss: 0.002511888276785612\n",
      "iteration 10287, loss: 0.0028154896572232246\n",
      "iteration 10288, loss: 0.0023613767698407173\n",
      "iteration 10289, loss: 0.0022997623309493065\n",
      "iteration 10290, loss: 0.002988241845741868\n",
      "iteration 10291, loss: 0.0025610271841287613\n",
      "iteration 10292, loss: 0.00267246225848794\n",
      "iteration 10293, loss: 0.0025376183912158012\n",
      "iteration 10294, loss: 0.0023687637876719236\n",
      "iteration 10295, loss: 0.002325783483684063\n",
      "iteration 10296, loss: 0.002114785835146904\n",
      "iteration 10297, loss: 0.0023888673167675734\n",
      "iteration 10298, loss: 0.0028858003206551075\n",
      "iteration 10299, loss: 0.0024125571362674236\n",
      "iteration 10300, loss: 0.0026042726822197437\n",
      "iteration 10301, loss: 0.0026678647845983505\n",
      "iteration 10302, loss: 0.0027226628735661507\n",
      "iteration 10303, loss: 0.0025422624312341213\n",
      "iteration 10304, loss: 0.002466599689796567\n",
      "iteration 10305, loss: 0.0025267547462135553\n",
      "iteration 10306, loss: 0.002202186267822981\n",
      "iteration 10307, loss: 0.002891254611313343\n",
      "iteration 10308, loss: 0.002448220970109105\n",
      "iteration 10309, loss: 0.0030374405905604362\n",
      "iteration 10310, loss: 0.0029393364675343037\n",
      "iteration 10311, loss: 0.0025466105435043573\n",
      "iteration 10312, loss: 0.002597113372758031\n",
      "iteration 10313, loss: 0.0027588331140577793\n",
      "iteration 10314, loss: 0.0025078104808926582\n",
      "iteration 10315, loss: 0.0028092439752072096\n",
      "iteration 10316, loss: 0.002388843335211277\n",
      "iteration 10317, loss: 0.0032395655289292336\n",
      "iteration 10318, loss: 0.002995901508256793\n",
      "iteration 10319, loss: 0.0030048044864088297\n",
      "iteration 10320, loss: 0.0027744716499000788\n",
      "iteration 10321, loss: 0.003116182517260313\n",
      "iteration 10322, loss: 0.0029893990140408278\n",
      "iteration 10323, loss: 0.003449611132964492\n",
      "iteration 10324, loss: 0.0022984277456998825\n",
      "iteration 10325, loss: 0.003604326630011201\n",
      "iteration 10326, loss: 0.0027386252768337727\n",
      "iteration 10327, loss: 0.003162056440487504\n",
      "iteration 10328, loss: 0.0029966775327920914\n",
      "iteration 10329, loss: 0.0029379597399383783\n",
      "iteration 10330, loss: 0.0023113684728741646\n",
      "iteration 10331, loss: 0.0021711704321205616\n",
      "iteration 10332, loss: 0.0027336683124303818\n",
      "iteration 10333, loss: 0.002872862620279193\n",
      "iteration 10334, loss: 0.0029907706193625927\n",
      "iteration 10335, loss: 0.002941745100542903\n",
      "iteration 10336, loss: 0.0029470794834196568\n",
      "iteration 10337, loss: 0.0025759972631931305\n",
      "iteration 10338, loss: 0.0027904382441192865\n",
      "iteration 10339, loss: 0.0029525025747716427\n",
      "iteration 10340, loss: 0.0023852093145251274\n",
      "iteration 10341, loss: 0.002667319495230913\n",
      "iteration 10342, loss: 0.0024624525103718042\n",
      "iteration 10343, loss: 0.0023967549204826355\n",
      "iteration 10344, loss: 0.0035461022052913904\n",
      "iteration 10345, loss: 0.003581414232030511\n",
      "iteration 10346, loss: 0.0033176227007061243\n",
      "iteration 10347, loss: 0.002991782734170556\n",
      "iteration 10348, loss: 0.0027574540581554174\n",
      "iteration 10349, loss: 0.002531907055526972\n",
      "iteration 10350, loss: 0.002590504242107272\n",
      "iteration 10351, loss: 0.002665736945345998\n",
      "iteration 10352, loss: 0.0024468726478517056\n",
      "iteration 10353, loss: 0.0029045436531305313\n",
      "iteration 10354, loss: 0.0031017321161925793\n",
      "iteration 10355, loss: 0.002995327115058899\n",
      "iteration 10356, loss: 0.002784605138003826\n",
      "iteration 10357, loss: 0.0026537450030446053\n",
      "iteration 10358, loss: 0.0029439865611493587\n",
      "iteration 10359, loss: 0.003137339139357209\n",
      "iteration 10360, loss: 0.0029088291339576244\n",
      "iteration 10361, loss: 0.0029182678554207087\n",
      "iteration 10362, loss: 0.0036992975510656834\n",
      "iteration 10363, loss: 0.0022885259240865707\n",
      "iteration 10364, loss: 0.0029780121985822916\n",
      "iteration 10365, loss: 0.002331239404156804\n",
      "iteration 10366, loss: 0.0027427272871136665\n",
      "iteration 10367, loss: 0.0027958452701568604\n",
      "iteration 10368, loss: 0.002999256132170558\n",
      "iteration 10369, loss: 0.0029843286611139774\n",
      "iteration 10370, loss: 0.0030412906780838966\n",
      "iteration 10371, loss: 0.002783131320029497\n",
      "iteration 10372, loss: 0.0027005020529031754\n",
      "iteration 10373, loss: 0.0025715490337461233\n",
      "iteration 10374, loss: 0.0028102435171604156\n",
      "iteration 10375, loss: 0.0028524710796773434\n",
      "iteration 10376, loss: 0.002992373425513506\n",
      "iteration 10377, loss: 0.0030532656237483025\n",
      "iteration 10378, loss: 0.0029179477132856846\n",
      "iteration 10379, loss: 0.0031454134732484818\n",
      "iteration 10380, loss: 0.0030928109772503376\n",
      "iteration 10381, loss: 0.002970057539641857\n",
      "iteration 10382, loss: 0.002620823448523879\n",
      "iteration 10383, loss: 0.0021399129182100296\n",
      "iteration 10384, loss: 0.0026105029974132776\n",
      "iteration 10385, loss: 0.00249386765062809\n",
      "iteration 10386, loss: 0.0024864505976438522\n",
      "iteration 10387, loss: 0.002653950359672308\n",
      "iteration 10388, loss: 0.0032378812320530415\n",
      "iteration 10389, loss: 0.0026327597443014383\n",
      "iteration 10390, loss: 0.002611301140859723\n",
      "iteration 10391, loss: 0.003022323828190565\n",
      "iteration 10392, loss: 0.0023919157683849335\n",
      "iteration 10393, loss: 0.002570508513599634\n",
      "iteration 10394, loss: 0.0027271523140370846\n",
      "iteration 10395, loss: 0.002477988600730896\n",
      "iteration 10396, loss: 0.0027598063461482525\n",
      "iteration 10397, loss: 0.0026263147592544556\n",
      "iteration 10398, loss: 0.003286228980869055\n",
      "iteration 10399, loss: 0.0029788746032863855\n",
      "iteration 10400, loss: 0.002404190367087722\n",
      "iteration 10401, loss: 0.0028446977958083153\n",
      "iteration 10402, loss: 0.00296446168795228\n",
      "iteration 10403, loss: 0.003098930697888136\n",
      "iteration 10404, loss: 0.002614785684272647\n",
      "iteration 10405, loss: 0.0029865093529224396\n",
      "iteration 10406, loss: 0.003208199515938759\n",
      "iteration 10407, loss: 0.0027229858096688986\n",
      "iteration 10408, loss: 0.0027218216564506292\n",
      "iteration 10409, loss: 0.0032040132209658623\n",
      "iteration 10410, loss: 0.0031456067226827145\n",
      "iteration 10411, loss: 0.002772544976323843\n",
      "iteration 10412, loss: 0.0024292338639497757\n",
      "iteration 10413, loss: 0.0026127230376005173\n",
      "iteration 10414, loss: 0.002688843058422208\n",
      "iteration 10415, loss: 0.0026841885410249233\n",
      "iteration 10416, loss: 0.0024533462710678577\n",
      "iteration 10417, loss: 0.002856519538909197\n",
      "iteration 10418, loss: 0.002963016275316477\n",
      "iteration 10419, loss: 0.002918497659265995\n",
      "iteration 10420, loss: 0.0024578005541116\n",
      "iteration 10421, loss: 0.0027819385286420584\n",
      "iteration 10422, loss: 0.002699892735108733\n",
      "iteration 10423, loss: 0.002828266005963087\n",
      "iteration 10424, loss: 0.0022071157582104206\n",
      "iteration 10425, loss: 0.0023359523620456457\n",
      "iteration 10426, loss: 0.0028729813639074564\n",
      "iteration 10427, loss: 0.0023628193885087967\n",
      "iteration 10428, loss: 0.0021908360067754984\n",
      "iteration 10429, loss: 0.0022355562541633844\n",
      "iteration 10430, loss: 0.002590747084468603\n",
      "iteration 10431, loss: 0.0024955275002866983\n",
      "iteration 10432, loss: 0.0022906337399035692\n",
      "iteration 10433, loss: 0.00262488704174757\n",
      "iteration 10434, loss: 0.002726502250880003\n",
      "iteration 10435, loss: 0.0026897117495536804\n",
      "iteration 10436, loss: 0.0022095651365816593\n",
      "iteration 10437, loss: 0.002024364657700062\n",
      "iteration 10438, loss: 0.0024384399875998497\n",
      "iteration 10439, loss: 0.0023239622823894024\n",
      "iteration 10440, loss: 0.002480552764609456\n",
      "iteration 10441, loss: 0.0028457066509872675\n",
      "iteration 10442, loss: 0.0021397769451141357\n",
      "iteration 10443, loss: 0.0024287444539368153\n",
      "iteration 10444, loss: 0.002369491383433342\n",
      "iteration 10445, loss: 0.002335210097953677\n",
      "iteration 10446, loss: 0.002557034371420741\n",
      "iteration 10447, loss: 0.00298410770483315\n",
      "iteration 10448, loss: 0.002060739556327462\n",
      "iteration 10449, loss: 0.002776336157694459\n",
      "iteration 10450, loss: 0.0020372294820845127\n",
      "iteration 10451, loss: 0.0028880704194307327\n",
      "iteration 10452, loss: 0.002048456110060215\n",
      "iteration 10453, loss: 0.002545356284826994\n",
      "iteration 10454, loss: 0.0023578146938234568\n",
      "iteration 10455, loss: 0.0031733750365674496\n",
      "iteration 10456, loss: 0.0024735755287110806\n",
      "iteration 10457, loss: 0.002338666934520006\n",
      "iteration 10458, loss: 0.0023214947432279587\n",
      "iteration 10459, loss: 0.002505144104361534\n",
      "iteration 10460, loss: 0.002616215031594038\n",
      "iteration 10461, loss: 0.002638771664351225\n",
      "iteration 10462, loss: 0.002642351668328047\n",
      "iteration 10463, loss: 0.0025027019437402487\n",
      "iteration 10464, loss: 0.003037509508430958\n",
      "iteration 10465, loss: 0.0021783295087516308\n",
      "iteration 10466, loss: 0.002681005746126175\n",
      "iteration 10467, loss: 0.002402590587735176\n",
      "iteration 10468, loss: 0.0027636729646474123\n",
      "iteration 10469, loss: 0.002558402717113495\n",
      "iteration 10470, loss: 0.002875696402043104\n",
      "iteration 10471, loss: 0.0030706997495144606\n",
      "iteration 10472, loss: 0.0029999236576259136\n",
      "iteration 10473, loss: 0.0027619567699730396\n",
      "iteration 10474, loss: 0.0027464721351861954\n",
      "iteration 10475, loss: 0.0021045501343905926\n",
      "iteration 10476, loss: 0.003152528777718544\n",
      "iteration 10477, loss: 0.001903448486700654\n",
      "iteration 10478, loss: 0.002727821934968233\n",
      "iteration 10479, loss: 0.0026115102227777243\n",
      "iteration 10480, loss: 0.002901660045608878\n",
      "iteration 10481, loss: 0.0026014759205281734\n",
      "iteration 10482, loss: 0.0024462630972266197\n",
      "iteration 10483, loss: 0.0022404869087040424\n",
      "iteration 10484, loss: 0.002521870657801628\n",
      "iteration 10485, loss: 0.0024232249706983566\n",
      "iteration 10486, loss: 0.002653086557984352\n",
      "iteration 10487, loss: 0.00226953299716115\n",
      "iteration 10488, loss: 0.0026817014440894127\n",
      "iteration 10489, loss: 0.0026212050579488277\n",
      "iteration 10490, loss: 0.0024205632507801056\n",
      "iteration 10491, loss: 0.0027441137935966253\n",
      "iteration 10492, loss: 0.002402483019977808\n",
      "iteration 10493, loss: 0.0027618445456027985\n",
      "iteration 10494, loss: 0.0028622238896787167\n",
      "iteration 10495, loss: 0.0024868811015039682\n",
      "iteration 10496, loss: 0.0024579698219895363\n",
      "iteration 10497, loss: 0.002618292346596718\n",
      "iteration 10498, loss: 0.002846534363925457\n",
      "iteration 10499, loss: 0.0025473684072494507\n",
      "iteration 10500, loss: 0.002380620688199997\n",
      "iteration 10501, loss: 0.002702766563743353\n",
      "iteration 10502, loss: 0.0023264214396476746\n",
      "iteration 10503, loss: 0.0024962774477899075\n",
      "iteration 10504, loss: 0.0023479831870645285\n",
      "iteration 10505, loss: 0.0029263822361826897\n",
      "iteration 10506, loss: 0.0030168709345161915\n",
      "iteration 10507, loss: 0.0029117322992533445\n",
      "iteration 10508, loss: 0.00179259292781353\n",
      "iteration 10509, loss: 0.002521997783333063\n",
      "iteration 10510, loss: 0.002108274959027767\n",
      "iteration 10511, loss: 0.0026343832723796368\n",
      "iteration 10512, loss: 0.0023157456889748573\n",
      "iteration 10513, loss: 0.0026985667645931244\n",
      "iteration 10514, loss: 0.002888015005737543\n",
      "iteration 10515, loss: 0.002694659400731325\n",
      "iteration 10516, loss: 0.002251136116683483\n",
      "iteration 10517, loss: 0.002288275398313999\n",
      "iteration 10518, loss: 0.0023093644995242357\n",
      "iteration 10519, loss: 0.0020873842295259237\n",
      "iteration 10520, loss: 0.002183903707191348\n",
      "iteration 10521, loss: 0.00269926805049181\n",
      "iteration 10522, loss: 0.0023715379647910595\n",
      "iteration 10523, loss: 0.002350530354306102\n",
      "iteration 10524, loss: 0.002460852265357971\n",
      "iteration 10525, loss: 0.002969077555462718\n",
      "iteration 10526, loss: 0.0027109524235129356\n",
      "iteration 10527, loss: 0.0026485128328204155\n",
      "iteration 10528, loss: 0.002879232633858919\n",
      "iteration 10529, loss: 0.0026314377319067717\n",
      "iteration 10530, loss: 0.0024964537005871534\n",
      "iteration 10531, loss: 0.002655010437592864\n",
      "iteration 10532, loss: 0.002458669012412429\n",
      "iteration 10533, loss: 0.0032571961637586355\n",
      "iteration 10534, loss: 0.0028362474404275417\n",
      "iteration 10535, loss: 0.0026152185164391994\n",
      "iteration 10536, loss: 0.0027835574001073837\n",
      "iteration 10537, loss: 0.0026486243586987257\n",
      "iteration 10538, loss: 0.0033636088483035564\n",
      "iteration 10539, loss: 0.0027582375332713127\n",
      "iteration 10540, loss: 0.002585488837212324\n",
      "iteration 10541, loss: 0.0024008038453757763\n",
      "iteration 10542, loss: 0.0024669186677783728\n",
      "iteration 10543, loss: 0.0031643956899642944\n",
      "iteration 10544, loss: 0.0026192914228886366\n",
      "iteration 10545, loss: 0.0029101327527314425\n",
      "iteration 10546, loss: 0.0025411895476281643\n",
      "iteration 10547, loss: 0.003002732992172241\n",
      "iteration 10548, loss: 0.002602045424282551\n",
      "iteration 10549, loss: 0.0024933076929301023\n",
      "iteration 10550, loss: 0.002953237621113658\n",
      "iteration 10551, loss: 0.002426679013296962\n",
      "iteration 10552, loss: 0.0027089593932032585\n",
      "iteration 10553, loss: 0.003043613862246275\n",
      "iteration 10554, loss: 0.0024915202520787716\n",
      "iteration 10555, loss: 0.0024887751787900925\n",
      "iteration 10556, loss: 0.0022262746933847666\n",
      "iteration 10557, loss: 0.00272749294526875\n",
      "iteration 10558, loss: 0.003133932128548622\n",
      "iteration 10559, loss: 0.002868836047127843\n",
      "iteration 10560, loss: 0.0028762719593942165\n",
      "iteration 10561, loss: 0.0031762418802827597\n",
      "iteration 10562, loss: 0.002774254884570837\n",
      "iteration 10563, loss: 0.002482444979250431\n",
      "iteration 10564, loss: 0.0029193214140832424\n",
      "iteration 10565, loss: 0.0028614981565624475\n",
      "iteration 10566, loss: 0.0026344957295805216\n",
      "iteration 10567, loss: 0.002700474578887224\n",
      "iteration 10568, loss: 0.0027079470455646515\n",
      "iteration 10569, loss: 0.0026898288633674383\n",
      "iteration 10570, loss: 0.0030182977207005024\n",
      "iteration 10571, loss: 0.0030940717551857233\n",
      "iteration 10572, loss: 0.002482654293999076\n",
      "iteration 10573, loss: 0.0031848023645579815\n",
      "iteration 10574, loss: 0.002780397655442357\n",
      "iteration 10575, loss: 0.0023907397408038378\n",
      "iteration 10576, loss: 0.002500785980373621\n",
      "iteration 10577, loss: 0.0029841107316315174\n",
      "iteration 10578, loss: 0.0027937714476138353\n",
      "iteration 10579, loss: 0.0027470728382468224\n",
      "iteration 10580, loss: 0.0023712976835668087\n",
      "iteration 10581, loss: 0.0023994515649974346\n",
      "iteration 10582, loss: 0.002540652872994542\n",
      "iteration 10583, loss: 0.0026108757592737675\n",
      "iteration 10584, loss: 0.0026461551897227764\n",
      "iteration 10585, loss: 0.0025134622119367123\n",
      "iteration 10586, loss: 0.0023887548595666885\n",
      "iteration 10587, loss: 0.0031774260569363832\n",
      "iteration 10588, loss: 0.002424743724986911\n",
      "iteration 10589, loss: 0.0025062207132577896\n",
      "iteration 10590, loss: 0.0029222990851849318\n",
      "iteration 10591, loss: 0.002906958805397153\n",
      "iteration 10592, loss: 0.0027437119279056787\n",
      "iteration 10593, loss: 0.002668490167707205\n",
      "iteration 10594, loss: 0.0021852131467312574\n",
      "iteration 10595, loss: 0.002449573017656803\n",
      "iteration 10596, loss: 0.002480326686054468\n",
      "iteration 10597, loss: 0.002572169993072748\n",
      "iteration 10598, loss: 0.002486768877133727\n",
      "iteration 10599, loss: 0.003137216903269291\n",
      "iteration 10600, loss: 0.002974503207951784\n",
      "iteration 10601, loss: 0.002900252118706703\n",
      "iteration 10602, loss: 0.002702426863834262\n",
      "iteration 10603, loss: 0.0019942973740398884\n",
      "iteration 10604, loss: 0.0028493984136730433\n",
      "iteration 10605, loss: 0.0027187513187527657\n",
      "iteration 10606, loss: 0.001825825427658856\n",
      "iteration 10607, loss: 0.002403662307187915\n",
      "iteration 10608, loss: 0.0027628093957901\n",
      "iteration 10609, loss: 0.0025589410215616226\n",
      "iteration 10610, loss: 0.0023892042227089405\n",
      "iteration 10611, loss: 0.002687862841412425\n",
      "iteration 10612, loss: 0.0026476429775357246\n",
      "iteration 10613, loss: 0.0022179330699145794\n",
      "iteration 10614, loss: 0.0025501567870378494\n",
      "iteration 10615, loss: 0.0025090568233281374\n",
      "iteration 10616, loss: 0.0026756199076771736\n",
      "iteration 10617, loss: 0.0026374077424407005\n",
      "iteration 10618, loss: 0.0030784732662141323\n",
      "iteration 10619, loss: 0.0023177210241556168\n",
      "iteration 10620, loss: 0.002816419582813978\n",
      "iteration 10621, loss: 0.0026923739351332188\n",
      "iteration 10622, loss: 0.0026500073727220297\n",
      "iteration 10623, loss: 0.0024238526821136475\n",
      "iteration 10624, loss: 0.0025482431519776583\n",
      "iteration 10625, loss: 0.0029053660109639168\n",
      "iteration 10626, loss: 0.0025068619288504124\n",
      "iteration 10627, loss: 0.003022360149770975\n",
      "iteration 10628, loss: 0.002411873545497656\n",
      "iteration 10629, loss: 0.0030198772437870502\n",
      "iteration 10630, loss: 0.00243261456489563\n",
      "iteration 10631, loss: 0.0023092757910490036\n",
      "iteration 10632, loss: 0.002472071908414364\n",
      "iteration 10633, loss: 0.0024865642189979553\n",
      "iteration 10634, loss: 0.002389082685112953\n",
      "iteration 10635, loss: 0.00239309249445796\n",
      "iteration 10636, loss: 0.002246144227683544\n",
      "iteration 10637, loss: 0.002390885027125478\n",
      "iteration 10638, loss: 0.002359548816457391\n",
      "iteration 10639, loss: 0.002029519062489271\n",
      "iteration 10640, loss: 0.002628633286803961\n",
      "iteration 10641, loss: 0.002617325633764267\n",
      "iteration 10642, loss: 0.002873591845855117\n",
      "iteration 10643, loss: 0.002375341486185789\n",
      "iteration 10644, loss: 0.002905971370637417\n",
      "iteration 10645, loss: 0.002225576899945736\n",
      "iteration 10646, loss: 0.0026578246615827084\n",
      "iteration 10647, loss: 0.0027044015005230904\n",
      "iteration 10648, loss: 0.002829483011737466\n",
      "iteration 10649, loss: 0.002379682846367359\n",
      "iteration 10650, loss: 0.0025691036134958267\n",
      "iteration 10651, loss: 0.0032233870588243008\n",
      "iteration 10652, loss: 0.0026081777177751064\n",
      "iteration 10653, loss: 0.002782378811389208\n",
      "iteration 10654, loss: 0.0025073750875890255\n",
      "iteration 10655, loss: 0.0029627527110278606\n",
      "iteration 10656, loss: 0.002396613359451294\n",
      "iteration 10657, loss: 0.0022599317599087954\n",
      "iteration 10658, loss: 0.0026927099097520113\n",
      "iteration 10659, loss: 0.0027310443110764027\n",
      "iteration 10660, loss: 0.0024889525957405567\n",
      "iteration 10661, loss: 0.002851411234587431\n",
      "iteration 10662, loss: 0.002303758868947625\n",
      "iteration 10663, loss: 0.0021616960875689983\n",
      "iteration 10664, loss: 0.0021506021730601788\n",
      "iteration 10665, loss: 0.0024406136944890022\n",
      "iteration 10666, loss: 0.003075430868193507\n",
      "iteration 10667, loss: 0.0022231366019695997\n",
      "iteration 10668, loss: 0.002431393601000309\n",
      "iteration 10669, loss: 0.0028281069826334715\n",
      "iteration 10670, loss: 0.002149408683180809\n",
      "iteration 10671, loss: 0.002380402758717537\n",
      "iteration 10672, loss: 0.002275852020829916\n",
      "iteration 10673, loss: 0.0024459720589220524\n",
      "iteration 10674, loss: 0.0024151394609361887\n",
      "iteration 10675, loss: 0.0024387608282268047\n",
      "iteration 10676, loss: 0.002970708068460226\n",
      "iteration 10677, loss: 0.00221484643407166\n",
      "iteration 10678, loss: 0.002546929055824876\n",
      "iteration 10679, loss: 0.0027889732737094164\n",
      "iteration 10680, loss: 0.0026636221446096897\n",
      "iteration 10681, loss: 0.002242365851998329\n",
      "iteration 10682, loss: 0.0025509893894195557\n",
      "iteration 10683, loss: 0.0022051064297556877\n",
      "iteration 10684, loss: 0.0021605633664876223\n",
      "iteration 10685, loss: 0.0024351649917662144\n",
      "iteration 10686, loss: 0.0020022704266011715\n",
      "iteration 10687, loss: 0.0022387353237718344\n",
      "iteration 10688, loss: 0.0027185766957700253\n",
      "iteration 10689, loss: 0.0023638042621314526\n",
      "iteration 10690, loss: 0.0028463853523135185\n",
      "iteration 10691, loss: 0.002281676046550274\n",
      "iteration 10692, loss: 0.0021981648169457912\n",
      "iteration 10693, loss: 0.0027564470656216145\n",
      "iteration 10694, loss: 0.002428674604743719\n",
      "iteration 10695, loss: 0.0027309060096740723\n",
      "iteration 10696, loss: 0.0029229475185275078\n",
      "iteration 10697, loss: 0.0025235346984118223\n",
      "iteration 10698, loss: 0.0025551472790539265\n",
      "iteration 10699, loss: 0.0026716655120253563\n",
      "iteration 10700, loss: 0.0025478946045041084\n",
      "iteration 10701, loss: 0.0026028903666883707\n",
      "iteration 10702, loss: 0.0025383844040334225\n",
      "iteration 10703, loss: 0.002343195490539074\n",
      "iteration 10704, loss: 0.002824285300448537\n",
      "iteration 10705, loss: 0.0023810528218746185\n",
      "iteration 10706, loss: 0.00309703778475523\n",
      "iteration 10707, loss: 0.0027237297035753727\n",
      "iteration 10708, loss: 0.0023262910544872284\n",
      "iteration 10709, loss: 0.003393566934391856\n",
      "iteration 10710, loss: 0.0022381870076060295\n",
      "iteration 10711, loss: 0.0025905235670506954\n",
      "iteration 10712, loss: 0.0026892402675002813\n",
      "iteration 10713, loss: 0.00225700531154871\n",
      "iteration 10714, loss: 0.0023698010481894016\n",
      "iteration 10715, loss: 0.0024654457811266184\n",
      "iteration 10716, loss: 0.0027164805214852095\n",
      "iteration 10717, loss: 0.002407725900411606\n",
      "iteration 10718, loss: 0.0028624304104596376\n",
      "iteration 10719, loss: 0.001983242342248559\n",
      "iteration 10720, loss: 0.002136537805199623\n",
      "iteration 10721, loss: 0.0024179317988455296\n",
      "iteration 10722, loss: 0.0021918341517448425\n",
      "iteration 10723, loss: 0.0024825488217175007\n",
      "iteration 10724, loss: 0.0023010096047073603\n",
      "iteration 10725, loss: 0.002190100494772196\n",
      "iteration 10726, loss: 0.0018707194831222296\n",
      "iteration 10727, loss: 0.0026574013754725456\n",
      "iteration 10728, loss: 0.002573362085968256\n",
      "iteration 10729, loss: 0.0020843716338276863\n",
      "iteration 10730, loss: 0.002196105197072029\n",
      "iteration 10731, loss: 0.002014766214415431\n",
      "iteration 10732, loss: 0.0028873153496533632\n",
      "iteration 10733, loss: 0.0027096231933683157\n",
      "iteration 10734, loss: 0.0026305115316063166\n",
      "iteration 10735, loss: 0.0027708804700523615\n",
      "iteration 10736, loss: 0.0024964644107967615\n",
      "iteration 10737, loss: 0.0025649776216596365\n",
      "iteration 10738, loss: 0.002502480521798134\n",
      "iteration 10739, loss: 0.0023509766906499863\n",
      "iteration 10740, loss: 0.0021190254483371973\n",
      "iteration 10741, loss: 0.002426876686513424\n",
      "iteration 10742, loss: 0.0031361961737275124\n",
      "iteration 10743, loss: 0.001982499845325947\n",
      "iteration 10744, loss: 0.002830126788467169\n",
      "iteration 10745, loss: 0.002674801740795374\n",
      "iteration 10746, loss: 0.0024314168840646744\n",
      "iteration 10747, loss: 0.002271152101457119\n",
      "iteration 10748, loss: 0.0025030476972460747\n",
      "iteration 10749, loss: 0.002635548822581768\n",
      "iteration 10750, loss: 0.0024158128071576357\n",
      "iteration 10751, loss: 0.002296753693372011\n",
      "iteration 10752, loss: 0.0027304079849272966\n",
      "iteration 10753, loss: 0.0023419461213052273\n",
      "iteration 10754, loss: 0.00220388057641685\n",
      "iteration 10755, loss: 0.0024672222789376974\n",
      "iteration 10756, loss: 0.002057737670838833\n",
      "iteration 10757, loss: 0.002226863522082567\n",
      "iteration 10758, loss: 0.002377941273152828\n",
      "iteration 10759, loss: 0.001983104972168803\n",
      "iteration 10760, loss: 0.002217730740085244\n",
      "iteration 10761, loss: 0.002650154521688819\n",
      "iteration 10762, loss: 0.002291668439283967\n",
      "iteration 10763, loss: 0.0024894988164305687\n",
      "iteration 10764, loss: 0.0024261095095425844\n",
      "iteration 10765, loss: 0.0023995311930775642\n",
      "iteration 10766, loss: 0.0021542245522141457\n",
      "iteration 10767, loss: 0.002405907493084669\n",
      "iteration 10768, loss: 0.0020191192161291838\n",
      "iteration 10769, loss: 0.0022927657701075077\n",
      "iteration 10770, loss: 0.0028890357352793217\n",
      "iteration 10771, loss: 0.002304255962371826\n",
      "iteration 10772, loss: 0.0020737836603075266\n",
      "iteration 10773, loss: 0.0024998835287988186\n",
      "iteration 10774, loss: 0.00251065194606781\n",
      "iteration 10775, loss: 0.0023721216712146997\n",
      "iteration 10776, loss: 0.0025478906463831663\n",
      "iteration 10777, loss: 0.0024710865691304207\n",
      "iteration 10778, loss: 0.002843966707587242\n",
      "iteration 10779, loss: 0.0026330873370170593\n",
      "iteration 10780, loss: 0.0028743643779307604\n",
      "iteration 10781, loss: 0.0021232597064226866\n",
      "iteration 10782, loss: 0.002110797446221113\n",
      "iteration 10783, loss: 0.0021484559401869774\n",
      "iteration 10784, loss: 0.0024604834616184235\n",
      "iteration 10785, loss: 0.0027719936333596706\n",
      "iteration 10786, loss: 0.002326948568224907\n",
      "iteration 10787, loss: 0.0023977651726454496\n",
      "iteration 10788, loss: 0.0023374250158667564\n",
      "iteration 10789, loss: 0.0024116411805152893\n",
      "iteration 10790, loss: 0.002111183013767004\n",
      "iteration 10791, loss: 0.0024448540061712265\n",
      "iteration 10792, loss: 0.0026338936295360327\n",
      "iteration 10793, loss: 0.0021097443532198668\n",
      "iteration 10794, loss: 0.002185621066018939\n",
      "iteration 10795, loss: 0.002422692719846964\n",
      "iteration 10796, loss: 0.002293403958901763\n",
      "iteration 10797, loss: 0.0020931619219481945\n",
      "iteration 10798, loss: 0.0022479924373328686\n",
      "iteration 10799, loss: 0.0027862852439284325\n",
      "iteration 10800, loss: 0.0024582056794315577\n",
      "iteration 10801, loss: 0.002127407118678093\n",
      "iteration 10802, loss: 0.0022680433467030525\n",
      "iteration 10803, loss: 0.003118854481726885\n",
      "iteration 10804, loss: 0.0022933073341846466\n",
      "iteration 10805, loss: 0.003016912378370762\n",
      "iteration 10806, loss: 0.002475860295817256\n",
      "iteration 10807, loss: 0.002880947897210717\n",
      "iteration 10808, loss: 0.0023695840500295162\n",
      "iteration 10809, loss: 0.002393303904682398\n",
      "iteration 10810, loss: 0.0027504372410476208\n",
      "iteration 10811, loss: 0.0029290528036653996\n",
      "iteration 10812, loss: 0.0025396677665412426\n",
      "iteration 10813, loss: 0.002808729652315378\n",
      "iteration 10814, loss: 0.0020661079324781895\n",
      "iteration 10815, loss: 0.002476213499903679\n",
      "iteration 10816, loss: 0.002399404998868704\n",
      "iteration 10817, loss: 0.002406046260148287\n",
      "iteration 10818, loss: 0.0022817023564130068\n",
      "iteration 10819, loss: 0.00224511930719018\n",
      "iteration 10820, loss: 0.0023273976985365152\n",
      "iteration 10821, loss: 0.0026694494299590588\n",
      "iteration 10822, loss: 0.002791818231344223\n",
      "iteration 10823, loss: 0.0022175959311425686\n",
      "iteration 10824, loss: 0.0023620605934411287\n",
      "iteration 10825, loss: 0.0026150173507630825\n",
      "iteration 10826, loss: 0.0021345766726881266\n",
      "iteration 10827, loss: 0.0031140041537582874\n",
      "iteration 10828, loss: 0.002864460926502943\n",
      "iteration 10829, loss: 0.0025949343107640743\n",
      "iteration 10830, loss: 0.0024808445014059544\n",
      "iteration 10831, loss: 0.002990840235725045\n",
      "iteration 10832, loss: 0.0025399033911526203\n",
      "iteration 10833, loss: 0.003120329463854432\n",
      "iteration 10834, loss: 0.0020941123366355896\n",
      "iteration 10835, loss: 0.0023188707418739796\n",
      "iteration 10836, loss: 0.0025379350408911705\n",
      "iteration 10837, loss: 0.002644289517775178\n",
      "iteration 10838, loss: 0.002335004508495331\n",
      "iteration 10839, loss: 0.002552102319896221\n",
      "iteration 10840, loss: 0.002388913184404373\n",
      "iteration 10841, loss: 0.0030506118200719357\n",
      "iteration 10842, loss: 0.0030124178156256676\n",
      "iteration 10843, loss: 0.002480520401149988\n",
      "iteration 10844, loss: 0.0022822716273367405\n",
      "iteration 10845, loss: 0.002426397055387497\n",
      "iteration 10846, loss: 0.003110179677605629\n",
      "iteration 10847, loss: 0.003184817498549819\n",
      "iteration 10848, loss: 0.0021011317148804665\n",
      "iteration 10849, loss: 0.0026299324817955494\n",
      "iteration 10850, loss: 0.0025821970775723457\n",
      "iteration 10851, loss: 0.002973779570311308\n",
      "iteration 10852, loss: 0.002510612830519676\n",
      "iteration 10853, loss: 0.0026696498971432447\n",
      "iteration 10854, loss: 0.002913210541009903\n",
      "iteration 10855, loss: 0.002984329592436552\n",
      "iteration 10856, loss: 0.0026293413247913122\n",
      "iteration 10857, loss: 0.002118476666510105\n",
      "iteration 10858, loss: 0.0026881929952651262\n",
      "iteration 10859, loss: 0.002308979630470276\n",
      "iteration 10860, loss: 0.0026807261165231466\n",
      "iteration 10861, loss: 0.002200301270931959\n",
      "iteration 10862, loss: 0.002250965218991041\n",
      "iteration 10863, loss: 0.002342129359021783\n",
      "iteration 10864, loss: 0.0029998302925378084\n",
      "iteration 10865, loss: 0.002180209383368492\n",
      "iteration 10866, loss: 0.002839275635778904\n",
      "iteration 10867, loss: 0.002585076726973057\n",
      "iteration 10868, loss: 0.0022163107059895992\n",
      "iteration 10869, loss: 0.00224978756159544\n",
      "iteration 10870, loss: 0.002424785401672125\n",
      "iteration 10871, loss: 0.0024994765408337116\n",
      "iteration 10872, loss: 0.0026490637101233006\n",
      "iteration 10873, loss: 0.002540568821132183\n",
      "iteration 10874, loss: 0.00291091063991189\n",
      "iteration 10875, loss: 0.0022238981910049915\n",
      "iteration 10876, loss: 0.00250615063123405\n",
      "iteration 10877, loss: 0.0024855248630046844\n",
      "iteration 10878, loss: 0.002347088884562254\n",
      "iteration 10879, loss: 0.0030427048914134502\n",
      "iteration 10880, loss: 0.0031068960670381784\n",
      "iteration 10881, loss: 0.002411565277725458\n",
      "iteration 10882, loss: 0.0027868335600942373\n",
      "iteration 10883, loss: 0.0022638547234237194\n",
      "iteration 10884, loss: 0.002567057032138109\n",
      "iteration 10885, loss: 0.0025379029102623463\n",
      "iteration 10886, loss: 0.0025083792861551046\n",
      "iteration 10887, loss: 0.0022206862922757864\n",
      "iteration 10888, loss: 0.002236641012132168\n",
      "iteration 10889, loss: 0.0024349121376872063\n",
      "iteration 10890, loss: 0.0025139572098851204\n",
      "iteration 10891, loss: 0.002416755072772503\n",
      "iteration 10892, loss: 0.002374220872297883\n",
      "iteration 10893, loss: 0.0022637273650616407\n",
      "iteration 10894, loss: 0.0024653596337884665\n",
      "iteration 10895, loss: 0.002529917052015662\n",
      "iteration 10896, loss: 0.002204465214163065\n",
      "iteration 10897, loss: 0.0023887972347438335\n",
      "iteration 10898, loss: 0.00257589528337121\n",
      "iteration 10899, loss: 0.002289241412654519\n",
      "iteration 10900, loss: 0.001780622755177319\n",
      "iteration 10901, loss: 0.0027445415034890175\n",
      "iteration 10902, loss: 0.002632359741255641\n",
      "iteration 10903, loss: 0.0022170362062752247\n",
      "iteration 10904, loss: 0.002564529422670603\n",
      "iteration 10905, loss: 0.0029286385979503393\n",
      "iteration 10906, loss: 0.0020935486536473036\n",
      "iteration 10907, loss: 0.002407875843346119\n",
      "iteration 10908, loss: 0.0024296119809150696\n",
      "iteration 10909, loss: 0.002562909619882703\n",
      "iteration 10910, loss: 0.0025481656193733215\n",
      "iteration 10911, loss: 0.002496444620192051\n",
      "iteration 10912, loss: 0.002757728099822998\n",
      "iteration 10913, loss: 0.002625797176733613\n",
      "iteration 10914, loss: 0.0022267834283411503\n",
      "iteration 10915, loss: 0.002120134187862277\n",
      "iteration 10916, loss: 0.002680904231965542\n",
      "iteration 10917, loss: 0.002668796107172966\n",
      "iteration 10918, loss: 0.003291680011898279\n",
      "iteration 10919, loss: 0.002561229979619384\n",
      "iteration 10920, loss: 0.0023292358964681625\n",
      "iteration 10921, loss: 0.0023328312672674656\n",
      "iteration 10922, loss: 0.0022603943943977356\n",
      "iteration 10923, loss: 0.0022037900052964687\n",
      "iteration 10924, loss: 0.0024341270327568054\n",
      "iteration 10925, loss: 0.0025065597146749496\n",
      "iteration 10926, loss: 0.0028701629489660263\n",
      "iteration 10927, loss: 0.0026608072221279144\n",
      "iteration 10928, loss: 0.002597013022750616\n",
      "iteration 10929, loss: 0.002472763881087303\n",
      "iteration 10930, loss: 0.002589175943285227\n",
      "iteration 10931, loss: 0.002381269820034504\n",
      "iteration 10932, loss: 0.002576023107394576\n",
      "iteration 10933, loss: 0.002540840767323971\n",
      "iteration 10934, loss: 0.0025011703837662935\n",
      "iteration 10935, loss: 0.0024978239089250565\n",
      "iteration 10936, loss: 0.0020986590534448624\n",
      "iteration 10937, loss: 0.0025378784630447626\n",
      "iteration 10938, loss: 0.00264171464368701\n",
      "iteration 10939, loss: 0.0025709958281368017\n",
      "iteration 10940, loss: 0.0035301316529512405\n",
      "iteration 10941, loss: 0.0022661741822957993\n",
      "iteration 10942, loss: 0.0029034873005002737\n",
      "iteration 10943, loss: 0.002559646964073181\n",
      "iteration 10944, loss: 0.0023064655251801014\n",
      "iteration 10945, loss: 0.0022722803987562656\n",
      "iteration 10946, loss: 0.0025450224056839943\n",
      "iteration 10947, loss: 0.002395816845819354\n",
      "iteration 10948, loss: 0.0020228158682584763\n",
      "iteration 10949, loss: 0.0021862462162971497\n",
      "iteration 10950, loss: 0.0022198462393134832\n",
      "iteration 10951, loss: 0.0024926424957811832\n",
      "iteration 10952, loss: 0.0020554494112730026\n",
      "iteration 10953, loss: 0.0025005750358104706\n",
      "iteration 10954, loss: 0.002260467503219843\n",
      "iteration 10955, loss: 0.0021477090194821358\n",
      "iteration 10956, loss: 0.0024487676564604044\n",
      "iteration 10957, loss: 0.002321382984519005\n",
      "iteration 10958, loss: 0.002021840075030923\n",
      "iteration 10959, loss: 0.0026134669315069914\n",
      "iteration 10960, loss: 0.002578216139227152\n",
      "iteration 10961, loss: 0.0023809135891497135\n",
      "iteration 10962, loss: 0.0028851223178207874\n",
      "iteration 10963, loss: 0.0025375178083777428\n",
      "iteration 10964, loss: 0.0026475158520042896\n",
      "iteration 10965, loss: 0.0022839833982288837\n",
      "iteration 10966, loss: 0.0025812326930463314\n",
      "iteration 10967, loss: 0.0027254465967416763\n",
      "iteration 10968, loss: 0.0023722443729639053\n",
      "iteration 10969, loss: 0.002562458859756589\n",
      "iteration 10970, loss: 0.002743898658081889\n",
      "iteration 10971, loss: 0.0026907739229500294\n",
      "iteration 10972, loss: 0.002610797295346856\n",
      "iteration 10973, loss: 0.0024821495171636343\n",
      "iteration 10974, loss: 0.0030572242103517056\n",
      "iteration 10975, loss: 0.0024931919761002064\n",
      "iteration 10976, loss: 0.0023986687883734703\n",
      "iteration 10977, loss: 0.0021795465145260096\n",
      "iteration 10978, loss: 0.0024188836105167866\n",
      "iteration 10979, loss: 0.0021111774258315563\n",
      "iteration 10980, loss: 0.002280743792653084\n",
      "iteration 10981, loss: 0.0024044234305620193\n",
      "iteration 10982, loss: 0.0019016958540305495\n",
      "iteration 10983, loss: 0.0026696454733610153\n",
      "iteration 10984, loss: 0.002569075906649232\n",
      "iteration 10985, loss: 0.002522288588806987\n",
      "iteration 10986, loss: 0.0023724352940917015\n",
      "iteration 10987, loss: 0.0027761422097682953\n",
      "iteration 10988, loss: 0.0020545015577226877\n",
      "iteration 10989, loss: 0.002449424471706152\n",
      "iteration 10990, loss: 0.002285782713443041\n",
      "iteration 10991, loss: 0.002738886047154665\n",
      "iteration 10992, loss: 0.0024183522909879684\n",
      "iteration 10993, loss: 0.0023545927833765745\n",
      "iteration 10994, loss: 0.002610105089843273\n",
      "iteration 10995, loss: 0.0020592431537806988\n",
      "iteration 10996, loss: 0.0023901653476059437\n",
      "iteration 10997, loss: 0.0026407944969832897\n",
      "iteration 10998, loss: 0.00261083897203207\n",
      "iteration 10999, loss: 0.00317175779491663\n",
      "iteration 11000, loss: 0.0027270610444247723\n",
      "iteration 11001, loss: 0.0019927998073399067\n",
      "iteration 11002, loss: 0.0022739539854228497\n",
      "iteration 11003, loss: 0.0024578578304499388\n",
      "iteration 11004, loss: 0.002323094056919217\n",
      "iteration 11005, loss: 0.0020442293025553226\n",
      "iteration 11006, loss: 0.002514971885830164\n",
      "iteration 11007, loss: 0.0028482167981565\n",
      "iteration 11008, loss: 0.002117432653903961\n",
      "iteration 11009, loss: 0.002363905543461442\n",
      "iteration 11010, loss: 0.0023932773619890213\n",
      "iteration 11011, loss: 0.0022114841267466545\n",
      "iteration 11012, loss: 0.0024959309957921505\n",
      "iteration 11013, loss: 0.0025761870201677084\n",
      "iteration 11014, loss: 0.0020126968156546354\n",
      "iteration 11015, loss: 0.0026314666029065847\n",
      "iteration 11016, loss: 0.0025841030292212963\n",
      "iteration 11017, loss: 0.003107242751866579\n",
      "iteration 11018, loss: 0.002315483521670103\n",
      "iteration 11019, loss: 0.0025960709899663925\n",
      "iteration 11020, loss: 0.0024116598069667816\n",
      "iteration 11021, loss: 0.00262481183744967\n",
      "iteration 11022, loss: 0.0022542872466146946\n",
      "iteration 11023, loss: 0.002410850953310728\n",
      "iteration 11024, loss: 0.0027279097121208906\n",
      "iteration 11025, loss: 0.0028291321359574795\n",
      "iteration 11026, loss: 0.002543048933148384\n",
      "iteration 11027, loss: 0.0016821818426251411\n",
      "iteration 11028, loss: 0.0021673720329999924\n",
      "iteration 11029, loss: 0.0025301449932157993\n",
      "iteration 11030, loss: 0.0021840925328433514\n",
      "iteration 11031, loss: 0.0025684204883873463\n",
      "iteration 11032, loss: 0.002351380418986082\n",
      "iteration 11033, loss: 0.0026186536997556686\n",
      "iteration 11034, loss: 0.002141285687685013\n",
      "iteration 11035, loss: 0.0023890419397503138\n",
      "iteration 11036, loss: 0.0026252283714711666\n",
      "iteration 11037, loss: 0.002931238617748022\n",
      "iteration 11038, loss: 0.002667074790224433\n",
      "iteration 11039, loss: 0.0020856065675616264\n",
      "iteration 11040, loss: 0.002838142216205597\n",
      "iteration 11041, loss: 0.002526515629142523\n",
      "iteration 11042, loss: 0.0022557578049600124\n",
      "iteration 11043, loss: 0.002716996241360903\n",
      "iteration 11044, loss: 0.0025444859638810158\n",
      "iteration 11045, loss: 0.002312691882252693\n",
      "iteration 11046, loss: 0.0022287569008767605\n",
      "iteration 11047, loss: 0.002321091014891863\n",
      "iteration 11048, loss: 0.002596660517156124\n",
      "iteration 11049, loss: 0.0026877557393163443\n",
      "iteration 11050, loss: 0.0021824378054589033\n",
      "iteration 11051, loss: 0.002529784804210067\n",
      "iteration 11052, loss: 0.0022400307934731245\n",
      "iteration 11053, loss: 0.0028696644585579634\n",
      "iteration 11054, loss: 0.0026403572410345078\n",
      "iteration 11055, loss: 0.0028015782590955496\n",
      "iteration 11056, loss: 0.002098662778735161\n",
      "iteration 11057, loss: 0.002560392487794161\n",
      "iteration 11058, loss: 0.0020798868499696255\n",
      "iteration 11059, loss: 0.002194121479988098\n",
      "iteration 11060, loss: 0.0025256986264139414\n",
      "iteration 11061, loss: 0.001996691105887294\n",
      "iteration 11062, loss: 0.0025420421734452248\n",
      "iteration 11063, loss: 0.0026553913485258818\n",
      "iteration 11064, loss: 0.0024233832955360413\n",
      "iteration 11065, loss: 0.0022380161099135876\n",
      "iteration 11066, loss: 0.001890690647996962\n",
      "iteration 11067, loss: 0.0025278422981500626\n",
      "iteration 11068, loss: 0.0025764170568436384\n",
      "iteration 11069, loss: 0.0023304293863475323\n",
      "iteration 11070, loss: 0.0019749037455767393\n",
      "iteration 11071, loss: 0.002460107207298279\n",
      "iteration 11072, loss: 0.0024973403196781874\n",
      "iteration 11073, loss: 0.002531093778088689\n",
      "iteration 11074, loss: 0.002299231942743063\n",
      "iteration 11075, loss: 0.003011671593412757\n",
      "iteration 11076, loss: 0.002193443477153778\n",
      "iteration 11077, loss: 0.0022846809588372707\n",
      "iteration 11078, loss: 0.002183659467846155\n",
      "iteration 11079, loss: 0.002282671630382538\n",
      "iteration 11080, loss: 0.002326015383005142\n",
      "iteration 11081, loss: 0.001989654963836074\n",
      "iteration 11082, loss: 0.0022371967788785696\n",
      "iteration 11083, loss: 0.002551110927015543\n",
      "iteration 11084, loss: 0.0021739541552960873\n",
      "iteration 11085, loss: 0.0027516894042491913\n",
      "iteration 11086, loss: 0.0023898049257695675\n",
      "iteration 11087, loss: 0.002202361822128296\n",
      "iteration 11088, loss: 0.002062865998595953\n",
      "iteration 11089, loss: 0.0026588742621243\n",
      "iteration 11090, loss: 0.002013758523389697\n",
      "iteration 11091, loss: 0.0024125119671225548\n",
      "iteration 11092, loss: 0.0023728557862341404\n",
      "iteration 11093, loss: 0.0023433743044734\n",
      "iteration 11094, loss: 0.0024328259751200676\n",
      "iteration 11095, loss: 0.0019563655368983746\n",
      "iteration 11096, loss: 0.0021952560637146235\n",
      "iteration 11097, loss: 0.0024990509264171124\n",
      "iteration 11098, loss: 0.0019235174404457211\n",
      "iteration 11099, loss: 0.0023696094285696745\n",
      "iteration 11100, loss: 0.0025643999688327312\n",
      "iteration 11101, loss: 0.0018256523180752993\n",
      "iteration 11102, loss: 0.0026049341540783644\n",
      "iteration 11103, loss: 0.0020274794660508633\n",
      "iteration 11104, loss: 0.002685591811314225\n",
      "iteration 11105, loss: 0.002458821050822735\n",
      "iteration 11106, loss: 0.002753174863755703\n",
      "iteration 11107, loss: 0.0025975052267313004\n",
      "iteration 11108, loss: 0.002110016532242298\n",
      "iteration 11109, loss: 0.002898712642490864\n",
      "iteration 11110, loss: 0.002851841039955616\n",
      "iteration 11111, loss: 0.0025732414796948433\n",
      "iteration 11112, loss: 0.0024874359369277954\n",
      "iteration 11113, loss: 0.0020718956366181374\n",
      "iteration 11114, loss: 0.0022502816282212734\n",
      "iteration 11115, loss: 0.002565608825534582\n",
      "iteration 11116, loss: 0.002306295558810234\n",
      "iteration 11117, loss: 0.002104128710925579\n",
      "iteration 11118, loss: 0.0018754592165350914\n",
      "iteration 11119, loss: 0.002743772231042385\n",
      "iteration 11120, loss: 0.002132697496563196\n",
      "iteration 11121, loss: 0.0021063820458948612\n",
      "iteration 11122, loss: 0.0024208910763263702\n",
      "iteration 11123, loss: 0.0024496293626725674\n",
      "iteration 11124, loss: 0.002956768963485956\n",
      "iteration 11125, loss: 0.0020462656393647194\n",
      "iteration 11126, loss: 0.0021466221660375595\n",
      "iteration 11127, loss: 0.0024818943347781897\n",
      "iteration 11128, loss: 0.002457716967910528\n",
      "iteration 11129, loss: 0.0021522697061300278\n",
      "iteration 11130, loss: 0.002376787131652236\n",
      "iteration 11131, loss: 0.002423003548756242\n",
      "iteration 11132, loss: 0.0024716900661587715\n",
      "iteration 11133, loss: 0.002385997213423252\n",
      "iteration 11134, loss: 0.0026326519437134266\n",
      "iteration 11135, loss: 0.003056042594835162\n",
      "iteration 11136, loss: 0.002924526110291481\n",
      "iteration 11137, loss: 0.00223292363807559\n",
      "iteration 11138, loss: 0.0028390963561832905\n",
      "iteration 11139, loss: 0.0024696707259863615\n",
      "iteration 11140, loss: 0.0029576169326901436\n",
      "iteration 11141, loss: 0.0027204640209674835\n",
      "iteration 11142, loss: 0.002845369279384613\n",
      "iteration 11143, loss: 0.0023794351145625114\n",
      "iteration 11144, loss: 0.0025642113760113716\n",
      "iteration 11145, loss: 0.0028976446483284235\n",
      "iteration 11146, loss: 0.0027548421639949083\n",
      "iteration 11147, loss: 0.0024121946189552546\n",
      "iteration 11148, loss: 0.002400769852101803\n",
      "iteration 11149, loss: 0.003122007939964533\n",
      "iteration 11150, loss: 0.0028628455474972725\n",
      "iteration 11151, loss: 0.0027098562568426132\n",
      "iteration 11152, loss: 0.0025472785346210003\n",
      "iteration 11153, loss: 0.002544993767514825\n",
      "iteration 11154, loss: 0.002675251569598913\n",
      "iteration 11155, loss: 0.0026833494193851948\n",
      "iteration 11156, loss: 0.0022551713045686483\n",
      "iteration 11157, loss: 0.002577925566583872\n",
      "iteration 11158, loss: 0.002259467728435993\n",
      "iteration 11159, loss: 0.00250557204708457\n",
      "iteration 11160, loss: 0.0026534085627645254\n",
      "iteration 11161, loss: 0.0023721151519566774\n",
      "iteration 11162, loss: 0.0024088602513074875\n",
      "iteration 11163, loss: 0.0018257140181958675\n",
      "iteration 11164, loss: 0.0021101473830640316\n",
      "iteration 11165, loss: 0.002425332320854068\n",
      "iteration 11166, loss: 0.001972686965018511\n",
      "iteration 11167, loss: 0.002521608956158161\n",
      "iteration 11168, loss: 0.0019379553850740194\n",
      "iteration 11169, loss: 0.002037400845438242\n",
      "iteration 11170, loss: 0.0023164567537605762\n",
      "iteration 11171, loss: 0.0022017713636159897\n",
      "iteration 11172, loss: 0.002207669895142317\n",
      "iteration 11173, loss: 0.002399330958724022\n",
      "iteration 11174, loss: 0.0023779268376529217\n",
      "iteration 11175, loss: 0.0022869082167744637\n",
      "iteration 11176, loss: 0.0023714876733720303\n",
      "iteration 11177, loss: 0.00175484421197325\n",
      "iteration 11178, loss: 0.002212804974988103\n",
      "iteration 11179, loss: 0.0020989621989428997\n",
      "iteration 11180, loss: 0.00288763758726418\n",
      "iteration 11181, loss: 0.00281707476824522\n",
      "iteration 11182, loss: 0.0022344919852912426\n",
      "iteration 11183, loss: 0.002723934128880501\n",
      "iteration 11184, loss: 0.0023088662419468164\n",
      "iteration 11185, loss: 0.0021369941532611847\n",
      "iteration 11186, loss: 0.0021231845021247864\n",
      "iteration 11187, loss: 0.002499829977750778\n",
      "iteration 11188, loss: 0.0020437208004295826\n",
      "iteration 11189, loss: 0.002198129426687956\n",
      "iteration 11190, loss: 0.0020512226037681103\n",
      "iteration 11191, loss: 0.0025373860262334347\n",
      "iteration 11192, loss: 0.00229342607781291\n",
      "iteration 11193, loss: 0.0021089017391204834\n",
      "iteration 11194, loss: 0.0020058928057551384\n",
      "iteration 11195, loss: 0.002185887424275279\n",
      "iteration 11196, loss: 0.0023388760164380074\n",
      "iteration 11197, loss: 0.0027321691159158945\n",
      "iteration 11198, loss: 0.001975767547264695\n",
      "iteration 11199, loss: 0.0021510475780814886\n",
      "iteration 11200, loss: 0.0019162188982591033\n",
      "iteration 11201, loss: 0.0019361770246177912\n",
      "iteration 11202, loss: 0.002718733623623848\n",
      "iteration 11203, loss: 0.0020819101482629776\n",
      "iteration 11204, loss: 0.00248298654332757\n",
      "iteration 11205, loss: 0.002541743218898773\n",
      "iteration 11206, loss: 0.002159410621970892\n",
      "iteration 11207, loss: 0.0021391804330050945\n",
      "iteration 11208, loss: 0.0022344612516462803\n",
      "iteration 11209, loss: 0.002284604124724865\n",
      "iteration 11210, loss: 0.001958003966137767\n",
      "iteration 11211, loss: 0.002609382849186659\n",
      "iteration 11212, loss: 0.002997316885739565\n",
      "iteration 11213, loss: 0.0023735917638987303\n",
      "iteration 11214, loss: 0.0024545234628021717\n",
      "iteration 11215, loss: 0.002479823073372245\n",
      "iteration 11216, loss: 0.0022763535380363464\n",
      "iteration 11217, loss: 0.002468160120770335\n",
      "iteration 11218, loss: 0.0028785071335732937\n",
      "iteration 11219, loss: 0.002423292025923729\n",
      "iteration 11220, loss: 0.002003459259867668\n",
      "iteration 11221, loss: 0.0026383143849670887\n",
      "iteration 11222, loss: 0.002458152361214161\n",
      "iteration 11223, loss: 0.0023158895783126354\n",
      "iteration 11224, loss: 0.0024853991344571114\n",
      "iteration 11225, loss: 0.0025223372504115105\n",
      "iteration 11226, loss: 0.002437098417431116\n",
      "iteration 11227, loss: 0.002301231725141406\n",
      "iteration 11228, loss: 0.0026680983137339354\n",
      "iteration 11229, loss: 0.0024899805430322886\n",
      "iteration 11230, loss: 0.0024216794408857822\n",
      "iteration 11231, loss: 0.002183518372476101\n",
      "iteration 11232, loss: 0.0030456746462732553\n",
      "iteration 11233, loss: 0.0019542125519365072\n",
      "iteration 11234, loss: 0.0025632281322032213\n",
      "iteration 11235, loss: 0.002714091446250677\n",
      "iteration 11236, loss: 0.002641064580529928\n",
      "iteration 11237, loss: 0.001992675242945552\n",
      "iteration 11238, loss: 0.0020555579103529453\n",
      "iteration 11239, loss: 0.0020650727674365044\n",
      "iteration 11240, loss: 0.00218741362914443\n",
      "iteration 11241, loss: 0.0022990310098975897\n",
      "iteration 11242, loss: 0.002201092429459095\n",
      "iteration 11243, loss: 0.0023788842372596264\n",
      "iteration 11244, loss: 0.0020828242413699627\n",
      "iteration 11245, loss: 0.0023520973045378923\n",
      "iteration 11246, loss: 0.0021448740735650063\n",
      "iteration 11247, loss: 0.002523231552913785\n",
      "iteration 11248, loss: 0.002132858382537961\n",
      "iteration 11249, loss: 0.0018988035153597593\n",
      "iteration 11250, loss: 0.002514779567718506\n",
      "iteration 11251, loss: 0.002294576959684491\n",
      "iteration 11252, loss: 0.002228670520707965\n",
      "iteration 11253, loss: 0.0023492639884352684\n",
      "iteration 11254, loss: 0.002235970925539732\n",
      "iteration 11255, loss: 0.002108408836647868\n",
      "iteration 11256, loss: 0.0021916793193668127\n",
      "iteration 11257, loss: 0.0023615951649844646\n",
      "iteration 11258, loss: 0.0021831332705914974\n",
      "iteration 11259, loss: 0.0020315018482506275\n",
      "iteration 11260, loss: 0.0022959932684898376\n",
      "iteration 11261, loss: 0.002737462054938078\n",
      "iteration 11262, loss: 0.0017354111187160015\n",
      "iteration 11263, loss: 0.002673861337825656\n",
      "iteration 11264, loss: 0.00232263607904315\n",
      "iteration 11265, loss: 0.0021483683958649635\n",
      "iteration 11266, loss: 0.002211192389950156\n",
      "iteration 11267, loss: 0.0022710233461111784\n",
      "iteration 11268, loss: 0.002138234442099929\n",
      "iteration 11269, loss: 0.001973183825612068\n",
      "iteration 11270, loss: 0.002161168958991766\n",
      "iteration 11271, loss: 0.0024947822093963623\n",
      "iteration 11272, loss: 0.002615836216136813\n",
      "iteration 11273, loss: 0.0026148578617721796\n",
      "iteration 11274, loss: 0.002448129002004862\n",
      "iteration 11275, loss: 0.0019782809540629387\n",
      "iteration 11276, loss: 0.002290966920554638\n",
      "iteration 11277, loss: 0.002231529913842678\n",
      "iteration 11278, loss: 0.0024329149164259434\n",
      "iteration 11279, loss: 0.0023906321730464697\n",
      "iteration 11280, loss: 0.0029777397867292166\n",
      "iteration 11281, loss: 0.0023919972591102123\n",
      "iteration 11282, loss: 0.002573116682469845\n",
      "iteration 11283, loss: 0.0024112919345498085\n",
      "iteration 11284, loss: 0.0022296516690403223\n",
      "iteration 11285, loss: 0.0023449501022696495\n",
      "iteration 11286, loss: 0.0023374068550765514\n",
      "iteration 11287, loss: 0.002136342693120241\n",
      "iteration 11288, loss: 0.002164025790989399\n",
      "iteration 11289, loss: 0.0022195721976459026\n",
      "iteration 11290, loss: 0.0019474297296255827\n",
      "iteration 11291, loss: 0.002410563174635172\n",
      "iteration 11292, loss: 0.0027719473000615835\n",
      "iteration 11293, loss: 0.0025739753618836403\n",
      "iteration 11294, loss: 0.002188402460888028\n",
      "iteration 11295, loss: 0.0021592990960925817\n",
      "iteration 11296, loss: 0.002097717020660639\n",
      "iteration 11297, loss: 0.002292283810675144\n",
      "iteration 11298, loss: 0.002035942394286394\n",
      "iteration 11299, loss: 0.003076781053096056\n",
      "iteration 11300, loss: 0.0018607471138238907\n",
      "iteration 11301, loss: 0.0028377000708132982\n",
      "iteration 11302, loss: 0.0024485921021550894\n",
      "iteration 11303, loss: 0.0026516360230743885\n",
      "iteration 11304, loss: 0.0019757200498133898\n",
      "iteration 11305, loss: 0.002449577674269676\n",
      "iteration 11306, loss: 0.0022624582052230835\n",
      "iteration 11307, loss: 0.0025589941069483757\n",
      "iteration 11308, loss: 0.00220510084182024\n",
      "iteration 11309, loss: 0.002388064283877611\n",
      "iteration 11310, loss: 0.0026598863769322634\n",
      "iteration 11311, loss: 0.002591290045529604\n",
      "iteration 11312, loss: 0.002523236908018589\n",
      "iteration 11313, loss: 0.0027797631919384003\n",
      "iteration 11314, loss: 0.0024256017059087753\n",
      "iteration 11315, loss: 0.002345504704862833\n",
      "iteration 11316, loss: 0.00256496574729681\n",
      "iteration 11317, loss: 0.0026028768625110388\n",
      "iteration 11318, loss: 0.002492363564670086\n",
      "iteration 11319, loss: 0.0027862731367349625\n",
      "iteration 11320, loss: 0.0027098183054476976\n",
      "iteration 11321, loss: 0.0022157456260174513\n",
      "iteration 11322, loss: 0.0026989192701876163\n",
      "iteration 11323, loss: 0.002466384321451187\n",
      "iteration 11324, loss: 0.00208613951690495\n",
      "iteration 11325, loss: 0.0027456965763121843\n",
      "iteration 11326, loss: 0.002499463502317667\n",
      "iteration 11327, loss: 0.002181360498070717\n",
      "iteration 11328, loss: 0.0027852458879351616\n",
      "iteration 11329, loss: 0.0023002061061561108\n",
      "iteration 11330, loss: 0.0017722384072840214\n",
      "iteration 11331, loss: 0.002500003669410944\n",
      "iteration 11332, loss: 0.0021842853166162968\n",
      "iteration 11333, loss: 0.0022301324643194675\n",
      "iteration 11334, loss: 0.0021106661297380924\n",
      "iteration 11335, loss: 0.0017755762673914433\n",
      "iteration 11336, loss: 0.0022080191411077976\n",
      "iteration 11337, loss: 0.002005169168114662\n",
      "iteration 11338, loss: 0.002517217770218849\n",
      "iteration 11339, loss: 0.0023012757301330566\n",
      "iteration 11340, loss: 0.0019431577529758215\n",
      "iteration 11341, loss: 0.0024883332662284374\n",
      "iteration 11342, loss: 0.0024536028504371643\n",
      "iteration 11343, loss: 0.0024567958898842335\n",
      "iteration 11344, loss: 0.0021998423617333174\n",
      "iteration 11345, loss: 0.002329451497644186\n",
      "iteration 11346, loss: 0.002483963966369629\n",
      "iteration 11347, loss: 0.002480308059602976\n",
      "iteration 11348, loss: 0.002235004212707281\n",
      "iteration 11349, loss: 0.0028948932886123657\n",
      "iteration 11350, loss: 0.0021200990304350853\n",
      "iteration 11351, loss: 0.0028363754972815514\n",
      "iteration 11352, loss: 0.0018869219347834587\n",
      "iteration 11353, loss: 0.002378313336521387\n",
      "iteration 11354, loss: 0.002120708581060171\n",
      "iteration 11355, loss: 0.002611409407109022\n",
      "iteration 11356, loss: 0.0023401062935590744\n",
      "iteration 11357, loss: 0.0023758888710290194\n",
      "iteration 11358, loss: 0.0025271843187510967\n",
      "iteration 11359, loss: 0.0028098332695662975\n",
      "iteration 11360, loss: 0.0017453477485105395\n",
      "iteration 11361, loss: 0.0025427560321986675\n",
      "iteration 11362, loss: 0.002388266148045659\n",
      "iteration 11363, loss: 0.0020606755279004574\n",
      "iteration 11364, loss: 0.0029153067152947187\n",
      "iteration 11365, loss: 0.0027183403726667166\n",
      "iteration 11366, loss: 0.002454583067446947\n",
      "iteration 11367, loss: 0.0027399356476962566\n",
      "iteration 11368, loss: 0.002659002784639597\n",
      "iteration 11369, loss: 0.002816056367009878\n",
      "iteration 11370, loss: 0.002269072225317359\n",
      "iteration 11371, loss: 0.0029106480069458485\n",
      "iteration 11372, loss: 0.002430144464597106\n",
      "iteration 11373, loss: 0.002323391381651163\n",
      "iteration 11374, loss: 0.002397220116108656\n",
      "iteration 11375, loss: 0.0023397449404001236\n",
      "iteration 11376, loss: 0.0023139482364058495\n",
      "iteration 11377, loss: 0.0021429250482469797\n",
      "iteration 11378, loss: 0.0022499440237879753\n",
      "iteration 11379, loss: 0.002232395811006427\n",
      "iteration 11380, loss: 0.0021407324820756912\n",
      "iteration 11381, loss: 0.002240345347672701\n",
      "iteration 11382, loss: 0.0024259903002530336\n",
      "iteration 11383, loss: 0.0023871399462223053\n",
      "iteration 11384, loss: 0.002550688572227955\n",
      "iteration 11385, loss: 0.002219951944425702\n",
      "iteration 11386, loss: 0.0022027369122952223\n",
      "iteration 11387, loss: 0.0018918549176305532\n",
      "iteration 11388, loss: 0.0025085387751460075\n",
      "iteration 11389, loss: 0.002204762538895011\n",
      "iteration 11390, loss: 0.002869468880817294\n",
      "iteration 11391, loss: 0.0020620650611817837\n",
      "iteration 11392, loss: 0.0023420474026352167\n",
      "iteration 11393, loss: 0.002144799567759037\n",
      "iteration 11394, loss: 0.002065683016553521\n",
      "iteration 11395, loss: 0.002450193278491497\n",
      "iteration 11396, loss: 0.002460245043039322\n",
      "iteration 11397, loss: 0.002008940791711211\n",
      "iteration 11398, loss: 0.002056701108813286\n",
      "iteration 11399, loss: 0.0024174265563488007\n",
      "iteration 11400, loss: 0.002490247366949916\n",
      "iteration 11401, loss: 0.002046361332759261\n",
      "iteration 11402, loss: 0.0022254171781241894\n",
      "iteration 11403, loss: 0.0020550889894366264\n",
      "iteration 11404, loss: 0.002293808851391077\n",
      "iteration 11405, loss: 0.002364733023568988\n",
      "iteration 11406, loss: 0.002818441716954112\n",
      "iteration 11407, loss: 0.002344942418858409\n",
      "iteration 11408, loss: 0.0024356653448194265\n",
      "iteration 11409, loss: 0.0023742711637169123\n",
      "iteration 11410, loss: 0.0023584426380693913\n",
      "iteration 11411, loss: 0.0019891210831701756\n",
      "iteration 11412, loss: 0.0028836303390562534\n",
      "iteration 11413, loss: 0.002446748549118638\n",
      "iteration 11414, loss: 0.002491691382601857\n",
      "iteration 11415, loss: 0.0020921595860272646\n",
      "iteration 11416, loss: 0.002484724158421159\n",
      "iteration 11417, loss: 0.0021959939040243626\n",
      "iteration 11418, loss: 0.002990055363625288\n",
      "iteration 11419, loss: 0.0028973394073545933\n",
      "iteration 11420, loss: 0.002478651702404022\n",
      "iteration 11421, loss: 0.0028123834636062384\n",
      "iteration 11422, loss: 0.002269549062475562\n",
      "iteration 11423, loss: 0.0024525229819118977\n",
      "iteration 11424, loss: 0.00293575506657362\n",
      "iteration 11425, loss: 0.0021727620624005795\n",
      "iteration 11426, loss: 0.0027661770582199097\n",
      "iteration 11427, loss: 0.002892551012337208\n",
      "iteration 11428, loss: 0.0025629729498177767\n",
      "iteration 11429, loss: 0.0021490035578608513\n",
      "iteration 11430, loss: 0.0023459051735699177\n",
      "iteration 11431, loss: 0.00287716556340456\n",
      "iteration 11432, loss: 0.0020037393551319838\n",
      "iteration 11433, loss: 0.002631149487569928\n",
      "iteration 11434, loss: 0.001987246097996831\n",
      "iteration 11435, loss: 0.0025003536138683558\n",
      "iteration 11436, loss: 0.0029846325051039457\n",
      "iteration 11437, loss: 0.0022724615409970284\n",
      "iteration 11438, loss: 0.0024592229165136814\n",
      "iteration 11439, loss: 0.002250827383249998\n",
      "iteration 11440, loss: 0.0023082406260073185\n",
      "iteration 11441, loss: 0.0025021652691066265\n",
      "iteration 11442, loss: 0.002297906670719385\n",
      "iteration 11443, loss: 0.002417116891592741\n",
      "iteration 11444, loss: 0.002285162452608347\n",
      "iteration 11445, loss: 0.002230204176157713\n",
      "iteration 11446, loss: 0.002918580546975136\n",
      "iteration 11447, loss: 0.0024704798124730587\n",
      "iteration 11448, loss: 0.002093758899718523\n",
      "iteration 11449, loss: 0.002128016669303179\n",
      "iteration 11450, loss: 0.001987186726182699\n",
      "iteration 11451, loss: 0.001782663632184267\n",
      "iteration 11452, loss: 0.002296880818903446\n",
      "iteration 11453, loss: 0.0021755914203822613\n",
      "iteration 11454, loss: 0.002199282869696617\n",
      "iteration 11455, loss: 0.002437019720673561\n",
      "iteration 11456, loss: 0.001887288992293179\n",
      "iteration 11457, loss: 0.002159298863261938\n",
      "iteration 11458, loss: 0.0021333876065909863\n",
      "iteration 11459, loss: 0.0023842500522732735\n",
      "iteration 11460, loss: 0.0017493123887106776\n",
      "iteration 11461, loss: 0.0026711286045610905\n",
      "iteration 11462, loss: 0.0026128932368010283\n",
      "iteration 11463, loss: 0.0023957169614732265\n",
      "iteration 11464, loss: 0.0029196562245488167\n",
      "iteration 11465, loss: 0.0024302350357174873\n",
      "iteration 11466, loss: 0.002152410103008151\n",
      "iteration 11467, loss: 0.0026141086127609015\n",
      "iteration 11468, loss: 0.0019685649313032627\n",
      "iteration 11469, loss: 0.002206837758421898\n",
      "iteration 11470, loss: 0.002344959182664752\n",
      "iteration 11471, loss: 0.0023636019323021173\n",
      "iteration 11472, loss: 0.0023460278753191233\n",
      "iteration 11473, loss: 0.0021133520640432835\n",
      "iteration 11474, loss: 0.0018418808467686176\n",
      "iteration 11475, loss: 0.0019238016102463007\n",
      "iteration 11476, loss: 0.002440056763589382\n",
      "iteration 11477, loss: 0.0026887175627052784\n",
      "iteration 11478, loss: 0.00259700370952487\n",
      "iteration 11479, loss: 0.0023071165196597576\n",
      "iteration 11480, loss: 0.002050455193966627\n",
      "iteration 11481, loss: 0.002397159580141306\n",
      "iteration 11482, loss: 0.002494960092008114\n",
      "iteration 11483, loss: 0.002695709001272917\n",
      "iteration 11484, loss: 0.002509648911654949\n",
      "iteration 11485, loss: 0.002115667099133134\n",
      "iteration 11486, loss: 0.0024644937366247177\n",
      "iteration 11487, loss: 0.0022015711292624474\n",
      "iteration 11488, loss: 0.0025458121672272682\n",
      "iteration 11489, loss: 0.0025573535822331905\n",
      "iteration 11490, loss: 0.002202071947976947\n",
      "iteration 11491, loss: 0.0022823289036750793\n",
      "iteration 11492, loss: 0.002035330981016159\n",
      "iteration 11493, loss: 0.0023635460529476404\n",
      "iteration 11494, loss: 0.0021078092977404594\n",
      "iteration 11495, loss: 0.0022908765822649\n",
      "iteration 11496, loss: 0.002100640442222357\n",
      "iteration 11497, loss: 0.002865054178982973\n",
      "iteration 11498, loss: 0.0024750777520239353\n",
      "iteration 11499, loss: 0.0021653606090694666\n",
      "iteration 11500, loss: 0.0029579629190266132\n",
      "iteration 11501, loss: 0.0026036191266030073\n",
      "iteration 11502, loss: 0.002480115042999387\n",
      "iteration 11503, loss: 0.00210657250136137\n",
      "iteration 11504, loss: 0.002471008338034153\n",
      "iteration 11505, loss: 0.001983408583328128\n",
      "iteration 11506, loss: 0.0024082192685455084\n",
      "iteration 11507, loss: 0.0024471222423017025\n",
      "iteration 11508, loss: 0.002260917332023382\n",
      "iteration 11509, loss: 0.002625637222081423\n",
      "iteration 11510, loss: 0.002145606791600585\n",
      "iteration 11511, loss: 0.002424377715215087\n",
      "iteration 11512, loss: 0.002209056867286563\n",
      "iteration 11513, loss: 0.002673669718205929\n",
      "iteration 11514, loss: 0.0022653257474303246\n",
      "iteration 11515, loss: 0.0023408494889736176\n",
      "iteration 11516, loss: 0.0025260059628635645\n",
      "iteration 11517, loss: 0.0025049294345080853\n",
      "iteration 11518, loss: 0.0022633152548223734\n",
      "iteration 11519, loss: 0.0026658903807401657\n",
      "iteration 11520, loss: 0.0020126928575336933\n",
      "iteration 11521, loss: 0.0020080427639186382\n",
      "iteration 11522, loss: 0.0023344901856034994\n",
      "iteration 11523, loss: 0.002084624720737338\n",
      "iteration 11524, loss: 0.0023111626505851746\n",
      "iteration 11525, loss: 0.0024386090226471424\n",
      "iteration 11526, loss: 0.0022050512488931417\n",
      "iteration 11527, loss: 0.0026063392870128155\n",
      "iteration 11528, loss: 0.0024087203200906515\n",
      "iteration 11529, loss: 0.0021601065527647734\n",
      "iteration 11530, loss: 0.0024683871306478977\n",
      "iteration 11531, loss: 0.00225513381883502\n",
      "iteration 11532, loss: 0.0022330156061798334\n",
      "iteration 11533, loss: 0.0022653958294540644\n",
      "iteration 11534, loss: 0.0021785302087664604\n",
      "iteration 11535, loss: 0.002299643587321043\n",
      "iteration 11536, loss: 0.0020692278631031513\n",
      "iteration 11537, loss: 0.0027178963646292686\n",
      "iteration 11538, loss: 0.0022709984332323074\n",
      "iteration 11539, loss: 0.00262703956104815\n",
      "iteration 11540, loss: 0.0018866187892854214\n",
      "iteration 11541, loss: 0.0017463518306612968\n",
      "iteration 11542, loss: 0.0019603478722274303\n",
      "iteration 11543, loss: 0.0022613718174397945\n",
      "iteration 11544, loss: 0.0023266086354851723\n",
      "iteration 11545, loss: 0.0023755310103297234\n",
      "iteration 11546, loss: 0.0022840469609946012\n",
      "iteration 11547, loss: 0.0018818615935742855\n",
      "iteration 11548, loss: 0.002146163024008274\n",
      "iteration 11549, loss: 0.002594709163531661\n",
      "iteration 11550, loss: 0.002641326515004039\n",
      "iteration 11551, loss: 0.002668410073965788\n",
      "iteration 11552, loss: 0.0025013727135956287\n",
      "iteration 11553, loss: 0.002306345384567976\n",
      "iteration 11554, loss: 0.002223535906523466\n",
      "iteration 11555, loss: 0.002804090268909931\n",
      "iteration 11556, loss: 0.0025255046784877777\n",
      "iteration 11557, loss: 0.0024791653268039227\n",
      "iteration 11558, loss: 0.0022001443430781364\n",
      "iteration 11559, loss: 0.0023369193077087402\n",
      "iteration 11560, loss: 0.002226853044703603\n",
      "iteration 11561, loss: 0.0023358138278126717\n",
      "iteration 11562, loss: 0.0018812402850016952\n",
      "iteration 11563, loss: 0.00187727902084589\n",
      "iteration 11564, loss: 0.002157071605324745\n",
      "iteration 11565, loss: 0.0023401514627039433\n",
      "iteration 11566, loss: 0.0021487902849912643\n",
      "iteration 11567, loss: 0.0021436167880892754\n",
      "iteration 11568, loss: 0.002271805889904499\n",
      "iteration 11569, loss: 0.002037727739661932\n",
      "iteration 11570, loss: 0.0024109468795359135\n",
      "iteration 11571, loss: 0.0022753463126719\n",
      "iteration 11572, loss: 0.001968898344784975\n",
      "iteration 11573, loss: 0.0023228575009852648\n",
      "iteration 11574, loss: 0.0024997324217110872\n",
      "iteration 11575, loss: 0.0027264056261628866\n",
      "iteration 11576, loss: 0.002503505675122142\n",
      "iteration 11577, loss: 0.0019440136384218931\n",
      "iteration 11578, loss: 0.002083340659737587\n",
      "iteration 11579, loss: 0.0021453662775456905\n",
      "iteration 11580, loss: 0.0024767080321907997\n",
      "iteration 11581, loss: 0.002355804666876793\n",
      "iteration 11582, loss: 0.0019600982777774334\n",
      "iteration 11583, loss: 0.002301698550581932\n",
      "iteration 11584, loss: 0.002047900576144457\n",
      "iteration 11585, loss: 0.002303090412169695\n",
      "iteration 11586, loss: 0.002004094421863556\n",
      "iteration 11587, loss: 0.0019278547260910273\n",
      "iteration 11588, loss: 0.0019604633562266827\n",
      "iteration 11589, loss: 0.0021441015414893627\n",
      "iteration 11590, loss: 0.0020719305612146854\n",
      "iteration 11591, loss: 0.0024361428804695606\n",
      "iteration 11592, loss: 0.0021472196094691753\n",
      "iteration 11593, loss: 0.002249406185001135\n",
      "iteration 11594, loss: 0.0022160657681524754\n",
      "iteration 11595, loss: 0.002102246508002281\n",
      "iteration 11596, loss: 0.0023414301685988903\n",
      "iteration 11597, loss: 0.0022846106439828873\n",
      "iteration 11598, loss: 0.0020808877889066935\n",
      "iteration 11599, loss: 0.0021194133441895247\n",
      "iteration 11600, loss: 0.0023061581887304783\n",
      "iteration 11601, loss: 0.0026462289970368147\n",
      "iteration 11602, loss: 0.0025542224757373333\n",
      "iteration 11603, loss: 0.002028134185820818\n",
      "iteration 11604, loss: 0.0019821433816105127\n",
      "iteration 11605, loss: 0.0019125411054119468\n",
      "iteration 11606, loss: 0.0023325555957853794\n",
      "iteration 11607, loss: 0.002149514853954315\n",
      "iteration 11608, loss: 0.002571635413914919\n",
      "iteration 11609, loss: 0.0024245232343673706\n",
      "iteration 11610, loss: 0.0021667401306331158\n",
      "iteration 11611, loss: 0.002195358509197831\n",
      "iteration 11612, loss: 0.00230595376342535\n",
      "iteration 11613, loss: 0.0019192330073565245\n",
      "iteration 11614, loss: 0.0027239040937274694\n",
      "iteration 11615, loss: 0.0023348762188106775\n",
      "iteration 11616, loss: 0.0024552445393055677\n",
      "iteration 11617, loss: 0.002398532349616289\n",
      "iteration 11618, loss: 0.0026688738726079464\n",
      "iteration 11619, loss: 0.002527222502976656\n",
      "iteration 11620, loss: 0.00230599963106215\n",
      "iteration 11621, loss: 0.0022819547448307276\n",
      "iteration 11622, loss: 0.0028300380799919367\n",
      "iteration 11623, loss: 0.0022072154097259045\n",
      "iteration 11624, loss: 0.001994055463001132\n",
      "iteration 11625, loss: 0.002396422903984785\n",
      "iteration 11626, loss: 0.0021940013393759727\n",
      "iteration 11627, loss: 0.0023222847376018763\n",
      "iteration 11628, loss: 0.002549584489315748\n",
      "iteration 11629, loss: 0.0018352933693677187\n",
      "iteration 11630, loss: 0.0021786242723464966\n",
      "iteration 11631, loss: 0.002534091705456376\n",
      "iteration 11632, loss: 0.0020020802039653063\n",
      "iteration 11633, loss: 0.002221441362053156\n",
      "iteration 11634, loss: 0.002322919201105833\n",
      "iteration 11635, loss: 0.0019098537741228938\n",
      "iteration 11636, loss: 0.0016391935059800744\n",
      "iteration 11637, loss: 0.002212334191426635\n",
      "iteration 11638, loss: 0.002204847987741232\n",
      "iteration 11639, loss: 0.0020271483808755875\n",
      "iteration 11640, loss: 0.0017685758648440242\n",
      "iteration 11641, loss: 0.002379857935011387\n",
      "iteration 11642, loss: 0.0020282738842070103\n",
      "iteration 11643, loss: 0.0021157432347536087\n",
      "iteration 11644, loss: 0.0022642507683485746\n",
      "iteration 11645, loss: 0.0024451827630400658\n",
      "iteration 11646, loss: 0.002329061971977353\n",
      "iteration 11647, loss: 0.0019531394354999065\n",
      "iteration 11648, loss: 0.002229574602097273\n",
      "iteration 11649, loss: 0.0018602402415126562\n",
      "iteration 11650, loss: 0.0020570275373756886\n",
      "iteration 11651, loss: 0.002292832126840949\n",
      "iteration 11652, loss: 0.0021509299986064434\n",
      "iteration 11653, loss: 0.0023130658082664013\n",
      "iteration 11654, loss: 0.001708179246634245\n",
      "iteration 11655, loss: 0.0021584187634289265\n",
      "iteration 11656, loss: 0.0018799451645463705\n",
      "iteration 11657, loss: 0.002119078766554594\n",
      "iteration 11658, loss: 0.00248639564961195\n",
      "iteration 11659, loss: 0.0019353224197402596\n",
      "iteration 11660, loss: 0.0022555841132998466\n",
      "iteration 11661, loss: 0.0021515064872801304\n",
      "iteration 11662, loss: 0.002224984113126993\n",
      "iteration 11663, loss: 0.0019027972593903542\n",
      "iteration 11664, loss: 0.002108335727825761\n",
      "iteration 11665, loss: 0.0023625274188816547\n",
      "iteration 11666, loss: 0.002479680348187685\n",
      "iteration 11667, loss: 0.002312044147402048\n",
      "iteration 11668, loss: 0.0019129728898406029\n",
      "iteration 11669, loss: 0.0020804195664823055\n",
      "iteration 11670, loss: 0.0021023214794695377\n",
      "iteration 11671, loss: 0.002079819794744253\n",
      "iteration 11672, loss: 0.0020926319994032383\n",
      "iteration 11673, loss: 0.0017982074059545994\n",
      "iteration 11674, loss: 0.002350485883653164\n",
      "iteration 11675, loss: 0.0018750883173197508\n",
      "iteration 11676, loss: 0.002411541063338518\n",
      "iteration 11677, loss: 0.002662431914359331\n",
      "iteration 11678, loss: 0.0020575132220983505\n",
      "iteration 11679, loss: 0.0021789974998682737\n",
      "iteration 11680, loss: 0.0019943497609347105\n",
      "iteration 11681, loss: 0.002431224100291729\n",
      "iteration 11682, loss: 0.0024416735395789146\n",
      "iteration 11683, loss: 0.0021930637303739786\n",
      "iteration 11684, loss: 0.0018495576223358512\n",
      "iteration 11685, loss: 0.002462696749716997\n",
      "iteration 11686, loss: 0.0022816089913249016\n",
      "iteration 11687, loss: 0.0025113336741924286\n",
      "iteration 11688, loss: 0.0017952362541109324\n",
      "iteration 11689, loss: 0.0018620353657752275\n",
      "iteration 11690, loss: 0.0021121827885508537\n",
      "iteration 11691, loss: 0.0020006257109344006\n",
      "iteration 11692, loss: 0.002049953443929553\n",
      "iteration 11693, loss: 0.001991442171856761\n",
      "iteration 11694, loss: 0.0021539349108934402\n",
      "iteration 11695, loss: 0.0024008476175367832\n",
      "iteration 11696, loss: 0.001923255855217576\n",
      "iteration 11697, loss: 0.0024537434801459312\n",
      "iteration 11698, loss: 0.0025749134365469217\n",
      "iteration 11699, loss: 0.002406725659966469\n",
      "iteration 11700, loss: 0.002467985963448882\n",
      "iteration 11701, loss: 0.001995349070057273\n",
      "iteration 11702, loss: 0.002110675210133195\n",
      "iteration 11703, loss: 0.002160156611353159\n",
      "iteration 11704, loss: 0.00213486491702497\n",
      "iteration 11705, loss: 0.002579320687800646\n",
      "iteration 11706, loss: 0.002192605519667268\n",
      "iteration 11707, loss: 0.0023146909661591053\n",
      "iteration 11708, loss: 0.002344153355807066\n",
      "iteration 11709, loss: 0.0024842112325131893\n",
      "iteration 11710, loss: 0.002139227231964469\n",
      "iteration 11711, loss: 0.001872821245342493\n",
      "iteration 11712, loss: 0.0019503245130181313\n",
      "iteration 11713, loss: 0.0023524300195276737\n",
      "iteration 11714, loss: 0.0025691580958664417\n",
      "iteration 11715, loss: 0.002246432937681675\n",
      "iteration 11716, loss: 0.002266913652420044\n",
      "iteration 11717, loss: 0.0021171525586396456\n",
      "iteration 11718, loss: 0.00225856713950634\n",
      "iteration 11719, loss: 0.0023609744384884834\n",
      "iteration 11720, loss: 0.002451566280797124\n",
      "iteration 11721, loss: 0.001827889820560813\n",
      "iteration 11722, loss: 0.002477749017998576\n",
      "iteration 11723, loss: 0.0024278375785797834\n",
      "iteration 11724, loss: 0.0020277933217585087\n",
      "iteration 11725, loss: 0.0022587189450860023\n",
      "iteration 11726, loss: 0.002256521489471197\n",
      "iteration 11727, loss: 0.002518441528081894\n",
      "iteration 11728, loss: 0.002491449238732457\n",
      "iteration 11729, loss: 0.002123521640896797\n",
      "iteration 11730, loss: 0.0021716461051255465\n",
      "iteration 11731, loss: 0.001972511410713196\n",
      "iteration 11732, loss: 0.0021356251090765\n",
      "iteration 11733, loss: 0.0024343361146748066\n",
      "iteration 11734, loss: 0.002363248961046338\n",
      "iteration 11735, loss: 0.002028360031545162\n",
      "iteration 11736, loss: 0.0020196556579321623\n",
      "iteration 11737, loss: 0.0026584244333207607\n",
      "iteration 11738, loss: 0.0027958042919635773\n",
      "iteration 11739, loss: 0.0022327802143990993\n",
      "iteration 11740, loss: 0.0025815265253186226\n",
      "iteration 11741, loss: 0.0022360454313457012\n",
      "iteration 11742, loss: 0.002281847409904003\n",
      "iteration 11743, loss: 0.002068684436380863\n",
      "iteration 11744, loss: 0.0023924955166876316\n",
      "iteration 11745, loss: 0.0027582996990531683\n",
      "iteration 11746, loss: 0.002203915733844042\n",
      "iteration 11747, loss: 0.0026013581082224846\n",
      "iteration 11748, loss: 0.0023492518812417984\n",
      "iteration 11749, loss: 0.00236682640388608\n",
      "iteration 11750, loss: 0.002390795387327671\n",
      "iteration 11751, loss: 0.0021058237180113792\n",
      "iteration 11752, loss: 0.002358686877414584\n",
      "iteration 11753, loss: 0.0021284082904458046\n",
      "iteration 11754, loss: 0.0023131705820560455\n",
      "iteration 11755, loss: 0.002414793474599719\n",
      "iteration 11756, loss: 0.00234015379101038\n",
      "iteration 11757, loss: 0.0019627991132438183\n",
      "iteration 11758, loss: 0.002180717419832945\n",
      "iteration 11759, loss: 0.002122563309967518\n",
      "iteration 11760, loss: 0.002352771582081914\n",
      "iteration 11761, loss: 0.0022846751380711794\n",
      "iteration 11762, loss: 0.0031043095514178276\n",
      "iteration 11763, loss: 0.0021546422503888607\n",
      "iteration 11764, loss: 0.0017891977913677692\n",
      "iteration 11765, loss: 0.002594536170363426\n",
      "iteration 11766, loss: 0.002498477464541793\n",
      "iteration 11767, loss: 0.0023184367455542088\n",
      "iteration 11768, loss: 0.0024314657784998417\n",
      "iteration 11769, loss: 0.0021716714836657047\n",
      "iteration 11770, loss: 0.0018834907095879316\n",
      "iteration 11771, loss: 0.002017253078520298\n",
      "iteration 11772, loss: 0.002282937290146947\n",
      "iteration 11773, loss: 0.0020480602979660034\n",
      "iteration 11774, loss: 0.0022455365397036076\n",
      "iteration 11775, loss: 0.0018783957930281758\n",
      "iteration 11776, loss: 0.002060883678495884\n",
      "iteration 11777, loss: 0.002208828693255782\n",
      "iteration 11778, loss: 0.002304350957274437\n",
      "iteration 11779, loss: 0.0019646871369332075\n",
      "iteration 11780, loss: 0.002233315259218216\n",
      "iteration 11781, loss: 0.001963346730917692\n",
      "iteration 11782, loss: 0.0021675669122487307\n",
      "iteration 11783, loss: 0.0023541024420410395\n",
      "iteration 11784, loss: 0.0020141704007983208\n",
      "iteration 11785, loss: 0.0018016875255852938\n",
      "iteration 11786, loss: 0.002137137111276388\n",
      "iteration 11787, loss: 0.002427891828119755\n",
      "iteration 11788, loss: 0.0021975277923047543\n",
      "iteration 11789, loss: 0.002055402146652341\n",
      "iteration 11790, loss: 0.0020822854712605476\n",
      "iteration 11791, loss: 0.001956961350515485\n",
      "iteration 11792, loss: 0.0023295977152884007\n",
      "iteration 11793, loss: 0.0022852099500596523\n",
      "iteration 11794, loss: 0.0021007782779634\n",
      "iteration 11795, loss: 0.0020241434685885906\n",
      "iteration 11796, loss: 0.001958605134859681\n",
      "iteration 11797, loss: 0.0025013950653374195\n",
      "iteration 11798, loss: 0.0021813674829900265\n",
      "iteration 11799, loss: 0.002058052457869053\n",
      "iteration 11800, loss: 0.0016688453033566475\n",
      "iteration 11801, loss: 0.0019872465636581182\n",
      "iteration 11802, loss: 0.002216558437794447\n",
      "iteration 11803, loss: 0.0019275997765362263\n",
      "iteration 11804, loss: 0.001926055527292192\n",
      "iteration 11805, loss: 0.0020488924346864223\n",
      "iteration 11806, loss: 0.0022781891748309135\n",
      "iteration 11807, loss: 0.0027465156745165586\n",
      "iteration 11808, loss: 0.0026555629447102547\n",
      "iteration 11809, loss: 0.0020156390964984894\n",
      "iteration 11810, loss: 0.0024839984253048897\n",
      "iteration 11811, loss: 0.0019194318447262049\n",
      "iteration 11812, loss: 0.002278708852827549\n",
      "iteration 11813, loss: 0.0021348269656300545\n",
      "iteration 11814, loss: 0.0022754743695259094\n",
      "iteration 11815, loss: 0.0019969523418694735\n",
      "iteration 11816, loss: 0.0023166746832430363\n",
      "iteration 11817, loss: 0.001930996309965849\n",
      "iteration 11818, loss: 0.0020763156935572624\n",
      "iteration 11819, loss: 0.002303654095157981\n",
      "iteration 11820, loss: 0.0022477477323263884\n",
      "iteration 11821, loss: 0.0023088157176971436\n",
      "iteration 11822, loss: 0.001984111499041319\n",
      "iteration 11823, loss: 0.0022535682655870914\n",
      "iteration 11824, loss: 0.0024054618552327156\n",
      "iteration 11825, loss: 0.002212395891547203\n",
      "iteration 11826, loss: 0.001957125263288617\n",
      "iteration 11827, loss: 0.002701957244426012\n",
      "iteration 11828, loss: 0.0019788017962127924\n",
      "iteration 11829, loss: 0.00219389284029603\n",
      "iteration 11830, loss: 0.002314560580998659\n",
      "iteration 11831, loss: 0.0023435913026332855\n",
      "iteration 11832, loss: 0.002322861924767494\n",
      "iteration 11833, loss: 0.0019745631143450737\n",
      "iteration 11834, loss: 0.0017606428591534495\n",
      "iteration 11835, loss: 0.0017464791890233755\n",
      "iteration 11836, loss: 0.0021191039122641087\n",
      "iteration 11837, loss: 0.002256106585264206\n",
      "iteration 11838, loss: 0.0024997256696224213\n",
      "iteration 11839, loss: 0.0024241171777248383\n",
      "iteration 11840, loss: 0.002165786223486066\n",
      "iteration 11841, loss: 0.0020251173991709948\n",
      "iteration 11842, loss: 0.001928073586896062\n",
      "iteration 11843, loss: 0.0023018610663712025\n",
      "iteration 11844, loss: 0.0021499083377420902\n",
      "iteration 11845, loss: 0.0023991453927010298\n",
      "iteration 11846, loss: 0.0019287350587546825\n",
      "iteration 11847, loss: 0.0024300599470734596\n",
      "iteration 11848, loss: 0.001945962430909276\n",
      "iteration 11849, loss: 0.002162193413823843\n",
      "iteration 11850, loss: 0.0023370725102722645\n",
      "iteration 11851, loss: 0.002059193793684244\n",
      "iteration 11852, loss: 0.002365624997764826\n",
      "iteration 11853, loss: 0.001939386478625238\n",
      "iteration 11854, loss: 0.002168925479054451\n",
      "iteration 11855, loss: 0.0018427374307066202\n",
      "iteration 11856, loss: 0.0020742807537317276\n",
      "iteration 11857, loss: 0.0024206340312957764\n",
      "iteration 11858, loss: 0.0020370646379888058\n",
      "iteration 11859, loss: 0.001903461990877986\n",
      "iteration 11860, loss: 0.0025152768939733505\n",
      "iteration 11861, loss: 0.0026185582391917706\n",
      "iteration 11862, loss: 0.002097233897075057\n",
      "iteration 11863, loss: 0.0027860328555107117\n",
      "iteration 11864, loss: 0.0019706652965396643\n",
      "iteration 11865, loss: 0.002263821894302964\n",
      "iteration 11866, loss: 0.0023716811556369066\n",
      "iteration 11867, loss: 0.002179159317165613\n",
      "iteration 11868, loss: 0.0019010117975994945\n",
      "iteration 11869, loss: 0.0020748572424054146\n",
      "iteration 11870, loss: 0.0019890537951141596\n",
      "iteration 11871, loss: 0.0019559445790946484\n",
      "iteration 11872, loss: 0.0019071639981120825\n",
      "iteration 11873, loss: 0.002198214875534177\n",
      "iteration 11874, loss: 0.0020809760317206383\n",
      "iteration 11875, loss: 0.0024977317079901695\n",
      "iteration 11876, loss: 0.002668515546247363\n",
      "iteration 11877, loss: 0.0019632582552731037\n",
      "iteration 11878, loss: 0.001926629920490086\n",
      "iteration 11879, loss: 0.0018720189109444618\n",
      "iteration 11880, loss: 0.0019339346326887608\n",
      "iteration 11881, loss: 0.002105402760207653\n",
      "iteration 11882, loss: 0.0022340870928019285\n",
      "iteration 11883, loss: 0.0018954200204461813\n",
      "iteration 11884, loss: 0.0018507433123886585\n",
      "iteration 11885, loss: 0.002092437818646431\n",
      "iteration 11886, loss: 0.002086120191961527\n",
      "iteration 11887, loss: 0.002171051222831011\n",
      "iteration 11888, loss: 0.00204771151766181\n",
      "iteration 11889, loss: 0.002282368950545788\n",
      "iteration 11890, loss: 0.001982689369469881\n",
      "iteration 11891, loss: 0.0024302538949996233\n",
      "iteration 11892, loss: 0.0023440979421138763\n",
      "iteration 11893, loss: 0.002150832675397396\n",
      "iteration 11894, loss: 0.002395469695329666\n",
      "iteration 11895, loss: 0.0017978021642193198\n",
      "iteration 11896, loss: 0.0023152329958975315\n",
      "iteration 11897, loss: 0.0022309499327093363\n",
      "iteration 11898, loss: 0.0017007426358759403\n",
      "iteration 11899, loss: 0.0025928691029548645\n",
      "iteration 11900, loss: 0.0019616594072431326\n",
      "iteration 11901, loss: 0.0024768002331256866\n",
      "iteration 11902, loss: 0.0025515754241496325\n",
      "iteration 11903, loss: 0.0024516882840543985\n",
      "iteration 11904, loss: 0.0021111522801220417\n",
      "iteration 11905, loss: 0.002532423473894596\n",
      "iteration 11906, loss: 0.002703823149204254\n",
      "iteration 11907, loss: 0.002317512407898903\n",
      "iteration 11908, loss: 0.0026236698031425476\n",
      "iteration 11909, loss: 0.002064833650365472\n",
      "iteration 11910, loss: 0.00225706584751606\n",
      "iteration 11911, loss: 0.0025806103367358446\n",
      "iteration 11912, loss: 0.001970697194337845\n",
      "iteration 11913, loss: 0.002273379359394312\n",
      "iteration 11914, loss: 0.0024846219457685947\n",
      "iteration 11915, loss: 0.002265586983412504\n",
      "iteration 11916, loss: 0.0020973014179617167\n",
      "iteration 11917, loss: 0.00231468235142529\n",
      "iteration 11918, loss: 0.002384549705311656\n",
      "iteration 11919, loss: 0.0024187935050576925\n",
      "iteration 11920, loss: 0.0024538394063711166\n",
      "iteration 11921, loss: 0.0021493169479072094\n",
      "iteration 11922, loss: 0.0019634144846349955\n",
      "iteration 11923, loss: 0.002110312459990382\n",
      "iteration 11924, loss: 0.0020015027839690447\n",
      "iteration 11925, loss: 0.0019554041791707277\n",
      "iteration 11926, loss: 0.0023417905904352665\n",
      "iteration 11927, loss: 0.0020822607912123203\n",
      "iteration 11928, loss: 0.0023630373179912567\n",
      "iteration 11929, loss: 0.002094636205583811\n",
      "iteration 11930, loss: 0.002280200831592083\n",
      "iteration 11931, loss: 0.002818093169480562\n",
      "iteration 11932, loss: 0.002272173762321472\n",
      "iteration 11933, loss: 0.002373525407165289\n",
      "iteration 11934, loss: 0.002703441306948662\n",
      "iteration 11935, loss: 0.0026824194937944412\n",
      "iteration 11936, loss: 0.0021566872019320726\n",
      "iteration 11937, loss: 0.0023333777207881212\n",
      "iteration 11938, loss: 0.00245093647390604\n",
      "iteration 11939, loss: 0.0019938210025429726\n",
      "iteration 11940, loss: 0.002190284663811326\n",
      "iteration 11941, loss: 0.002057288307696581\n",
      "iteration 11942, loss: 0.0016477121971547604\n",
      "iteration 11943, loss: 0.002202924806624651\n",
      "iteration 11944, loss: 0.0020091633778065443\n",
      "iteration 11945, loss: 0.002149997279047966\n",
      "iteration 11946, loss: 0.0023158988915383816\n",
      "iteration 11947, loss: 0.0019959518685936928\n",
      "iteration 11948, loss: 0.0021505847107619047\n",
      "iteration 11949, loss: 0.0022003990598022938\n",
      "iteration 11950, loss: 0.00177146983332932\n",
      "iteration 11951, loss: 0.0019152977038174868\n",
      "iteration 11952, loss: 0.0021277638152241707\n",
      "iteration 11953, loss: 0.002332485280930996\n",
      "iteration 11954, loss: 0.0017837887862697244\n",
      "iteration 11955, loss: 0.0018597475718706846\n",
      "iteration 11956, loss: 0.002323138527572155\n",
      "iteration 11957, loss: 0.0023559709079563618\n",
      "iteration 11958, loss: 0.0020363139919936657\n",
      "iteration 11959, loss: 0.0016746609471738338\n",
      "iteration 11960, loss: 0.002322910586372018\n",
      "iteration 11961, loss: 0.0023225441109389067\n",
      "iteration 11962, loss: 0.0022855866700410843\n",
      "iteration 11963, loss: 0.002263137139379978\n",
      "iteration 11964, loss: 0.0024764607660472393\n",
      "iteration 11965, loss: 0.002118906006217003\n",
      "iteration 11966, loss: 0.0018174790311604738\n",
      "iteration 11967, loss: 0.0017068521119654179\n",
      "iteration 11968, loss: 0.0018372441409155726\n",
      "iteration 11969, loss: 0.002360158134251833\n",
      "iteration 11970, loss: 0.0022693558130413294\n",
      "iteration 11971, loss: 0.0018164166249334812\n",
      "iteration 11972, loss: 0.0022510765120387077\n",
      "iteration 11973, loss: 0.002072315663099289\n",
      "iteration 11974, loss: 0.0018523491453379393\n",
      "iteration 11975, loss: 0.0024232547730207443\n",
      "iteration 11976, loss: 0.0018917710985988379\n",
      "iteration 11977, loss: 0.001955416752025485\n",
      "iteration 11978, loss: 0.0020506237633526325\n",
      "iteration 11979, loss: 0.0019285257440060377\n",
      "iteration 11980, loss: 0.0023079775273799896\n",
      "iteration 11981, loss: 0.002436004113405943\n",
      "iteration 11982, loss: 0.0019559140782803297\n",
      "iteration 11983, loss: 0.0017982710851356387\n",
      "iteration 11984, loss: 0.0017533686477690935\n",
      "iteration 11985, loss: 0.0020694155246019363\n",
      "iteration 11986, loss: 0.00258830888196826\n",
      "iteration 11987, loss: 0.0020136944949626923\n",
      "iteration 11988, loss: 0.0019416671711951494\n",
      "iteration 11989, loss: 0.0023195433896034956\n",
      "iteration 11990, loss: 0.002163670025765896\n",
      "iteration 11991, loss: 0.0020676623098552227\n",
      "iteration 11992, loss: 0.0020586890168488026\n",
      "iteration 11993, loss: 0.0023979460820555687\n",
      "iteration 11994, loss: 0.0019353742245584726\n",
      "iteration 11995, loss: 0.002113811206072569\n",
      "iteration 11996, loss: 0.00215141917578876\n",
      "iteration 11997, loss: 0.0019972040317952633\n",
      "iteration 11998, loss: 0.002181985415518284\n",
      "iteration 11999, loss: 0.0028527434915304184\n",
      "iteration 12000, loss: 0.002312005963176489\n",
      "iteration 12001, loss: 0.0018269591964781284\n",
      "iteration 12002, loss: 0.0023138942196965218\n",
      "iteration 12003, loss: 0.001936361426487565\n",
      "iteration 12004, loss: 0.0025408966466784477\n",
      "iteration 12005, loss: 0.001785600557923317\n",
      "iteration 12006, loss: 0.0019806609489023685\n",
      "iteration 12007, loss: 0.002245326992124319\n",
      "iteration 12008, loss: 0.002362379338592291\n",
      "iteration 12009, loss: 0.0021245151292532682\n",
      "iteration 12010, loss: 0.0019723623991012573\n",
      "iteration 12011, loss: 0.002395856659859419\n",
      "iteration 12012, loss: 0.001954748062416911\n",
      "iteration 12013, loss: 0.00246674963273108\n",
      "iteration 12014, loss: 0.0023557604290544987\n",
      "iteration 12015, loss: 0.001868321793153882\n",
      "iteration 12016, loss: 0.001927258912473917\n",
      "iteration 12017, loss: 0.0019359613070264459\n",
      "iteration 12018, loss: 0.0019923776853829622\n",
      "iteration 12019, loss: 0.0018377223750576377\n",
      "iteration 12020, loss: 0.001884544501081109\n",
      "iteration 12021, loss: 0.0021761604584753513\n",
      "iteration 12022, loss: 0.001988788601011038\n",
      "iteration 12023, loss: 0.0018225146923214197\n",
      "iteration 12024, loss: 0.0021681145299226046\n",
      "iteration 12025, loss: 0.0018176485318690538\n",
      "iteration 12026, loss: 0.002161569893360138\n",
      "iteration 12027, loss: 0.0020024231635034084\n",
      "iteration 12028, loss: 0.0019230118487030268\n",
      "iteration 12029, loss: 0.0026543792337179184\n",
      "iteration 12030, loss: 0.002008525189012289\n",
      "iteration 12031, loss: 0.0020592943765223026\n",
      "iteration 12032, loss: 0.002039231127128005\n",
      "iteration 12033, loss: 0.0020457636564970016\n",
      "iteration 12034, loss: 0.0023652524687349796\n",
      "iteration 12035, loss: 0.002451055683195591\n",
      "iteration 12036, loss: 0.0024848803877830505\n",
      "iteration 12037, loss: 0.0025583337992429733\n",
      "iteration 12038, loss: 0.0018738582730293274\n",
      "iteration 12039, loss: 0.0023806127719581127\n",
      "iteration 12040, loss: 0.001867941813543439\n",
      "iteration 12041, loss: 0.001748481998220086\n",
      "iteration 12042, loss: 0.002002277411520481\n",
      "iteration 12043, loss: 0.0024156926665455103\n",
      "iteration 12044, loss: 0.0023224549368023872\n",
      "iteration 12045, loss: 0.0020893546752631664\n",
      "iteration 12046, loss: 0.002146187238395214\n",
      "iteration 12047, loss: 0.0021713501773774624\n",
      "iteration 12048, loss: 0.002341445302590728\n",
      "iteration 12049, loss: 0.0025228657759726048\n",
      "iteration 12050, loss: 0.0019516656175255775\n",
      "iteration 12051, loss: 0.0017381426878273487\n",
      "iteration 12052, loss: 0.0022156417835503817\n",
      "iteration 12053, loss: 0.001878263778053224\n",
      "iteration 12054, loss: 0.0021103809121996164\n",
      "iteration 12055, loss: 0.002365649212151766\n",
      "iteration 12056, loss: 0.002084823790937662\n",
      "iteration 12057, loss: 0.002359796781092882\n",
      "iteration 12058, loss: 0.0023389689158648252\n",
      "iteration 12059, loss: 0.0018507770728319883\n",
      "iteration 12060, loss: 0.0022288828622549772\n",
      "iteration 12061, loss: 0.002040171530097723\n",
      "iteration 12062, loss: 0.001982069807127118\n",
      "iteration 12063, loss: 0.0019556733313947916\n",
      "iteration 12064, loss: 0.0024748481810092926\n",
      "iteration 12065, loss: 0.0026203617453575134\n",
      "iteration 12066, loss: 0.002367140259593725\n",
      "iteration 12067, loss: 0.002161299344152212\n",
      "iteration 12068, loss: 0.0021096186246722937\n",
      "iteration 12069, loss: 0.0021251272410154343\n",
      "iteration 12070, loss: 0.0023952974006533623\n",
      "iteration 12071, loss: 0.002015883568674326\n",
      "iteration 12072, loss: 0.002102077007293701\n",
      "iteration 12073, loss: 0.0023010526783764362\n",
      "iteration 12074, loss: 0.001930916216224432\n",
      "iteration 12075, loss: 0.0020028813742101192\n",
      "iteration 12076, loss: 0.00253532687202096\n",
      "iteration 12077, loss: 0.0018671017605811357\n",
      "iteration 12078, loss: 0.0022156506311148405\n",
      "iteration 12079, loss: 0.0018998822197318077\n",
      "iteration 12080, loss: 0.0025063902139663696\n",
      "iteration 12081, loss: 0.001937185414135456\n",
      "iteration 12082, loss: 0.001921751769259572\n",
      "iteration 12083, loss: 0.002513282932341099\n",
      "iteration 12084, loss: 0.002063125604763627\n",
      "iteration 12085, loss: 0.0026024263352155685\n",
      "iteration 12086, loss: 0.002039846032857895\n",
      "iteration 12087, loss: 0.0019808937795460224\n",
      "iteration 12088, loss: 0.0023262365721166134\n",
      "iteration 12089, loss: 0.0018824705621227622\n",
      "iteration 12090, loss: 0.002094302326440811\n",
      "iteration 12091, loss: 0.0020162248983979225\n",
      "iteration 12092, loss: 0.0023775515146553516\n",
      "iteration 12093, loss: 0.0019839899614453316\n",
      "iteration 12094, loss: 0.0020672434475272894\n",
      "iteration 12095, loss: 0.0022473272401839495\n",
      "iteration 12096, loss: 0.0023856894113123417\n",
      "iteration 12097, loss: 0.0022376037668436766\n",
      "iteration 12098, loss: 0.0023792400024831295\n",
      "iteration 12099, loss: 0.002082378137856722\n",
      "iteration 12100, loss: 0.0020126174204051495\n",
      "iteration 12101, loss: 0.0016579442890360951\n",
      "iteration 12102, loss: 0.002385379746556282\n",
      "iteration 12103, loss: 0.002181317890062928\n",
      "iteration 12104, loss: 0.002092445734888315\n",
      "iteration 12105, loss: 0.002118661068379879\n",
      "iteration 12106, loss: 0.0018094470724463463\n",
      "iteration 12107, loss: 0.0021190172992646694\n",
      "iteration 12108, loss: 0.002169291954487562\n",
      "iteration 12109, loss: 0.002249190118163824\n",
      "iteration 12110, loss: 0.0023123568389564753\n",
      "iteration 12111, loss: 0.001971278805285692\n",
      "iteration 12112, loss: 0.0024224137887358665\n",
      "iteration 12113, loss: 0.0018028849735856056\n",
      "iteration 12114, loss: 0.0019930251874029636\n",
      "iteration 12115, loss: 0.0022028975654393435\n",
      "iteration 12116, loss: 0.0022652968764305115\n",
      "iteration 12117, loss: 0.002440965734422207\n",
      "iteration 12118, loss: 0.002214121865108609\n",
      "iteration 12119, loss: 0.002088399836793542\n",
      "iteration 12120, loss: 0.002104265848174691\n",
      "iteration 12121, loss: 0.0017859123181551695\n",
      "iteration 12122, loss: 0.0021726954728364944\n",
      "iteration 12123, loss: 0.001730696763843298\n",
      "iteration 12124, loss: 0.0022608547005802393\n",
      "iteration 12125, loss: 0.0021130612585693598\n",
      "iteration 12126, loss: 0.0018884728197008371\n",
      "iteration 12127, loss: 0.0019571250304579735\n",
      "iteration 12128, loss: 0.0020321703050285578\n",
      "iteration 12129, loss: 0.0020392220467329025\n",
      "iteration 12130, loss: 0.0020970338955521584\n",
      "iteration 12131, loss: 0.0019266570452600718\n",
      "iteration 12132, loss: 0.002263628412038088\n",
      "iteration 12133, loss: 0.002019076142460108\n",
      "iteration 12134, loss: 0.0022602095268666744\n",
      "iteration 12135, loss: 0.001960362773388624\n",
      "iteration 12136, loss: 0.0021324201952666044\n",
      "iteration 12137, loss: 0.002499931026250124\n",
      "iteration 12138, loss: 0.0018825313309207559\n",
      "iteration 12139, loss: 0.0019842535257339478\n",
      "iteration 12140, loss: 0.0022431579418480396\n",
      "iteration 12141, loss: 0.002062862040475011\n",
      "iteration 12142, loss: 0.0016581047093495727\n",
      "iteration 12143, loss: 0.002185418037697673\n",
      "iteration 12144, loss: 0.002217038068920374\n",
      "iteration 12145, loss: 0.002245325595140457\n",
      "iteration 12146, loss: 0.0017937659285962582\n",
      "iteration 12147, loss: 0.0018332907930016518\n",
      "iteration 12148, loss: 0.0018292628228664398\n",
      "iteration 12149, loss: 0.0020446493290364742\n",
      "iteration 12150, loss: 0.0018981695175170898\n",
      "iteration 12151, loss: 0.002256878186017275\n",
      "iteration 12152, loss: 0.0024722530506551266\n",
      "iteration 12153, loss: 0.002424274105578661\n",
      "iteration 12154, loss: 0.0022124690003693104\n",
      "iteration 12155, loss: 0.002234988147392869\n",
      "iteration 12156, loss: 0.0020718115847557783\n",
      "iteration 12157, loss: 0.0020281204488128424\n",
      "iteration 12158, loss: 0.001697475090622902\n",
      "iteration 12159, loss: 0.001938624307513237\n",
      "iteration 12160, loss: 0.002200877759605646\n",
      "iteration 12161, loss: 0.002211363520473242\n",
      "iteration 12162, loss: 0.002378181554377079\n",
      "iteration 12163, loss: 0.0024664271622896194\n",
      "iteration 12164, loss: 0.0021711094304919243\n",
      "iteration 12165, loss: 0.0018697853665798903\n",
      "iteration 12166, loss: 0.0017854503821581602\n",
      "iteration 12167, loss: 0.0018706457922235131\n",
      "iteration 12168, loss: 0.0021028071641921997\n",
      "iteration 12169, loss: 0.0021776054054498672\n",
      "iteration 12170, loss: 0.002338418737053871\n",
      "iteration 12171, loss: 0.0023136509116739035\n",
      "iteration 12172, loss: 0.001903824508190155\n",
      "iteration 12173, loss: 0.002217255998402834\n",
      "iteration 12174, loss: 0.001955577405169606\n",
      "iteration 12175, loss: 0.002044163877144456\n",
      "iteration 12176, loss: 0.0022069215774536133\n",
      "iteration 12177, loss: 0.00214434415102005\n",
      "iteration 12178, loss: 0.0020384241361171007\n",
      "iteration 12179, loss: 0.0019329492934048176\n",
      "iteration 12180, loss: 0.0018631166312843561\n",
      "iteration 12181, loss: 0.0020503040868788958\n",
      "iteration 12182, loss: 0.0019572204910218716\n",
      "iteration 12183, loss: 0.002012002281844616\n",
      "iteration 12184, loss: 0.0022272656206041574\n",
      "iteration 12185, loss: 0.001934276893734932\n",
      "iteration 12186, loss: 0.0021412698552012444\n",
      "iteration 12187, loss: 0.002654003445059061\n",
      "iteration 12188, loss: 0.001963228452950716\n",
      "iteration 12189, loss: 0.0023295264691114426\n",
      "iteration 12190, loss: 0.0023108175955712795\n",
      "iteration 12191, loss: 0.002208658494055271\n",
      "iteration 12192, loss: 0.001645755721256137\n",
      "iteration 12193, loss: 0.0017469871090725064\n",
      "iteration 12194, loss: 0.002262796275317669\n",
      "iteration 12195, loss: 0.00200446043163538\n",
      "iteration 12196, loss: 0.001857095630839467\n",
      "iteration 12197, loss: 0.002425899263471365\n",
      "iteration 12198, loss: 0.0018713411409407854\n",
      "iteration 12199, loss: 0.0021474577952176332\n",
      "iteration 12200, loss: 0.002077593933790922\n",
      "iteration 12201, loss: 0.0021403853315860033\n",
      "iteration 12202, loss: 0.0016003911150619388\n",
      "iteration 12203, loss: 0.002292622346431017\n",
      "iteration 12204, loss: 0.0021980623714625835\n",
      "iteration 12205, loss: 0.0022907513193786144\n",
      "iteration 12206, loss: 0.0022460175678133965\n",
      "iteration 12207, loss: 0.0019245920702815056\n",
      "iteration 12208, loss: 0.0022131693549454212\n",
      "iteration 12209, loss: 0.0019964631646871567\n",
      "iteration 12210, loss: 0.0023544076830148697\n",
      "iteration 12211, loss: 0.001919278409332037\n",
      "iteration 12212, loss: 0.0020693850237876177\n",
      "iteration 12213, loss: 0.001974410843104124\n",
      "iteration 12214, loss: 0.001862500561401248\n",
      "iteration 12215, loss: 0.0020174114033579826\n",
      "iteration 12216, loss: 0.0021594963036477566\n",
      "iteration 12217, loss: 0.001929182093590498\n",
      "iteration 12218, loss: 0.0019726764876395464\n",
      "iteration 12219, loss: 0.0023301169276237488\n",
      "iteration 12220, loss: 0.002503810916095972\n",
      "iteration 12221, loss: 0.002147585153579712\n",
      "iteration 12222, loss: 0.002009003423154354\n",
      "iteration 12223, loss: 0.002268044278025627\n",
      "iteration 12224, loss: 0.0018755770288407803\n",
      "iteration 12225, loss: 0.002069622976705432\n",
      "iteration 12226, loss: 0.002176235895603895\n",
      "iteration 12227, loss: 0.0019622398540377617\n",
      "iteration 12228, loss: 0.0019567625131458044\n",
      "iteration 12229, loss: 0.002268038224428892\n",
      "iteration 12230, loss: 0.0022152236197143793\n",
      "iteration 12231, loss: 0.001981230452656746\n",
      "iteration 12232, loss: 0.0023253983817994595\n",
      "iteration 12233, loss: 0.0021217826288193464\n",
      "iteration 12234, loss: 0.0022214441560208797\n",
      "iteration 12235, loss: 0.002036463003605604\n",
      "iteration 12236, loss: 0.0019149890867993236\n",
      "iteration 12237, loss: 0.0023698010481894016\n",
      "iteration 12238, loss: 0.002167054684832692\n",
      "iteration 12239, loss: 0.001900858129374683\n",
      "iteration 12240, loss: 0.0019704503938555717\n",
      "iteration 12241, loss: 0.002420582342892885\n",
      "iteration 12242, loss: 0.0022706298623234034\n",
      "iteration 12243, loss: 0.0025363017339259386\n",
      "iteration 12244, loss: 0.002040364546701312\n",
      "iteration 12245, loss: 0.0020596161484718323\n",
      "iteration 12246, loss: 0.0021535127889364958\n",
      "iteration 12247, loss: 0.0025309098418802023\n",
      "iteration 12248, loss: 0.002243906492367387\n",
      "iteration 12249, loss: 0.0019084429368376732\n",
      "iteration 12250, loss: 0.002539142034947872\n",
      "iteration 12251, loss: 0.001911127008497715\n",
      "iteration 12252, loss: 0.0022637946531176567\n",
      "iteration 12253, loss: 0.0022468636743724346\n",
      "iteration 12254, loss: 0.0019256233936175704\n",
      "iteration 12255, loss: 0.0021981587633490562\n",
      "iteration 12256, loss: 0.002168248873203993\n",
      "iteration 12257, loss: 0.002518935827538371\n",
      "iteration 12258, loss: 0.001799009507521987\n",
      "iteration 12259, loss: 0.002167308935895562\n",
      "iteration 12260, loss: 0.0019126739352941513\n",
      "iteration 12261, loss: 0.0020139277912676334\n",
      "iteration 12262, loss: 0.002203281968832016\n",
      "iteration 12263, loss: 0.0021477905102074146\n",
      "iteration 12264, loss: 0.002164390403777361\n",
      "iteration 12265, loss: 0.0020941877737641335\n",
      "iteration 12266, loss: 0.0021408156026154757\n",
      "iteration 12267, loss: 0.0017418836941942573\n",
      "iteration 12268, loss: 0.001770056551322341\n",
      "iteration 12269, loss: 0.0020089144818484783\n",
      "iteration 12270, loss: 0.0023993724025785923\n",
      "iteration 12271, loss: 0.0023971404880285263\n",
      "iteration 12272, loss: 0.0025555528700351715\n",
      "iteration 12273, loss: 0.002115469193086028\n",
      "iteration 12274, loss: 0.002197319408878684\n",
      "iteration 12275, loss: 0.0025424372870475054\n",
      "iteration 12276, loss: 0.0021864185109734535\n",
      "iteration 12277, loss: 0.0021013538353145123\n",
      "iteration 12278, loss: 0.0019362266175448895\n",
      "iteration 12279, loss: 0.001956697553396225\n",
      "iteration 12280, loss: 0.0023470891173928976\n",
      "iteration 12281, loss: 0.0019584642723202705\n",
      "iteration 12282, loss: 0.002130031120032072\n",
      "iteration 12283, loss: 0.002133178524672985\n",
      "iteration 12284, loss: 0.0019285527523607016\n",
      "iteration 12285, loss: 0.0019267458701506257\n",
      "iteration 12286, loss: 0.0023229471407830715\n",
      "iteration 12287, loss: 0.0023676040582358837\n",
      "iteration 12288, loss: 0.0026757647283375263\n",
      "iteration 12289, loss: 0.0018409922486171126\n",
      "iteration 12290, loss: 0.0021969699300825596\n",
      "iteration 12291, loss: 0.0022780485451221466\n",
      "iteration 12292, loss: 0.0017879155930131674\n",
      "iteration 12293, loss: 0.0022647075820714235\n",
      "iteration 12294, loss: 0.0020931120961904526\n",
      "iteration 12295, loss: 0.002181289717555046\n",
      "iteration 12296, loss: 0.0020740346517413855\n",
      "iteration 12297, loss: 0.0023272838443517685\n",
      "iteration 12298, loss: 0.0018395330989733338\n",
      "iteration 12299, loss: 0.002184896031394601\n",
      "iteration 12300, loss: 0.002180784475058317\n",
      "iteration 12301, loss: 0.0017856938065961003\n",
      "iteration 12302, loss: 0.0021423897705972195\n",
      "iteration 12303, loss: 0.0017189392820000648\n",
      "iteration 12304, loss: 0.0020012729801237583\n",
      "iteration 12305, loss: 0.002101005520671606\n",
      "iteration 12306, loss: 0.0019991728477180004\n",
      "iteration 12307, loss: 0.0017867527203634381\n",
      "iteration 12308, loss: 0.002366507425904274\n",
      "iteration 12309, loss: 0.0020502994302660227\n",
      "iteration 12310, loss: 0.0020373936276882887\n",
      "iteration 12311, loss: 0.0020355163142085075\n",
      "iteration 12312, loss: 0.0024667130783200264\n",
      "iteration 12313, loss: 0.00217589084059\n",
      "iteration 12314, loss: 0.0019120447104796767\n",
      "iteration 12315, loss: 0.0017791998106986284\n",
      "iteration 12316, loss: 0.001937376451678574\n",
      "iteration 12317, loss: 0.0022258195094764233\n",
      "iteration 12318, loss: 0.0018607497913762927\n",
      "iteration 12319, loss: 0.0022015897557139397\n",
      "iteration 12320, loss: 0.0020181029103696346\n",
      "iteration 12321, loss: 0.0026729037053883076\n",
      "iteration 12322, loss: 0.0017433165339753032\n",
      "iteration 12323, loss: 0.0020625994075089693\n",
      "iteration 12324, loss: 0.002036391058936715\n",
      "iteration 12325, loss: 0.001981440931558609\n",
      "iteration 12326, loss: 0.0020054494962096214\n",
      "iteration 12327, loss: 0.002192124491557479\n",
      "iteration 12328, loss: 0.0017445237608626485\n",
      "iteration 12329, loss: 0.002135810675099492\n",
      "iteration 12330, loss: 0.0018393192440271378\n",
      "iteration 12331, loss: 0.0021554105915129185\n",
      "iteration 12332, loss: 0.002014212077483535\n",
      "iteration 12333, loss: 0.0019478246103972197\n",
      "iteration 12334, loss: 0.0020117899402976036\n",
      "iteration 12335, loss: 0.0020295835565775633\n",
      "iteration 12336, loss: 0.0021807984448969364\n",
      "iteration 12337, loss: 0.001952424761839211\n",
      "iteration 12338, loss: 0.002038104459643364\n",
      "iteration 12339, loss: 0.0016934388549998403\n",
      "iteration 12340, loss: 0.0020966881420463324\n",
      "iteration 12341, loss: 0.0017008127178996801\n",
      "iteration 12342, loss: 0.0019077747128903866\n",
      "iteration 12343, loss: 0.0020893560722470284\n",
      "iteration 12344, loss: 0.0019711446948349476\n",
      "iteration 12345, loss: 0.001891045132651925\n",
      "iteration 12346, loss: 0.001868044724687934\n",
      "iteration 12347, loss: 0.002279232256114483\n",
      "iteration 12348, loss: 0.0021696300245821476\n",
      "iteration 12349, loss: 0.0018268125131726265\n",
      "iteration 12350, loss: 0.0023087302688509226\n",
      "iteration 12351, loss: 0.0020436998456716537\n",
      "iteration 12352, loss: 0.002215838059782982\n",
      "iteration 12353, loss: 0.0019565890543162823\n",
      "iteration 12354, loss: 0.0022630509920418262\n",
      "iteration 12355, loss: 0.0019300831481814384\n",
      "iteration 12356, loss: 0.0018126319628208876\n",
      "iteration 12357, loss: 0.0020819930359721184\n",
      "iteration 12358, loss: 0.001752158161252737\n",
      "iteration 12359, loss: 0.0020714872516691685\n",
      "iteration 12360, loss: 0.0021047594491392374\n",
      "iteration 12361, loss: 0.002670983551070094\n",
      "iteration 12362, loss: 0.00197116332128644\n",
      "iteration 12363, loss: 0.001740725478157401\n",
      "iteration 12364, loss: 0.0021236203610897064\n",
      "iteration 12365, loss: 0.001840714132413268\n",
      "iteration 12366, loss: 0.001721937325783074\n",
      "iteration 12367, loss: 0.0015782442642375827\n",
      "iteration 12368, loss: 0.0021228957921266556\n",
      "iteration 12369, loss: 0.0017453656764701009\n",
      "iteration 12370, loss: 0.00238633481785655\n",
      "iteration 12371, loss: 0.0016811882378533483\n",
      "iteration 12372, loss: 0.0019948668777942657\n",
      "iteration 12373, loss: 0.0020381170324981213\n",
      "iteration 12374, loss: 0.002416711300611496\n",
      "iteration 12375, loss: 0.0019066997338086367\n",
      "iteration 12376, loss: 0.0023421563673764467\n",
      "iteration 12377, loss: 0.0020279043819755316\n",
      "iteration 12378, loss: 0.0020516456570476294\n",
      "iteration 12379, loss: 0.0021192862186580896\n",
      "iteration 12380, loss: 0.0018427125178277493\n",
      "iteration 12381, loss: 0.0017204898176714778\n",
      "iteration 12382, loss: 0.0021745585836470127\n",
      "iteration 12383, loss: 0.0018405299633741379\n",
      "iteration 12384, loss: 0.0017988637555390596\n",
      "iteration 12385, loss: 0.0026360838674008846\n",
      "iteration 12386, loss: 0.0019365684129297733\n",
      "iteration 12387, loss: 0.0020994506776332855\n",
      "iteration 12388, loss: 0.0018626494565978646\n",
      "iteration 12389, loss: 0.0022599692456424236\n",
      "iteration 12390, loss: 0.0019398631993681192\n",
      "iteration 12391, loss: 0.00199308549053967\n",
      "iteration 12392, loss: 0.002259118016809225\n",
      "iteration 12393, loss: 0.001961690140888095\n",
      "iteration 12394, loss: 0.001735851401463151\n",
      "iteration 12395, loss: 0.0017466386780142784\n",
      "iteration 12396, loss: 0.002228262834250927\n",
      "iteration 12397, loss: 0.002199208363890648\n",
      "iteration 12398, loss: 0.0021035242825746536\n",
      "iteration 12399, loss: 0.001902677002362907\n",
      "iteration 12400, loss: 0.001878837589174509\n",
      "iteration 12401, loss: 0.002450545784085989\n",
      "iteration 12402, loss: 0.00203600968234241\n",
      "iteration 12403, loss: 0.0021926844492554665\n",
      "iteration 12404, loss: 0.0021272609010338783\n",
      "iteration 12405, loss: 0.0020805406384170055\n",
      "iteration 12406, loss: 0.0018659342313185334\n",
      "iteration 12407, loss: 0.002222940791398287\n",
      "iteration 12408, loss: 0.0022000500466674566\n",
      "iteration 12409, loss: 0.00213936110958457\n",
      "iteration 12410, loss: 0.0020936266519129276\n",
      "iteration 12411, loss: 0.0016664552967995405\n",
      "iteration 12412, loss: 0.0022312276996672153\n",
      "iteration 12413, loss: 0.002143201418220997\n",
      "iteration 12414, loss: 0.0025665219873189926\n",
      "iteration 12415, loss: 0.0020291178952902555\n",
      "iteration 12416, loss: 0.0019891716074198484\n",
      "iteration 12417, loss: 0.0018306782003492117\n",
      "iteration 12418, loss: 0.0017511994810774922\n",
      "iteration 12419, loss: 0.00187990115955472\n",
      "iteration 12420, loss: 0.0019331986550241709\n",
      "iteration 12421, loss: 0.0019256421364843845\n",
      "iteration 12422, loss: 0.0017015531193464994\n",
      "iteration 12423, loss: 0.0022694228682667017\n",
      "iteration 12424, loss: 0.0017818856285884976\n",
      "iteration 12425, loss: 0.002628688234835863\n",
      "iteration 12426, loss: 0.0018337996443733573\n",
      "iteration 12427, loss: 0.002040489576756954\n",
      "iteration 12428, loss: 0.002275777980685234\n",
      "iteration 12429, loss: 0.0024120479356497526\n",
      "iteration 12430, loss: 0.0019089729757979512\n",
      "iteration 12431, loss: 0.001979151740670204\n",
      "iteration 12432, loss: 0.002199179958552122\n",
      "iteration 12433, loss: 0.0024487636983394623\n",
      "iteration 12434, loss: 0.0022187994327396154\n",
      "iteration 12435, loss: 0.002069764072075486\n",
      "iteration 12436, loss: 0.0018494266550987959\n",
      "iteration 12437, loss: 0.002571696415543556\n",
      "iteration 12438, loss: 0.002053404226899147\n",
      "iteration 12439, loss: 0.002679042983800173\n",
      "iteration 12440, loss: 0.0017521751578897238\n",
      "iteration 12441, loss: 0.0022154739126563072\n",
      "iteration 12442, loss: 0.0017681536264717579\n",
      "iteration 12443, loss: 0.0023232558742165565\n",
      "iteration 12444, loss: 0.0020916827488690615\n",
      "iteration 12445, loss: 0.002233309904113412\n",
      "iteration 12446, loss: 0.0023663374595344067\n",
      "iteration 12447, loss: 0.002067453693598509\n",
      "iteration 12448, loss: 0.0026656626723706722\n",
      "iteration 12449, loss: 0.0020395047031342983\n",
      "iteration 12450, loss: 0.0021325990092009306\n",
      "iteration 12451, loss: 0.001958136912435293\n",
      "iteration 12452, loss: 0.002057783305644989\n",
      "iteration 12453, loss: 0.0023412290029227734\n",
      "iteration 12454, loss: 0.002234984189271927\n",
      "iteration 12455, loss: 0.0021623526699841022\n",
      "iteration 12456, loss: 0.002024767454713583\n",
      "iteration 12457, loss: 0.002065590349957347\n",
      "iteration 12458, loss: 0.0020609255880117416\n",
      "iteration 12459, loss: 0.0023812816943973303\n",
      "iteration 12460, loss: 0.0019556477200239897\n",
      "iteration 12461, loss: 0.002119279932230711\n",
      "iteration 12462, loss: 0.0022585466504096985\n",
      "iteration 12463, loss: 0.002030644565820694\n",
      "iteration 12464, loss: 0.0018027634359896183\n",
      "iteration 12465, loss: 0.002250099554657936\n",
      "iteration 12466, loss: 0.002334972843527794\n",
      "iteration 12467, loss: 0.0025031156837940216\n",
      "iteration 12468, loss: 0.002119806595146656\n",
      "iteration 12469, loss: 0.0023204353637993336\n",
      "iteration 12470, loss: 0.0030889620538800955\n",
      "iteration 12471, loss: 0.0021094363182783127\n",
      "iteration 12472, loss: 0.0025456929579377174\n",
      "iteration 12473, loss: 0.0024840408004820347\n",
      "iteration 12474, loss: 0.0022126222029328346\n",
      "iteration 12475, loss: 0.002265496179461479\n",
      "iteration 12476, loss: 0.0019016792066395283\n",
      "iteration 12477, loss: 0.0017178962007164955\n",
      "iteration 12478, loss: 0.002134446520358324\n",
      "iteration 12479, loss: 0.001989696640521288\n",
      "iteration 12480, loss: 0.0023982045240700245\n",
      "iteration 12481, loss: 0.0019411665853112936\n",
      "iteration 12482, loss: 0.0020620073191821575\n",
      "iteration 12483, loss: 0.0024275179021060467\n",
      "iteration 12484, loss: 0.001974661136046052\n",
      "iteration 12485, loss: 0.0017947792075574398\n",
      "iteration 12486, loss: 0.002484842436388135\n",
      "iteration 12487, loss: 0.0024239469785243273\n",
      "iteration 12488, loss: 0.002231379272416234\n",
      "iteration 12489, loss: 0.0020927567966282368\n",
      "iteration 12490, loss: 0.002285615773871541\n",
      "iteration 12491, loss: 0.002323517110198736\n",
      "iteration 12492, loss: 0.001871492713689804\n",
      "iteration 12493, loss: 0.0023749135434627533\n",
      "iteration 12494, loss: 0.0020445697009563446\n",
      "iteration 12495, loss: 0.0019317815313115716\n",
      "iteration 12496, loss: 0.0016614305786788464\n",
      "iteration 12497, loss: 0.0017923214472830296\n",
      "iteration 12498, loss: 0.0020477501675486565\n",
      "iteration 12499, loss: 0.0017004141118377447\n",
      "iteration 12500, loss: 0.001902618445456028\n",
      "iteration 12501, loss: 0.002051771152764559\n",
      "iteration 12502, loss: 0.0023334138095378876\n",
      "iteration 12503, loss: 0.0022495496086776257\n",
      "iteration 12504, loss: 0.002114462899044156\n",
      "iteration 12505, loss: 0.0016799287404865026\n",
      "iteration 12506, loss: 0.001999637810513377\n",
      "iteration 12507, loss: 0.0022601766977459192\n",
      "iteration 12508, loss: 0.0017896175850182772\n",
      "iteration 12509, loss: 0.002225138247013092\n",
      "iteration 12510, loss: 0.002014029771089554\n",
      "iteration 12511, loss: 0.002232091501355171\n",
      "iteration 12512, loss: 0.0017195256659761071\n",
      "iteration 12513, loss: 0.001839202712289989\n",
      "iteration 12514, loss: 0.0022087639663368464\n",
      "iteration 12515, loss: 0.0019729386549443007\n",
      "iteration 12516, loss: 0.0017586819594725966\n",
      "iteration 12517, loss: 0.0020640024449676275\n",
      "iteration 12518, loss: 0.0019670960027724504\n",
      "iteration 12519, loss: 0.002154136775061488\n",
      "iteration 12520, loss: 0.0020484840497374535\n",
      "iteration 12521, loss: 0.001655990956351161\n",
      "iteration 12522, loss: 0.0025818219874054193\n",
      "iteration 12523, loss: 0.0018825741717591882\n",
      "iteration 12524, loss: 0.0018024386372417212\n",
      "iteration 12525, loss: 0.0016535960603505373\n",
      "iteration 12526, loss: 0.0021777465008199215\n",
      "iteration 12527, loss: 0.002148226136341691\n",
      "iteration 12528, loss: 0.0020250973757356405\n",
      "iteration 12529, loss: 0.001872396795079112\n",
      "iteration 12530, loss: 0.002164764329791069\n",
      "iteration 12531, loss: 0.0020078020170331\n",
      "iteration 12532, loss: 0.002086581662297249\n",
      "iteration 12533, loss: 0.001683559501543641\n",
      "iteration 12534, loss: 0.0019818036817014217\n",
      "iteration 12535, loss: 0.0018527342472225428\n",
      "iteration 12536, loss: 0.0022294209338724613\n",
      "iteration 12537, loss: 0.001991165103390813\n",
      "iteration 12538, loss: 0.0021722563542425632\n",
      "iteration 12539, loss: 0.0021376428194344044\n",
      "iteration 12540, loss: 0.0020592266228049994\n",
      "iteration 12541, loss: 0.002091278787702322\n",
      "iteration 12542, loss: 0.0020142144057899714\n",
      "iteration 12543, loss: 0.001423726207576692\n",
      "iteration 12544, loss: 0.0016880840994417667\n",
      "iteration 12545, loss: 0.0014303146163001657\n",
      "iteration 12546, loss: 0.0019937115721404552\n",
      "iteration 12547, loss: 0.0021345275454223156\n",
      "iteration 12548, loss: 0.0020478698424994946\n",
      "iteration 12549, loss: 0.0019426061771810055\n",
      "iteration 12550, loss: 0.0020715058781206608\n",
      "iteration 12551, loss: 0.0020574890077114105\n",
      "iteration 12552, loss: 0.0016995210899040103\n",
      "iteration 12553, loss: 0.002075349446386099\n",
      "iteration 12554, loss: 0.0019718962721526623\n",
      "iteration 12555, loss: 0.0018113661790266633\n",
      "iteration 12556, loss: 0.0018968742806464434\n",
      "iteration 12557, loss: 0.001819874974898994\n",
      "iteration 12558, loss: 0.0017958967946469784\n",
      "iteration 12559, loss: 0.0020016897469758987\n",
      "iteration 12560, loss: 0.0017747594974935055\n",
      "iteration 12561, loss: 0.002054629847407341\n",
      "iteration 12562, loss: 0.0019243068527430296\n",
      "iteration 12563, loss: 0.0017505234573036432\n",
      "iteration 12564, loss: 0.0014550946652889252\n",
      "iteration 12565, loss: 0.002061164937913418\n",
      "iteration 12566, loss: 0.0017482922412455082\n",
      "iteration 12567, loss: 0.0018576944712549448\n",
      "iteration 12568, loss: 0.0018592167180031538\n",
      "iteration 12569, loss: 0.0018892120569944382\n",
      "iteration 12570, loss: 0.0017042651306837797\n",
      "iteration 12571, loss: 0.0020248121581971645\n",
      "iteration 12572, loss: 0.0017603719606995583\n",
      "iteration 12573, loss: 0.0016497776377946138\n",
      "iteration 12574, loss: 0.001964742783457041\n",
      "iteration 12575, loss: 0.002451497595757246\n",
      "iteration 12576, loss: 0.0020747967064380646\n",
      "iteration 12577, loss: 0.0017602930311113596\n",
      "iteration 12578, loss: 0.0019911727868020535\n",
      "iteration 12579, loss: 0.0018364820862188935\n",
      "iteration 12580, loss: 0.0019532740116119385\n",
      "iteration 12581, loss: 0.0021463714074343443\n",
      "iteration 12582, loss: 0.00201453547924757\n",
      "iteration 12583, loss: 0.0024350089952349663\n",
      "iteration 12584, loss: 0.0019805936608463526\n",
      "iteration 12585, loss: 0.0020131042692810297\n",
      "iteration 12586, loss: 0.002121336990967393\n",
      "iteration 12587, loss: 0.0020077049266546965\n",
      "iteration 12588, loss: 0.0019113331800326705\n",
      "iteration 12589, loss: 0.002116523217409849\n",
      "iteration 12590, loss: 0.0021560820750892162\n",
      "iteration 12591, loss: 0.0020703249610960484\n",
      "iteration 12592, loss: 0.00221742270514369\n",
      "iteration 12593, loss: 0.0015737523790448904\n",
      "iteration 12594, loss: 0.0019133614841848612\n",
      "iteration 12595, loss: 0.001838858937844634\n",
      "iteration 12596, loss: 0.0018117991276085377\n",
      "iteration 12597, loss: 0.0021372539922595024\n",
      "iteration 12598, loss: 0.0021619985345751047\n",
      "iteration 12599, loss: 0.001772517804056406\n",
      "iteration 12600, loss: 0.0015126685611903667\n",
      "iteration 12601, loss: 0.0019312932854518294\n",
      "iteration 12602, loss: 0.0019186476711183786\n",
      "iteration 12603, loss: 0.0020229737274348736\n",
      "iteration 12604, loss: 0.0016536895418539643\n",
      "iteration 12605, loss: 0.0018627039389684796\n",
      "iteration 12606, loss: 0.0019822693429887295\n",
      "iteration 12607, loss: 0.0018774992786347866\n",
      "iteration 12608, loss: 0.0017804686212912202\n",
      "iteration 12609, loss: 0.002110061701387167\n",
      "iteration 12610, loss: 0.0019435952417552471\n",
      "iteration 12611, loss: 0.0015724593540653586\n",
      "iteration 12612, loss: 0.0019932473078370094\n",
      "iteration 12613, loss: 0.002130475826561451\n",
      "iteration 12614, loss: 0.0022153339814394712\n",
      "iteration 12615, loss: 0.001979069085791707\n",
      "iteration 12616, loss: 0.0018949781078845263\n",
      "iteration 12617, loss: 0.00219519529491663\n",
      "iteration 12618, loss: 0.0022150930017232895\n",
      "iteration 12619, loss: 0.002020976273342967\n",
      "iteration 12620, loss: 0.0015272870659828186\n",
      "iteration 12621, loss: 0.0019666405860334635\n",
      "iteration 12622, loss: 0.0022337203845381737\n",
      "iteration 12623, loss: 0.0017626398475840688\n",
      "iteration 12624, loss: 0.0022314349189400673\n",
      "iteration 12625, loss: 0.0016185403801500797\n",
      "iteration 12626, loss: 0.0019270798657089472\n",
      "iteration 12627, loss: 0.0019000805914402008\n",
      "iteration 12628, loss: 0.002026734873652458\n",
      "iteration 12629, loss: 0.0018067818600684404\n",
      "iteration 12630, loss: 0.0019267151365056634\n",
      "iteration 12631, loss: 0.0019159878138452768\n",
      "iteration 12632, loss: 0.0021155737340450287\n",
      "iteration 12633, loss: 0.0021679478231817484\n",
      "iteration 12634, loss: 0.002239563036710024\n",
      "iteration 12635, loss: 0.0017226357012987137\n",
      "iteration 12636, loss: 0.0021621081978082657\n",
      "iteration 12637, loss: 0.002208416350185871\n",
      "iteration 12638, loss: 0.0018200897611677647\n",
      "iteration 12639, loss: 0.0018378604436293244\n",
      "iteration 12640, loss: 0.001583113451488316\n",
      "iteration 12641, loss: 0.0018551326356828213\n",
      "iteration 12642, loss: 0.001715121092274785\n",
      "iteration 12643, loss: 0.0016947716940194368\n",
      "iteration 12644, loss: 0.0018761856481432915\n",
      "iteration 12645, loss: 0.0018818527460098267\n",
      "iteration 12646, loss: 0.0016615466447547078\n",
      "iteration 12647, loss: 0.0017218438442796469\n",
      "iteration 12648, loss: 0.0016897767782211304\n",
      "iteration 12649, loss: 0.0020853832829743624\n",
      "iteration 12650, loss: 0.002077855635434389\n",
      "iteration 12651, loss: 0.001750855939462781\n",
      "iteration 12652, loss: 0.0019760802388191223\n",
      "iteration 12653, loss: 0.0017732327105477452\n",
      "iteration 12654, loss: 0.0015645077219232917\n",
      "iteration 12655, loss: 0.00229744054377079\n",
      "iteration 12656, loss: 0.0022932207211852074\n",
      "iteration 12657, loss: 0.002258904743939638\n",
      "iteration 12658, loss: 0.0022846367210149765\n",
      "iteration 12659, loss: 0.002207670360803604\n",
      "iteration 12660, loss: 0.0021153069101274014\n",
      "iteration 12661, loss: 0.0020419396460056305\n",
      "iteration 12662, loss: 0.0019197657238692045\n",
      "iteration 12663, loss: 0.0022223228588700294\n",
      "iteration 12664, loss: 0.002481289440765977\n",
      "iteration 12665, loss: 0.00262945843860507\n",
      "iteration 12666, loss: 0.002374510280787945\n",
      "iteration 12667, loss: 0.0019322631414979696\n",
      "iteration 12668, loss: 0.0021855803206562996\n",
      "iteration 12669, loss: 0.0019860276952385902\n",
      "iteration 12670, loss: 0.0017799183260649443\n",
      "iteration 12671, loss: 0.0018094138940796256\n",
      "iteration 12672, loss: 0.0017323540523648262\n",
      "iteration 12673, loss: 0.002310898620635271\n",
      "iteration 12674, loss: 0.0019976741168648005\n",
      "iteration 12675, loss: 0.0025295019149780273\n",
      "iteration 12676, loss: 0.0017386033432558179\n",
      "iteration 12677, loss: 0.0020796956960111856\n",
      "iteration 12678, loss: 0.002126081380993128\n",
      "iteration 12679, loss: 0.002155024092644453\n",
      "iteration 12680, loss: 0.0015796292573213577\n",
      "iteration 12681, loss: 0.0026560104452073574\n",
      "iteration 12682, loss: 0.0020301577169448137\n",
      "iteration 12683, loss: 0.0015764499548822641\n",
      "iteration 12684, loss: 0.0018913672538474202\n",
      "iteration 12685, loss: 0.0021127713844180107\n",
      "iteration 12686, loss: 0.0018092538230121136\n",
      "iteration 12687, loss: 0.002432409208267927\n",
      "iteration 12688, loss: 0.0017693797126412392\n",
      "iteration 12689, loss: 0.0021298020146787167\n",
      "iteration 12690, loss: 0.0019061323255300522\n",
      "iteration 12691, loss: 0.0020133526995778084\n",
      "iteration 12692, loss: 0.0020447573624551296\n",
      "iteration 12693, loss: 0.0021072858944535255\n",
      "iteration 12694, loss: 0.00191051559522748\n",
      "iteration 12695, loss: 0.00229309662245214\n",
      "iteration 12696, loss: 0.0019791177473962307\n",
      "iteration 12697, loss: 0.0020730076357722282\n",
      "iteration 12698, loss: 0.002181391231715679\n",
      "iteration 12699, loss: 0.0023413917515426874\n",
      "iteration 12700, loss: 0.0019454992143437266\n",
      "iteration 12701, loss: 0.0017331879353150725\n",
      "iteration 12702, loss: 0.0019890866242349148\n",
      "iteration 12703, loss: 0.001994880149140954\n",
      "iteration 12704, loss: 0.001777714118361473\n",
      "iteration 12705, loss: 0.0015446282923221588\n",
      "iteration 12706, loss: 0.0019519557245075703\n",
      "iteration 12707, loss: 0.0018139230087399483\n",
      "iteration 12708, loss: 0.0018769283778965473\n",
      "iteration 12709, loss: 0.002370960544794798\n",
      "iteration 12710, loss: 0.001924528507515788\n",
      "iteration 12711, loss: 0.002011843491345644\n",
      "iteration 12712, loss: 0.002267136238515377\n",
      "iteration 12713, loss: 0.002415272407233715\n",
      "iteration 12714, loss: 0.0018159698229283094\n",
      "iteration 12715, loss: 0.002246484160423279\n",
      "iteration 12716, loss: 0.0022925017401576042\n",
      "iteration 12717, loss: 0.002159079071134329\n",
      "iteration 12718, loss: 0.002362677827477455\n",
      "iteration 12719, loss: 0.001861048280261457\n",
      "iteration 12720, loss: 0.0019733700901269913\n",
      "iteration 12721, loss: 0.002260893117636442\n",
      "iteration 12722, loss: 0.0018382050329819322\n",
      "iteration 12723, loss: 0.0017746764933690429\n",
      "iteration 12724, loss: 0.0015267101116478443\n",
      "iteration 12725, loss: 0.0018429913325235248\n",
      "iteration 12726, loss: 0.0024933458771556616\n",
      "iteration 12727, loss: 0.002883677836507559\n",
      "iteration 12728, loss: 0.0017943568527698517\n",
      "iteration 12729, loss: 0.0022908677347004414\n",
      "iteration 12730, loss: 0.0018663887167349458\n",
      "iteration 12731, loss: 0.002871970646083355\n",
      "iteration 12732, loss: 0.002334960736334324\n",
      "iteration 12733, loss: 0.0019125081598758698\n",
      "iteration 12734, loss: 0.0019969255663454533\n",
      "iteration 12735, loss: 0.0020891258027404547\n",
      "iteration 12736, loss: 0.0017396921757608652\n",
      "iteration 12737, loss: 0.0025062034837901592\n",
      "iteration 12738, loss: 0.0016810803208500147\n",
      "iteration 12739, loss: 0.0017575421370565891\n",
      "iteration 12740, loss: 0.0021942558232694864\n",
      "iteration 12741, loss: 0.002154019195586443\n",
      "iteration 12742, loss: 0.0024461043067276478\n",
      "iteration 12743, loss: 0.0021473662927746773\n",
      "iteration 12744, loss: 0.0018853273941203952\n",
      "iteration 12745, loss: 0.001962918322533369\n",
      "iteration 12746, loss: 0.0017333319410681725\n",
      "iteration 12747, loss: 0.001806913292966783\n",
      "iteration 12748, loss: 0.0019141011871397495\n",
      "iteration 12749, loss: 0.002294630976393819\n",
      "iteration 12750, loss: 0.0017249995144084096\n",
      "iteration 12751, loss: 0.0018379776738584042\n",
      "iteration 12752, loss: 0.002127023646607995\n",
      "iteration 12753, loss: 0.0017899810336530209\n",
      "iteration 12754, loss: 0.0023207610938698053\n",
      "iteration 12755, loss: 0.0017892062896862626\n",
      "iteration 12756, loss: 0.002310226671397686\n",
      "iteration 12757, loss: 0.002339713741093874\n",
      "iteration 12758, loss: 0.002072565257549286\n",
      "iteration 12759, loss: 0.0021491560619324446\n",
      "iteration 12760, loss: 0.0019398881122469902\n",
      "iteration 12761, loss: 0.0024547192733734846\n",
      "iteration 12762, loss: 0.002108517102897167\n",
      "iteration 12763, loss: 0.002204522490501404\n",
      "iteration 12764, loss: 0.002347394824028015\n",
      "iteration 12765, loss: 0.0017754054861143231\n",
      "iteration 12766, loss: 0.002368861809372902\n",
      "iteration 12767, loss: 0.0017400623764842749\n",
      "iteration 12768, loss: 0.0023477538488805294\n",
      "iteration 12769, loss: 0.0019342054147273302\n",
      "iteration 12770, loss: 0.0018425910966470838\n",
      "iteration 12771, loss: 0.0017034035408869386\n",
      "iteration 12772, loss: 0.0016558635979890823\n",
      "iteration 12773, loss: 0.002395746996626258\n",
      "iteration 12774, loss: 0.002078742254525423\n",
      "iteration 12775, loss: 0.00169318076223135\n",
      "iteration 12776, loss: 0.002165096113458276\n",
      "iteration 12777, loss: 0.0016789160436019301\n",
      "iteration 12778, loss: 0.0020152810029685497\n",
      "iteration 12779, loss: 0.0021668551489710808\n",
      "iteration 12780, loss: 0.0018087269272655249\n",
      "iteration 12781, loss: 0.002257151994854212\n",
      "iteration 12782, loss: 0.001591214444488287\n",
      "iteration 12783, loss: 0.0019248513272032142\n",
      "iteration 12784, loss: 0.0021495125256478786\n",
      "iteration 12785, loss: 0.001925355987623334\n",
      "iteration 12786, loss: 0.0020114961080253124\n",
      "iteration 12787, loss: 0.001911926781758666\n",
      "iteration 12788, loss: 0.0018025654135271907\n",
      "iteration 12789, loss: 0.00143427646253258\n",
      "iteration 12790, loss: 0.001869539963081479\n",
      "iteration 12791, loss: 0.0017403913661837578\n",
      "iteration 12792, loss: 0.0017079066019505262\n",
      "iteration 12793, loss: 0.0017239549197256565\n",
      "iteration 12794, loss: 0.0019655185751616955\n",
      "iteration 12795, loss: 0.001812738599255681\n",
      "iteration 12796, loss: 0.0018751100869849324\n",
      "iteration 12797, loss: 0.0018336961511522532\n",
      "iteration 12798, loss: 0.002005793619900942\n",
      "iteration 12799, loss: 0.0017271754331886768\n",
      "iteration 12800, loss: 0.0018110708333551884\n",
      "iteration 12801, loss: 0.0018568327650427818\n",
      "iteration 12802, loss: 0.0017767671961337328\n",
      "iteration 12803, loss: 0.0021575368009507656\n",
      "iteration 12804, loss: 0.0016134715406224132\n",
      "iteration 12805, loss: 0.0013717785477638245\n",
      "iteration 12806, loss: 0.0019949572160840034\n",
      "iteration 12807, loss: 0.0018916944973170757\n",
      "iteration 12808, loss: 0.0021512152161449194\n",
      "iteration 12809, loss: 0.0015930323861539364\n",
      "iteration 12810, loss: 0.0017977573443204165\n",
      "iteration 12811, loss: 0.001508735353127122\n",
      "iteration 12812, loss: 0.0017687082290649414\n",
      "iteration 12813, loss: 0.002007321920245886\n",
      "iteration 12814, loss: 0.0018049299251288176\n",
      "iteration 12815, loss: 0.0019364095060154796\n",
      "iteration 12816, loss: 0.0019776704721152782\n",
      "iteration 12817, loss: 0.0013511847937479615\n",
      "iteration 12818, loss: 0.0016046208329498768\n",
      "iteration 12819, loss: 0.0021425262093544006\n",
      "iteration 12820, loss: 0.0022785309702157974\n",
      "iteration 12821, loss: 0.0019664315041154623\n",
      "iteration 12822, loss: 0.0018404230941087008\n",
      "iteration 12823, loss: 0.0016334173269569874\n",
      "iteration 12824, loss: 0.0021398155950009823\n",
      "iteration 12825, loss: 0.002125263912603259\n",
      "iteration 12826, loss: 0.0021993350237607956\n",
      "iteration 12827, loss: 0.0019179580267518759\n",
      "iteration 12828, loss: 0.0018695599865168333\n",
      "iteration 12829, loss: 0.0019047585083171725\n",
      "iteration 12830, loss: 0.0017784714000299573\n",
      "iteration 12831, loss: 0.002070006914436817\n",
      "iteration 12832, loss: 0.0019500006455928087\n",
      "iteration 12833, loss: 0.0015911123482510448\n",
      "iteration 12834, loss: 0.001915177796036005\n",
      "iteration 12835, loss: 0.001628286438062787\n",
      "iteration 12836, loss: 0.0016518909251317382\n",
      "iteration 12837, loss: 0.0018112158868461847\n",
      "iteration 12838, loss: 0.0019191533792763948\n",
      "iteration 12839, loss: 0.002154852729290724\n",
      "iteration 12840, loss: 0.0019818181172013283\n",
      "iteration 12841, loss: 0.0016231269109994173\n",
      "iteration 12842, loss: 0.0021531605161726475\n",
      "iteration 12843, loss: 0.0027754264883697033\n",
      "iteration 12844, loss: 0.001923576113767922\n",
      "iteration 12845, loss: 0.002114925067871809\n",
      "iteration 12846, loss: 0.0021904788445681334\n",
      "iteration 12847, loss: 0.002012389712035656\n",
      "iteration 12848, loss: 0.0018682279624044895\n",
      "iteration 12849, loss: 0.002080315724015236\n",
      "iteration 12850, loss: 0.0023023304529488087\n",
      "iteration 12851, loss: 0.002048396971076727\n",
      "iteration 12852, loss: 0.002195421140640974\n",
      "iteration 12853, loss: 0.0021872827783226967\n",
      "iteration 12854, loss: 0.0020798519253730774\n",
      "iteration 12855, loss: 0.001918114721775055\n",
      "iteration 12856, loss: 0.0019538872875273228\n",
      "iteration 12857, loss: 0.0018275631591677666\n",
      "iteration 12858, loss: 0.0017555614467710257\n",
      "iteration 12859, loss: 0.002238793298602104\n",
      "iteration 12860, loss: 0.0022928095422685146\n",
      "iteration 12861, loss: 0.002041501458734274\n",
      "iteration 12862, loss: 0.0019525893731042743\n",
      "iteration 12863, loss: 0.0017033617477864027\n",
      "iteration 12864, loss: 0.001839070813730359\n",
      "iteration 12865, loss: 0.0017984375590458512\n",
      "iteration 12866, loss: 0.0015945847844704986\n",
      "iteration 12867, loss: 0.0017988721374422312\n",
      "iteration 12868, loss: 0.0021181791089475155\n",
      "iteration 12869, loss: 0.002011677948758006\n",
      "iteration 12870, loss: 0.001936196582391858\n",
      "iteration 12871, loss: 0.0016150756273418665\n",
      "iteration 12872, loss: 0.001956288469955325\n",
      "iteration 12873, loss: 0.002049144357442856\n",
      "iteration 12874, loss: 0.0018827649764716625\n",
      "iteration 12875, loss: 0.0017771170241758227\n",
      "iteration 12876, loss: 0.001697993604466319\n",
      "iteration 12877, loss: 0.002006747992709279\n",
      "iteration 12878, loss: 0.0021406346932053566\n",
      "iteration 12879, loss: 0.002266564639285207\n",
      "iteration 12880, loss: 0.00195990689098835\n",
      "iteration 12881, loss: 0.002160175470635295\n",
      "iteration 12882, loss: 0.0020004226826131344\n",
      "iteration 12883, loss: 0.002328836126253009\n",
      "iteration 12884, loss: 0.0019715572707355022\n",
      "iteration 12885, loss: 0.0016082411166280508\n",
      "iteration 12886, loss: 0.0017969502368941903\n",
      "iteration 12887, loss: 0.0016890178667381406\n",
      "iteration 12888, loss: 0.0021095317788422108\n",
      "iteration 12889, loss: 0.0017694520065560937\n",
      "iteration 12890, loss: 0.0022137905471026897\n",
      "iteration 12891, loss: 0.0017023622058331966\n",
      "iteration 12892, loss: 0.001629204023629427\n",
      "iteration 12893, loss: 0.0019438521703705192\n",
      "iteration 12894, loss: 0.001584300771355629\n",
      "iteration 12895, loss: 0.002105122432112694\n",
      "iteration 12896, loss: 0.002012214157730341\n",
      "iteration 12897, loss: 0.001985145267099142\n",
      "iteration 12898, loss: 0.0019658294040709734\n",
      "iteration 12899, loss: 0.0020102737471461296\n",
      "iteration 12900, loss: 0.0015454571694135666\n",
      "iteration 12901, loss: 0.0020158481784164906\n",
      "iteration 12902, loss: 0.0018071540398523211\n",
      "iteration 12903, loss: 0.0020242114551365376\n",
      "iteration 12904, loss: 0.002173776738345623\n",
      "iteration 12905, loss: 0.002106384141370654\n",
      "iteration 12906, loss: 0.0020569697953760624\n",
      "iteration 12907, loss: 0.001786587992683053\n",
      "iteration 12908, loss: 0.0021502019371837378\n",
      "iteration 12909, loss: 0.0023011264856904745\n",
      "iteration 12910, loss: 0.0018043705495074391\n",
      "iteration 12911, loss: 0.0022940714843571186\n",
      "iteration 12912, loss: 0.001813601003959775\n",
      "iteration 12913, loss: 0.002199390670284629\n",
      "iteration 12914, loss: 0.0017904640408232808\n",
      "iteration 12915, loss: 0.00182751240208745\n",
      "iteration 12916, loss: 0.0017916373908519745\n",
      "iteration 12917, loss: 0.001782421488314867\n",
      "iteration 12918, loss: 0.001792993862181902\n",
      "iteration 12919, loss: 0.0020020445808768272\n",
      "iteration 12920, loss: 0.0018106941133737564\n",
      "iteration 12921, loss: 0.0020309716928750277\n",
      "iteration 12922, loss: 0.0019838300067931414\n",
      "iteration 12923, loss: 0.002168253529816866\n",
      "iteration 12924, loss: 0.0017505106516182423\n",
      "iteration 12925, loss: 0.002286580391228199\n",
      "iteration 12926, loss: 0.002031434327363968\n",
      "iteration 12927, loss: 0.0020370613783597946\n",
      "iteration 12928, loss: 0.0016465046210214496\n",
      "iteration 12929, loss: 0.0019051677081733942\n",
      "iteration 12930, loss: 0.0018065755721181631\n",
      "iteration 12931, loss: 0.0016218767268583179\n",
      "iteration 12932, loss: 0.002390438923612237\n",
      "iteration 12933, loss: 0.0018838096875697374\n",
      "iteration 12934, loss: 0.0023061148822307587\n",
      "iteration 12935, loss: 0.002080867998301983\n",
      "iteration 12936, loss: 0.0017363096121698618\n",
      "iteration 12937, loss: 0.00211589178070426\n",
      "iteration 12938, loss: 0.0018360971007496119\n",
      "iteration 12939, loss: 0.0016074619488790631\n",
      "iteration 12940, loss: 0.0021181157790124416\n",
      "iteration 12941, loss: 0.0019367008935660124\n",
      "iteration 12942, loss: 0.0018038942944258451\n",
      "iteration 12943, loss: 0.0015884770546108484\n",
      "iteration 12944, loss: 0.0017025689594447613\n",
      "iteration 12945, loss: 0.0020138812251389027\n",
      "iteration 12946, loss: 0.0022308435291051865\n",
      "iteration 12947, loss: 0.00138947949744761\n",
      "iteration 12948, loss: 0.0019067220855504274\n",
      "iteration 12949, loss: 0.0021565768402069807\n",
      "iteration 12950, loss: 0.0018766509601846337\n",
      "iteration 12951, loss: 0.0017112914938479662\n",
      "iteration 12952, loss: 0.002294202335178852\n",
      "iteration 12953, loss: 0.0022168168798089027\n",
      "iteration 12954, loss: 0.0020115235820412636\n",
      "iteration 12955, loss: 0.0020541176199913025\n",
      "iteration 12956, loss: 0.0018359426176175475\n",
      "iteration 12957, loss: 0.0018041529692709446\n",
      "iteration 12958, loss: 0.0016468993853777647\n",
      "iteration 12959, loss: 0.0019908202812075615\n",
      "iteration 12960, loss: 0.0016725563909858465\n",
      "iteration 12961, loss: 0.0017587952315807343\n",
      "iteration 12962, loss: 0.0020469711162149906\n",
      "iteration 12963, loss: 0.00171182700432837\n",
      "iteration 12964, loss: 0.0018578874878585339\n",
      "iteration 12965, loss: 0.0016971052391454577\n",
      "iteration 12966, loss: 0.0020521043334156275\n",
      "iteration 12967, loss: 0.0017927346052601933\n",
      "iteration 12968, loss: 0.0018046488985419273\n",
      "iteration 12969, loss: 0.002092378679662943\n",
      "iteration 12970, loss: 0.0025195442140102386\n",
      "iteration 12971, loss: 0.0018622316420078278\n",
      "iteration 12972, loss: 0.001836253795772791\n",
      "iteration 12973, loss: 0.0018577193841338158\n",
      "iteration 12974, loss: 0.0019369053188711405\n",
      "iteration 12975, loss: 0.0019524542149156332\n",
      "iteration 12976, loss: 0.0018792153568938375\n",
      "iteration 12977, loss: 0.0019235957879573107\n",
      "iteration 12978, loss: 0.0020636948756873608\n",
      "iteration 12979, loss: 0.0016984399408102036\n",
      "iteration 12980, loss: 0.001854128553532064\n",
      "iteration 12981, loss: 0.0022127688862383366\n",
      "iteration 12982, loss: 0.001627572812139988\n",
      "iteration 12983, loss: 0.0018830937333405018\n",
      "iteration 12984, loss: 0.002008476760238409\n",
      "iteration 12985, loss: 0.001709089265204966\n",
      "iteration 12986, loss: 0.001859110314399004\n",
      "iteration 12987, loss: 0.001722431043162942\n",
      "iteration 12988, loss: 0.0019176288042217493\n",
      "iteration 12989, loss: 0.0018415303202345967\n",
      "iteration 12990, loss: 0.002179753500968218\n",
      "iteration 12991, loss: 0.002205952536314726\n",
      "iteration 12992, loss: 0.002225527074187994\n",
      "iteration 12993, loss: 0.0018933187238872051\n",
      "iteration 12994, loss: 0.001727646216750145\n",
      "iteration 12995, loss: 0.0021548536606132984\n",
      "iteration 12996, loss: 0.0018874993547797203\n",
      "iteration 12997, loss: 0.0016151577001437545\n",
      "iteration 12998, loss: 0.0018491183873265982\n",
      "iteration 12999, loss: 0.0018986506620422006\n",
      "iteration 13000, loss: 0.0020834996830672026\n",
      "iteration 13001, loss: 0.0017872676253318787\n",
      "iteration 13002, loss: 0.001765416469424963\n",
      "iteration 13003, loss: 0.0020375230815261602\n",
      "iteration 13004, loss: 0.001824461156502366\n",
      "iteration 13005, loss: 0.0020820251666009426\n",
      "iteration 13006, loss: 0.0023750842083245516\n",
      "iteration 13007, loss: 0.0019258593674749136\n",
      "iteration 13008, loss: 0.0018596615409478545\n",
      "iteration 13009, loss: 0.0017422143137082458\n",
      "iteration 13010, loss: 0.0020081596449017525\n",
      "iteration 13011, loss: 0.002059466205537319\n",
      "iteration 13012, loss: 0.0020729373209178448\n",
      "iteration 13013, loss: 0.0019251913763582706\n",
      "iteration 13014, loss: 0.0018635899759829044\n",
      "iteration 13015, loss: 0.0017983003053814173\n",
      "iteration 13016, loss: 0.001991430763155222\n",
      "iteration 13017, loss: 0.0015562627231702209\n",
      "iteration 13018, loss: 0.0019502643262967467\n",
      "iteration 13019, loss: 0.0018045048927888274\n",
      "iteration 13020, loss: 0.0022368363570421934\n",
      "iteration 13021, loss: 0.0021397345699369907\n",
      "iteration 13022, loss: 0.002193937310948968\n",
      "iteration 13023, loss: 0.0019601345993578434\n",
      "iteration 13024, loss: 0.0018167621456086636\n",
      "iteration 13025, loss: 0.0019032665295526385\n",
      "iteration 13026, loss: 0.002127793850377202\n",
      "iteration 13027, loss: 0.002174980007112026\n",
      "iteration 13028, loss: 0.0018190675182268023\n",
      "iteration 13029, loss: 0.0019067151006311178\n",
      "iteration 13030, loss: 0.002253197832033038\n",
      "iteration 13031, loss: 0.0018673388985916972\n",
      "iteration 13032, loss: 0.0017674267292022705\n",
      "iteration 13033, loss: 0.0017265749629586935\n",
      "iteration 13034, loss: 0.001418181462213397\n",
      "iteration 13035, loss: 0.001641811104491353\n",
      "iteration 13036, loss: 0.0019741510041058064\n",
      "iteration 13037, loss: 0.0017255294369533658\n",
      "iteration 13038, loss: 0.0018413304351270199\n",
      "iteration 13039, loss: 0.0017509802710264921\n",
      "iteration 13040, loss: 0.0015794229693710804\n",
      "iteration 13041, loss: 0.0017991979839280248\n",
      "iteration 13042, loss: 0.0015866921748965979\n",
      "iteration 13043, loss: 0.0018018987029790878\n",
      "iteration 13044, loss: 0.0017430754378437996\n",
      "iteration 13045, loss: 0.0022046021185815334\n",
      "iteration 13046, loss: 0.0018980622990056872\n",
      "iteration 13047, loss: 0.002037114929407835\n",
      "iteration 13048, loss: 0.001852489891462028\n",
      "iteration 13049, loss: 0.0019373400136828423\n",
      "iteration 13050, loss: 0.0018673823215067387\n",
      "iteration 13051, loss: 0.0020358385518193245\n",
      "iteration 13052, loss: 0.002046056557446718\n",
      "iteration 13053, loss: 0.0019891951233148575\n",
      "iteration 13054, loss: 0.002028652699664235\n",
      "iteration 13055, loss: 0.001988236326724291\n",
      "iteration 13056, loss: 0.00205807201564312\n",
      "iteration 13057, loss: 0.0020728465169668198\n",
      "iteration 13058, loss: 0.0018642215291038156\n",
      "iteration 13059, loss: 0.001469175796955824\n",
      "iteration 13060, loss: 0.0018598444294184446\n",
      "iteration 13061, loss: 0.0017570385243743658\n",
      "iteration 13062, loss: 0.0016014917055144906\n",
      "iteration 13063, loss: 0.001655475702136755\n",
      "iteration 13064, loss: 0.0018083868781104684\n",
      "iteration 13065, loss: 0.0020039621740579605\n",
      "iteration 13066, loss: 0.0016735945828258991\n",
      "iteration 13067, loss: 0.002182705793529749\n",
      "iteration 13068, loss: 0.0016503583174198866\n",
      "iteration 13069, loss: 0.0014947912422940135\n",
      "iteration 13070, loss: 0.0019256190862506628\n",
      "iteration 13071, loss: 0.0021722123492509127\n",
      "iteration 13072, loss: 0.0015632698778063059\n",
      "iteration 13073, loss: 0.002308649942278862\n",
      "iteration 13074, loss: 0.0018395158695057034\n",
      "iteration 13075, loss: 0.0021213404834270477\n",
      "iteration 13076, loss: 0.0017421733355149627\n",
      "iteration 13077, loss: 0.0017734698485583067\n",
      "iteration 13078, loss: 0.002105888444930315\n",
      "iteration 13079, loss: 0.0016742271836847067\n",
      "iteration 13080, loss: 0.0018226774409413338\n",
      "iteration 13081, loss: 0.0016080578789114952\n",
      "iteration 13082, loss: 0.0015019085258245468\n",
      "iteration 13083, loss: 0.0017943358980119228\n",
      "iteration 13084, loss: 0.001968714874237776\n",
      "iteration 13085, loss: 0.0021809465251863003\n",
      "iteration 13086, loss: 0.0016112006269395351\n",
      "iteration 13087, loss: 0.002258779015392065\n",
      "iteration 13088, loss: 0.0021701669320464134\n",
      "iteration 13089, loss: 0.0020397091284394264\n",
      "iteration 13090, loss: 0.002171387430280447\n",
      "iteration 13091, loss: 0.0016201504040509462\n",
      "iteration 13092, loss: 0.0017431552987545729\n",
      "iteration 13093, loss: 0.0015700729563832283\n",
      "iteration 13094, loss: 0.0016646840376779437\n",
      "iteration 13095, loss: 0.0018958395812660456\n",
      "iteration 13096, loss: 0.001899386988952756\n",
      "iteration 13097, loss: 0.0018664284143596888\n",
      "iteration 13098, loss: 0.0016950963763520122\n",
      "iteration 13099, loss: 0.0017886892892420292\n",
      "iteration 13100, loss: 0.0020310431718826294\n",
      "iteration 13101, loss: 0.002115170005708933\n",
      "iteration 13102, loss: 0.0020224563777446747\n",
      "iteration 13103, loss: 0.0017715138383209705\n",
      "iteration 13104, loss: 0.0017425023252144456\n",
      "iteration 13105, loss: 0.0016141667729243636\n",
      "iteration 13106, loss: 0.001896532834507525\n",
      "iteration 13107, loss: 0.0016991678858175874\n",
      "iteration 13108, loss: 0.0019796439446508884\n",
      "iteration 13109, loss: 0.002179065253585577\n",
      "iteration 13110, loss: 0.002188344020396471\n",
      "iteration 13111, loss: 0.0020010785665363073\n",
      "iteration 13112, loss: 0.0015896931290626526\n",
      "iteration 13113, loss: 0.0021735546179115772\n",
      "iteration 13114, loss: 0.0017948546446859837\n",
      "iteration 13115, loss: 0.0022412631660699844\n",
      "iteration 13116, loss: 0.001888873754069209\n",
      "iteration 13117, loss: 0.0018053671810775995\n",
      "iteration 13118, loss: 0.0018551957327872515\n",
      "iteration 13119, loss: 0.0021569612435996532\n",
      "iteration 13120, loss: 0.0027268514968454838\n",
      "iteration 13121, loss: 0.0016640634275972843\n",
      "iteration 13122, loss: 0.001983676105737686\n",
      "iteration 13123, loss: 0.0017281037289649248\n",
      "iteration 13124, loss: 0.0014924898277968168\n",
      "iteration 13125, loss: 0.0018568728119134903\n",
      "iteration 13126, loss: 0.0018003550358116627\n",
      "iteration 13127, loss: 0.0016216820804402232\n",
      "iteration 13128, loss: 0.0019468381069600582\n",
      "iteration 13129, loss: 0.0021509029902517796\n",
      "iteration 13130, loss: 0.0018827797612175345\n",
      "iteration 13131, loss: 0.0017494356725364923\n",
      "iteration 13132, loss: 0.0021528585348278284\n",
      "iteration 13133, loss: 0.0015860747080296278\n",
      "iteration 13134, loss: 0.0016515841707587242\n",
      "iteration 13135, loss: 0.0018285649130120873\n",
      "iteration 13136, loss: 0.00149880803655833\n",
      "iteration 13137, loss: 0.001667872886173427\n",
      "iteration 13138, loss: 0.002393969800323248\n",
      "iteration 13139, loss: 0.0020052820909768343\n",
      "iteration 13140, loss: 0.001628762693144381\n",
      "iteration 13141, loss: 0.0018703964306041598\n",
      "iteration 13142, loss: 0.002385494066402316\n",
      "iteration 13143, loss: 0.0021560778841376305\n",
      "iteration 13144, loss: 0.0018978536827489734\n",
      "iteration 13145, loss: 0.001599707524292171\n",
      "iteration 13146, loss: 0.002108498476445675\n",
      "iteration 13147, loss: 0.002263545524328947\n",
      "iteration 13148, loss: 0.0018253218149766326\n",
      "iteration 13149, loss: 0.001955189276486635\n",
      "iteration 13150, loss: 0.0017681655008345842\n",
      "iteration 13151, loss: 0.001862489734776318\n",
      "iteration 13152, loss: 0.001848995336331427\n",
      "iteration 13153, loss: 0.0018803866114467382\n",
      "iteration 13154, loss: 0.0014774356968700886\n",
      "iteration 13155, loss: 0.002497033216059208\n",
      "iteration 13156, loss: 0.0016687802271917462\n",
      "iteration 13157, loss: 0.0017015027115121484\n",
      "iteration 13158, loss: 0.0015854239463806152\n",
      "iteration 13159, loss: 0.0019382575992494822\n",
      "iteration 13160, loss: 0.002051296643912792\n",
      "iteration 13161, loss: 0.0018743660766631365\n",
      "iteration 13162, loss: 0.0019569944124668837\n",
      "iteration 13163, loss: 0.0021415019873529673\n",
      "iteration 13164, loss: 0.0024906820617616177\n",
      "iteration 13165, loss: 0.001909560291096568\n",
      "iteration 13166, loss: 0.0019513517618179321\n",
      "iteration 13167, loss: 0.002230981830507517\n",
      "iteration 13168, loss: 0.002396542113274336\n",
      "iteration 13169, loss: 0.0018992064287886024\n",
      "iteration 13170, loss: 0.001904000760987401\n",
      "iteration 13171, loss: 0.0020417021587491035\n",
      "iteration 13172, loss: 0.001962364185601473\n",
      "iteration 13173, loss: 0.0019427355146035552\n",
      "iteration 13174, loss: 0.0020086700096726418\n",
      "iteration 13175, loss: 0.0019967893604189157\n",
      "iteration 13176, loss: 0.00206531654112041\n",
      "iteration 13177, loss: 0.0025033047422766685\n",
      "iteration 13178, loss: 0.0019926605746150017\n",
      "iteration 13179, loss: 0.0018943182658404112\n",
      "iteration 13180, loss: 0.0019867231603711843\n",
      "iteration 13181, loss: 0.001968602417036891\n",
      "iteration 13182, loss: 0.0019783577881753445\n",
      "iteration 13183, loss: 0.001935614156536758\n",
      "iteration 13184, loss: 0.0021038895938545465\n",
      "iteration 13185, loss: 0.0020655361004173756\n",
      "iteration 13186, loss: 0.001846851548179984\n",
      "iteration 13187, loss: 0.0017528318567201495\n",
      "iteration 13188, loss: 0.0021134677808731794\n",
      "iteration 13189, loss: 0.0018213563598692417\n",
      "iteration 13190, loss: 0.0019225847208872437\n",
      "iteration 13191, loss: 0.0022121556103229523\n",
      "iteration 13192, loss: 0.0016328359488397837\n",
      "iteration 13193, loss: 0.0019917800091207027\n",
      "iteration 13194, loss: 0.0018425032030791044\n",
      "iteration 13195, loss: 0.0016235633520409465\n",
      "iteration 13196, loss: 0.0017900393577292562\n",
      "iteration 13197, loss: 0.001959405606612563\n",
      "iteration 13198, loss: 0.0018781396793201566\n",
      "iteration 13199, loss: 0.0019454674329608679\n",
      "iteration 13200, loss: 0.0015483733732253313\n",
      "iteration 13201, loss: 0.002280972898006439\n",
      "iteration 13202, loss: 0.0016562819946557283\n",
      "iteration 13203, loss: 0.002321974141523242\n",
      "iteration 13204, loss: 0.0023452267050743103\n",
      "iteration 13205, loss: 0.002049022354185581\n",
      "iteration 13206, loss: 0.00215795636177063\n",
      "iteration 13207, loss: 0.002131665823981166\n",
      "iteration 13208, loss: 0.001821786048822105\n",
      "iteration 13209, loss: 0.0019637513905763626\n",
      "iteration 13210, loss: 0.0019281601998955011\n",
      "iteration 13211, loss: 0.0019687379244714975\n",
      "iteration 13212, loss: 0.0022550795692950487\n",
      "iteration 13213, loss: 0.001767782960087061\n",
      "iteration 13214, loss: 0.00219875224865973\n",
      "iteration 13215, loss: 0.002057670848444104\n",
      "iteration 13216, loss: 0.0020996741950511932\n",
      "iteration 13217, loss: 0.002008630894124508\n",
      "iteration 13218, loss: 0.0021370919421315193\n",
      "iteration 13219, loss: 0.0016145531553775072\n",
      "iteration 13220, loss: 0.0024689931888133287\n",
      "iteration 13221, loss: 0.001714476733468473\n",
      "iteration 13222, loss: 0.0016166650457307696\n",
      "iteration 13223, loss: 0.0017784970114007592\n",
      "iteration 13224, loss: 0.001711334683932364\n",
      "iteration 13225, loss: 0.002147973282262683\n",
      "iteration 13226, loss: 0.0020336334127932787\n",
      "iteration 13227, loss: 0.00190607993863523\n",
      "iteration 13228, loss: 0.001973341917619109\n",
      "iteration 13229, loss: 0.0016888475511223078\n",
      "iteration 13230, loss: 0.0019845322240144014\n",
      "iteration 13231, loss: 0.0019038676982745528\n",
      "iteration 13232, loss: 0.002011841395869851\n",
      "iteration 13233, loss: 0.0017909398302435875\n",
      "iteration 13234, loss: 0.0018310346640646458\n",
      "iteration 13235, loss: 0.0019294964149594307\n",
      "iteration 13236, loss: 0.0018953682156279683\n",
      "iteration 13237, loss: 0.001962320413440466\n",
      "iteration 13238, loss: 0.0019745370373129845\n",
      "iteration 13239, loss: 0.0016391881508752704\n",
      "iteration 13240, loss: 0.0018094356637448072\n",
      "iteration 13241, loss: 0.0016082347137853503\n",
      "iteration 13242, loss: 0.002124051097780466\n",
      "iteration 13243, loss: 0.0022083837538957596\n",
      "iteration 13244, loss: 0.0018269445281475782\n",
      "iteration 13245, loss: 0.001966261537745595\n",
      "iteration 13246, loss: 0.0018782520201057196\n",
      "iteration 13247, loss: 0.001820376142859459\n",
      "iteration 13248, loss: 0.001720090163871646\n",
      "iteration 13249, loss: 0.0020254431292414665\n",
      "iteration 13250, loss: 0.0020669419318437576\n",
      "iteration 13251, loss: 0.0016834210837259889\n",
      "iteration 13252, loss: 0.0017215897096320987\n",
      "iteration 13253, loss: 0.001982039539143443\n",
      "iteration 13254, loss: 0.002057970967143774\n",
      "iteration 13255, loss: 0.0016298670088872313\n",
      "iteration 13256, loss: 0.0020796083845198154\n",
      "iteration 13257, loss: 0.0021041561849415302\n",
      "iteration 13258, loss: 0.0018219300545752048\n",
      "iteration 13259, loss: 0.002121083904057741\n",
      "iteration 13260, loss: 0.001762230647727847\n",
      "iteration 13261, loss: 0.0016481068450957537\n",
      "iteration 13262, loss: 0.0019541981164366007\n",
      "iteration 13263, loss: 0.0024406504817306995\n",
      "iteration 13264, loss: 0.002277500694617629\n",
      "iteration 13265, loss: 0.0021249642595648766\n",
      "iteration 13266, loss: 0.0020576510578393936\n",
      "iteration 13267, loss: 0.0017562962602823973\n",
      "iteration 13268, loss: 0.0014181790174916387\n",
      "iteration 13269, loss: 0.002157364971935749\n",
      "iteration 13270, loss: 0.0017350518610328436\n",
      "iteration 13271, loss: 0.0017665245104581118\n",
      "iteration 13272, loss: 0.0022346314508467913\n",
      "iteration 13273, loss: 0.0019584912806749344\n",
      "iteration 13274, loss: 0.0016988578718155622\n",
      "iteration 13275, loss: 0.002121325582265854\n",
      "iteration 13276, loss: 0.0017068940214812756\n",
      "iteration 13277, loss: 0.0019208078738301992\n",
      "iteration 13278, loss: 0.0016370072262361646\n",
      "iteration 13279, loss: 0.0015648542903363705\n",
      "iteration 13280, loss: 0.001580060226842761\n",
      "iteration 13281, loss: 0.0018600926268845797\n",
      "iteration 13282, loss: 0.0016916149761527777\n",
      "iteration 13283, loss: 0.0018793140770867467\n",
      "iteration 13284, loss: 0.0016937967156991363\n",
      "iteration 13285, loss: 0.0019512902945280075\n",
      "iteration 13286, loss: 0.001811228459700942\n",
      "iteration 13287, loss: 0.001928291516378522\n",
      "iteration 13288, loss: 0.001808852655813098\n",
      "iteration 13289, loss: 0.0015920274890959263\n",
      "iteration 13290, loss: 0.0016919388435781002\n",
      "iteration 13291, loss: 0.0015447658952325583\n",
      "iteration 13292, loss: 0.0023163885343819857\n",
      "iteration 13293, loss: 0.0016680362168699503\n",
      "iteration 13294, loss: 0.001798948273062706\n",
      "iteration 13295, loss: 0.0023642200976610184\n",
      "iteration 13296, loss: 0.002078478457406163\n",
      "iteration 13297, loss: 0.001653193961828947\n",
      "iteration 13298, loss: 0.0021860734559595585\n",
      "iteration 13299, loss: 0.0016604670090600848\n",
      "iteration 13300, loss: 0.0019257855601608753\n",
      "iteration 13301, loss: 0.001997667597606778\n",
      "iteration 13302, loss: 0.0017898848745971918\n",
      "iteration 13303, loss: 0.0019437111914157867\n",
      "iteration 13304, loss: 0.0021084381733089685\n",
      "iteration 13305, loss: 0.0018620106857270002\n",
      "iteration 13306, loss: 0.0019788118079304695\n",
      "iteration 13307, loss: 0.001940866932272911\n",
      "iteration 13308, loss: 0.0019759114366024733\n",
      "iteration 13309, loss: 0.0019384268671274185\n",
      "iteration 13310, loss: 0.0018484931206330657\n",
      "iteration 13311, loss: 0.001958307111635804\n",
      "iteration 13312, loss: 0.002020258456468582\n",
      "iteration 13313, loss: 0.0020801734644919634\n",
      "iteration 13314, loss: 0.001414366764947772\n",
      "iteration 13315, loss: 0.0016100378707051277\n",
      "iteration 13316, loss: 0.0018418706022202969\n",
      "iteration 13317, loss: 0.0021011107601225376\n",
      "iteration 13318, loss: 0.0021059245336800814\n",
      "iteration 13319, loss: 0.0020199730060994625\n",
      "iteration 13320, loss: 0.0018869363702833652\n",
      "iteration 13321, loss: 0.0014340020716190338\n",
      "iteration 13322, loss: 0.0020068115554749966\n",
      "iteration 13323, loss: 0.0019010670948773623\n",
      "iteration 13324, loss: 0.002119907643646002\n",
      "iteration 13325, loss: 0.0020269991364330053\n",
      "iteration 13326, loss: 0.0021313249599188566\n",
      "iteration 13327, loss: 0.0018066808115690947\n",
      "iteration 13328, loss: 0.0017783703515306115\n",
      "iteration 13329, loss: 0.0016717526596039534\n",
      "iteration 13330, loss: 0.0016321372240781784\n",
      "iteration 13331, loss: 0.002009715884923935\n",
      "iteration 13332, loss: 0.0016215724172070622\n",
      "iteration 13333, loss: 0.0018784892745316029\n",
      "iteration 13334, loss: 0.002094350755214691\n",
      "iteration 13335, loss: 0.002091721398755908\n",
      "iteration 13336, loss: 0.0018706397386267781\n",
      "iteration 13337, loss: 0.0018188742687925696\n",
      "iteration 13338, loss: 0.0017177374102175236\n",
      "iteration 13339, loss: 0.0019701304845511913\n",
      "iteration 13340, loss: 0.0017295533325523138\n",
      "iteration 13341, loss: 0.002609690884128213\n",
      "iteration 13342, loss: 0.0015291173476725817\n",
      "iteration 13343, loss: 0.0015209866687655449\n",
      "iteration 13344, loss: 0.002128077670931816\n",
      "iteration 13345, loss: 0.0016942326910793781\n",
      "iteration 13346, loss: 0.002027657814323902\n",
      "iteration 13347, loss: 0.0017241587629541755\n",
      "iteration 13348, loss: 0.0017965890001505613\n",
      "iteration 13349, loss: 0.0017790573183447123\n",
      "iteration 13350, loss: 0.0019306300673633814\n",
      "iteration 13351, loss: 0.0017397395567968488\n",
      "iteration 13352, loss: 0.0017726602964103222\n",
      "iteration 13353, loss: 0.0019357773708179593\n",
      "iteration 13354, loss: 0.0021022544242441654\n",
      "iteration 13355, loss: 0.00183915626257658\n",
      "iteration 13356, loss: 0.0017787766410037875\n",
      "iteration 13357, loss: 0.002057584235444665\n",
      "iteration 13358, loss: 0.001632397179491818\n",
      "iteration 13359, loss: 0.0019108320120722055\n",
      "iteration 13360, loss: 0.0017895469209179282\n",
      "iteration 13361, loss: 0.0016744495369493961\n",
      "iteration 13362, loss: 0.001982519868761301\n",
      "iteration 13363, loss: 0.0021170536056160927\n",
      "iteration 13364, loss: 0.002142487093806267\n",
      "iteration 13365, loss: 0.0018448247574269772\n",
      "iteration 13366, loss: 0.002279365435242653\n",
      "iteration 13367, loss: 0.0017616351833567023\n",
      "iteration 13368, loss: 0.0017533532809466124\n",
      "iteration 13369, loss: 0.00198747543618083\n",
      "iteration 13370, loss: 0.0021596569567918777\n",
      "iteration 13371, loss: 0.0020059505477547646\n",
      "iteration 13372, loss: 0.002133851870894432\n",
      "iteration 13373, loss: 0.0018438075203448534\n",
      "iteration 13374, loss: 0.002030035015195608\n",
      "iteration 13375, loss: 0.002005812479183078\n",
      "iteration 13376, loss: 0.0018873602384701371\n",
      "iteration 13377, loss: 0.0018280347576364875\n",
      "iteration 13378, loss: 0.0021851137280464172\n",
      "iteration 13379, loss: 0.002220661612227559\n",
      "iteration 13380, loss: 0.0015634892042726278\n",
      "iteration 13381, loss: 0.0020725878421217203\n",
      "iteration 13382, loss: 0.0019665018189698458\n",
      "iteration 13383, loss: 0.002025455702096224\n",
      "iteration 13384, loss: 0.0022503803484141827\n",
      "iteration 13385, loss: 0.001992355100810528\n",
      "iteration 13386, loss: 0.0017019456718116999\n",
      "iteration 13387, loss: 0.0017287172377109528\n",
      "iteration 13388, loss: 0.002124495804309845\n",
      "iteration 13389, loss: 0.001700107241049409\n",
      "iteration 13390, loss: 0.0021422449499368668\n",
      "iteration 13391, loss: 0.002174368593841791\n",
      "iteration 13392, loss: 0.0018693588208407164\n",
      "iteration 13393, loss: 0.0017557010287418962\n",
      "iteration 13394, loss: 0.0019268968608230352\n",
      "iteration 13395, loss: 0.0019718031398952007\n",
      "iteration 13396, loss: 0.001984706614166498\n",
      "iteration 13397, loss: 0.00188082130625844\n",
      "iteration 13398, loss: 0.0016966869588941336\n",
      "iteration 13399, loss: 0.0016985530965030193\n",
      "iteration 13400, loss: 0.0024132835678756237\n",
      "iteration 13401, loss: 0.001749824732542038\n",
      "iteration 13402, loss: 0.00208419025875628\n",
      "iteration 13403, loss: 0.0016492391005158424\n",
      "iteration 13404, loss: 0.0017778828041628003\n",
      "iteration 13405, loss: 0.0017176850233227015\n",
      "iteration 13406, loss: 0.001646903227083385\n",
      "iteration 13407, loss: 0.0015900495927780867\n",
      "iteration 13408, loss: 0.0017951455665752292\n",
      "iteration 13409, loss: 0.0018181070918217301\n",
      "iteration 13410, loss: 0.0016928563127294183\n",
      "iteration 13411, loss: 0.0017323587089776993\n",
      "iteration 13412, loss: 0.002449518535286188\n",
      "iteration 13413, loss: 0.0015051255468279123\n",
      "iteration 13414, loss: 0.001973263453692198\n",
      "iteration 13415, loss: 0.0020047142170369625\n",
      "iteration 13416, loss: 0.0016961293295025826\n",
      "iteration 13417, loss: 0.0018336568027734756\n",
      "iteration 13418, loss: 0.002064607571810484\n",
      "iteration 13419, loss: 0.0017109026666730642\n",
      "iteration 13420, loss: 0.0018139631720259786\n",
      "iteration 13421, loss: 0.002149277599528432\n",
      "iteration 13422, loss: 0.0017325548687949777\n",
      "iteration 13423, loss: 0.0018067352939397097\n",
      "iteration 13424, loss: 0.001548656728118658\n",
      "iteration 13425, loss: 0.001750471186824143\n",
      "iteration 13426, loss: 0.0018590998370200396\n",
      "iteration 13427, loss: 0.0020649689249694347\n",
      "iteration 13428, loss: 0.0017985935555770993\n",
      "iteration 13429, loss: 0.001669307705014944\n",
      "iteration 13430, loss: 0.002292250981554389\n",
      "iteration 13431, loss: 0.001674535102210939\n",
      "iteration 13432, loss: 0.0017659284640103579\n",
      "iteration 13433, loss: 0.0015990260289981961\n",
      "iteration 13434, loss: 0.0019253400387242436\n",
      "iteration 13435, loss: 0.0016860165633261204\n",
      "iteration 13436, loss: 0.001581097487360239\n",
      "iteration 13437, loss: 0.001966055016964674\n",
      "iteration 13438, loss: 0.001422297558747232\n",
      "iteration 13439, loss: 0.001771282171830535\n",
      "iteration 13440, loss: 0.0016221473924815655\n",
      "iteration 13441, loss: 0.001963139045983553\n",
      "iteration 13442, loss: 0.0016998001374304295\n",
      "iteration 13443, loss: 0.0017803636146709323\n",
      "iteration 13444, loss: 0.0018803076818585396\n",
      "iteration 13445, loss: 0.001776000251993537\n",
      "iteration 13446, loss: 0.0016339411959052086\n",
      "iteration 13447, loss: 0.0017071813344955444\n",
      "iteration 13448, loss: 0.001686115749180317\n",
      "iteration 13449, loss: 0.0020065647549927235\n",
      "iteration 13450, loss: 0.0017233126563951373\n",
      "iteration 13451, loss: 0.0023140881676226854\n",
      "iteration 13452, loss: 0.0015470547368749976\n",
      "iteration 13453, loss: 0.0014825136167928576\n",
      "iteration 13454, loss: 0.002003408968448639\n",
      "iteration 13455, loss: 0.0022598044015467167\n",
      "iteration 13456, loss: 0.0022101323120296\n",
      "iteration 13457, loss: 0.0022166965063661337\n",
      "iteration 13458, loss: 0.001849146094173193\n",
      "iteration 13459, loss: 0.0021416887175291777\n",
      "iteration 13460, loss: 0.002030505333095789\n",
      "iteration 13461, loss: 0.0019399486482143402\n",
      "iteration 13462, loss: 0.0021900313440710306\n",
      "iteration 13463, loss: 0.001961566973477602\n",
      "iteration 13464, loss: 0.0017773890867829323\n",
      "iteration 13465, loss: 0.0016814519185572863\n",
      "iteration 13466, loss: 0.0017250952078029513\n",
      "iteration 13467, loss: 0.0016942740185186267\n",
      "iteration 13468, loss: 0.002099016448482871\n",
      "iteration 13469, loss: 0.0018234953749924898\n",
      "iteration 13470, loss: 0.0016842588083818555\n",
      "iteration 13471, loss: 0.00236291391775012\n",
      "iteration 13472, loss: 0.0021836122032254934\n",
      "iteration 13473, loss: 0.0019007704686373472\n",
      "iteration 13474, loss: 0.0019301683641970158\n",
      "iteration 13475, loss: 0.0018005894962698221\n",
      "iteration 13476, loss: 0.0019199135713279247\n",
      "iteration 13477, loss: 0.002021392807364464\n",
      "iteration 13478, loss: 0.0019305564928799868\n",
      "iteration 13479, loss: 0.0014669328229501843\n",
      "iteration 13480, loss: 0.0016709696501493454\n",
      "iteration 13481, loss: 0.0019035448785871267\n",
      "iteration 13482, loss: 0.0019155589397996664\n",
      "iteration 13483, loss: 0.002113496419042349\n",
      "iteration 13484, loss: 0.0017467300640419126\n",
      "iteration 13485, loss: 0.00150456209667027\n",
      "iteration 13486, loss: 0.0018683754606172442\n",
      "iteration 13487, loss: 0.001991719240322709\n",
      "iteration 13488, loss: 0.0018176506273448467\n",
      "iteration 13489, loss: 0.0017138335388153791\n",
      "iteration 13490, loss: 0.0018030412029474974\n",
      "iteration 13491, loss: 0.0018942804308608174\n",
      "iteration 13492, loss: 0.0016682012937963009\n",
      "iteration 13493, loss: 0.0018703278619796038\n",
      "iteration 13494, loss: 0.0019275224767625332\n",
      "iteration 13495, loss: 0.0016945244278758764\n",
      "iteration 13496, loss: 0.0017048941226676106\n",
      "iteration 13497, loss: 0.002045920118689537\n",
      "iteration 13498, loss: 0.00150801963172853\n",
      "iteration 13499, loss: 0.001956324093043804\n",
      "iteration 13500, loss: 0.0017283086199313402\n",
      "iteration 13501, loss: 0.0016391623066738248\n",
      "iteration 13502, loss: 0.0013844247441738844\n",
      "iteration 13503, loss: 0.002267471980303526\n",
      "iteration 13504, loss: 0.0018685837276279926\n",
      "iteration 13505, loss: 0.0017762953648343682\n",
      "iteration 13506, loss: 0.0018122625770047307\n",
      "iteration 13507, loss: 0.0018665781244635582\n",
      "iteration 13508, loss: 0.0017469336744397879\n",
      "iteration 13509, loss: 0.0023469170555472374\n",
      "iteration 13510, loss: 0.0020218032877892256\n",
      "iteration 13511, loss: 0.0020490246824920177\n",
      "iteration 13512, loss: 0.0018930670339614153\n",
      "iteration 13513, loss: 0.0016684754518792033\n",
      "iteration 13514, loss: 0.0016630914760753512\n",
      "iteration 13515, loss: 0.0019490290433168411\n",
      "iteration 13516, loss: 0.0020802693907171488\n",
      "iteration 13517, loss: 0.0019056417513638735\n",
      "iteration 13518, loss: 0.0019267067546024919\n",
      "iteration 13519, loss: 0.001795644173398614\n",
      "iteration 13520, loss: 0.0021418770775198936\n",
      "iteration 13521, loss: 0.001927209086716175\n",
      "iteration 13522, loss: 0.0022072051651775837\n",
      "iteration 13523, loss: 0.0017629824578762054\n",
      "iteration 13524, loss: 0.0017387964762747288\n",
      "iteration 13525, loss: 0.001916574314236641\n",
      "iteration 13526, loss: 0.0017303756903856993\n",
      "iteration 13527, loss: 0.0021613959688693285\n",
      "iteration 13528, loss: 0.0017659143777564168\n",
      "iteration 13529, loss: 0.001850542495958507\n",
      "iteration 13530, loss: 0.0015068325446918607\n",
      "iteration 13531, loss: 0.0017563404981046915\n",
      "iteration 13532, loss: 0.001962818903848529\n",
      "iteration 13533, loss: 0.0019978703930974007\n",
      "iteration 13534, loss: 0.0015093263937160373\n",
      "iteration 13535, loss: 0.0019148923456668854\n",
      "iteration 13536, loss: 0.001727875554934144\n",
      "iteration 13537, loss: 0.0020353649742901325\n",
      "iteration 13538, loss: 0.0017619498539716005\n",
      "iteration 13539, loss: 0.0013892903225496411\n",
      "iteration 13540, loss: 0.0018957850988954306\n",
      "iteration 13541, loss: 0.0015348801389336586\n",
      "iteration 13542, loss: 0.0019340133294463158\n",
      "iteration 13543, loss: 0.002316724509000778\n",
      "iteration 13544, loss: 0.0017126600723713636\n",
      "iteration 13545, loss: 0.0015564083587378263\n",
      "iteration 13546, loss: 0.001830666558817029\n",
      "iteration 13547, loss: 0.001978666055947542\n",
      "iteration 13548, loss: 0.0015828756149858236\n",
      "iteration 13549, loss: 0.0015849157935008407\n",
      "iteration 13550, loss: 0.0017356034368276596\n",
      "iteration 13551, loss: 0.001939629204571247\n",
      "iteration 13552, loss: 0.001649355050176382\n",
      "iteration 13553, loss: 0.0018570837564766407\n",
      "iteration 13554, loss: 0.0015143088530749083\n",
      "iteration 13555, loss: 0.0017524105496704578\n",
      "iteration 13556, loss: 0.0016187590081244707\n",
      "iteration 13557, loss: 0.0020073193591088057\n",
      "iteration 13558, loss: 0.002026598434895277\n",
      "iteration 13559, loss: 0.00175390113145113\n",
      "iteration 13560, loss: 0.0015945413615554571\n",
      "iteration 13561, loss: 0.0016480323392897844\n",
      "iteration 13562, loss: 0.0017931658076122403\n",
      "iteration 13563, loss: 0.0017131960485130548\n",
      "iteration 13564, loss: 0.0016444390639662743\n",
      "iteration 13565, loss: 0.0016662394627928734\n",
      "iteration 13566, loss: 0.002105116844177246\n",
      "iteration 13567, loss: 0.0016997144557535648\n",
      "iteration 13568, loss: 0.0013480676570907235\n",
      "iteration 13569, loss: 0.00183026900049299\n",
      "iteration 13570, loss: 0.001874819048680365\n",
      "iteration 13571, loss: 0.0019344663014635444\n",
      "iteration 13572, loss: 0.0018597645685076714\n",
      "iteration 13573, loss: 0.001895386609248817\n",
      "iteration 13574, loss: 0.0015771916368976235\n",
      "iteration 13575, loss: 0.0014613207895308733\n",
      "iteration 13576, loss: 0.001494717551395297\n",
      "iteration 13577, loss: 0.0019772422965615988\n",
      "iteration 13578, loss: 0.0015992760891094804\n",
      "iteration 13579, loss: 0.0019281691638752818\n",
      "iteration 13580, loss: 0.001797481207177043\n",
      "iteration 13581, loss: 0.0017953705973923206\n",
      "iteration 13582, loss: 0.0017606676556169987\n",
      "iteration 13583, loss: 0.001809468725696206\n",
      "iteration 13584, loss: 0.0018932907842099667\n",
      "iteration 13585, loss: 0.002005946123972535\n",
      "iteration 13586, loss: 0.0023532258346676826\n",
      "iteration 13587, loss: 0.0018294613109901547\n",
      "iteration 13588, loss: 0.002131173387169838\n",
      "iteration 13589, loss: 0.002371847163885832\n",
      "iteration 13590, loss: 0.0016290420899167657\n",
      "iteration 13591, loss: 0.0017565537709742785\n",
      "iteration 13592, loss: 0.002022694330662489\n",
      "iteration 13593, loss: 0.0018584297504276037\n",
      "iteration 13594, loss: 0.0019064628286287189\n",
      "iteration 13595, loss: 0.0016681163106113672\n",
      "iteration 13596, loss: 0.0024715308099985123\n",
      "iteration 13597, loss: 0.001887426245957613\n",
      "iteration 13598, loss: 0.0019386701751500368\n",
      "iteration 13599, loss: 0.002097218995913863\n",
      "iteration 13600, loss: 0.001742967520840466\n",
      "iteration 13601, loss: 0.0019931658171117306\n",
      "iteration 13602, loss: 0.0013910465640947223\n",
      "iteration 13603, loss: 0.0017213881947100163\n",
      "iteration 13604, loss: 0.002321097068488598\n",
      "iteration 13605, loss: 0.0019861492328345776\n",
      "iteration 13606, loss: 0.001968125347048044\n",
      "iteration 13607, loss: 0.0016277122776955366\n",
      "iteration 13608, loss: 0.0016060578636825085\n",
      "iteration 13609, loss: 0.0016260429983958602\n",
      "iteration 13610, loss: 0.002380941528826952\n",
      "iteration 13611, loss: 0.00172383151948452\n",
      "iteration 13612, loss: 0.0017763269133865833\n",
      "iteration 13613, loss: 0.0019691509660333395\n",
      "iteration 13614, loss: 0.001569814165122807\n",
      "iteration 13615, loss: 0.0019082612125203013\n",
      "iteration 13616, loss: 0.0018033597152680159\n",
      "iteration 13617, loss: 0.0017078763339668512\n",
      "iteration 13618, loss: 0.0017194023821502924\n",
      "iteration 13619, loss: 0.0017892178148031235\n",
      "iteration 13620, loss: 0.001930328900925815\n",
      "iteration 13621, loss: 0.0019600566010922194\n",
      "iteration 13622, loss: 0.0017011838499456644\n",
      "iteration 13623, loss: 0.0018537105061113834\n",
      "iteration 13624, loss: 0.0017554683145135641\n",
      "iteration 13625, loss: 0.001856903312727809\n",
      "iteration 13626, loss: 0.0015626682434231043\n",
      "iteration 13627, loss: 0.0022597708739340305\n",
      "iteration 13628, loss: 0.0015580346807837486\n",
      "iteration 13629, loss: 0.0016970242140814662\n",
      "iteration 13630, loss: 0.0016732326475903392\n",
      "iteration 13631, loss: 0.001520126243121922\n",
      "iteration 13632, loss: 0.0019304368179291487\n",
      "iteration 13633, loss: 0.0015268888091668487\n",
      "iteration 13634, loss: 0.0018804417923092842\n",
      "iteration 13635, loss: 0.001654257532209158\n",
      "iteration 13636, loss: 0.0018734430195763707\n",
      "iteration 13637, loss: 0.001660460839048028\n",
      "iteration 13638, loss: 0.001709815813228488\n",
      "iteration 13639, loss: 0.0014093989739194512\n",
      "iteration 13640, loss: 0.0016889513935893774\n",
      "iteration 13641, loss: 0.001987371826544404\n",
      "iteration 13642, loss: 0.0018064684700220823\n",
      "iteration 13643, loss: 0.0018107086652889848\n",
      "iteration 13644, loss: 0.001914782915264368\n",
      "iteration 13645, loss: 0.001603571930900216\n",
      "iteration 13646, loss: 0.001804880565032363\n",
      "iteration 13647, loss: 0.0017579824198037386\n",
      "iteration 13648, loss: 0.001757611520588398\n",
      "iteration 13649, loss: 0.0017455979250371456\n",
      "iteration 13650, loss: 0.0016138009959831834\n",
      "iteration 13651, loss: 0.002023886190727353\n",
      "iteration 13652, loss: 0.0017418927745893598\n",
      "iteration 13653, loss: 0.0016932236030697823\n",
      "iteration 13654, loss: 0.001726000220514834\n",
      "iteration 13655, loss: 0.0017550528282299638\n",
      "iteration 13656, loss: 0.002004660200327635\n",
      "iteration 13657, loss: 0.001775321550667286\n",
      "iteration 13658, loss: 0.001801076577976346\n",
      "iteration 13659, loss: 0.0017922873375937343\n",
      "iteration 13660, loss: 0.002110592555254698\n",
      "iteration 13661, loss: 0.001979745225980878\n",
      "iteration 13662, loss: 0.0018158971797674894\n",
      "iteration 13663, loss: 0.0014700617175549269\n",
      "iteration 13664, loss: 0.0017750011757016182\n",
      "iteration 13665, loss: 0.0016494826413691044\n",
      "iteration 13666, loss: 0.0017943985294550657\n",
      "iteration 13667, loss: 0.0015994127606973052\n",
      "iteration 13668, loss: 0.0019213752821087837\n",
      "iteration 13669, loss: 0.001816950156353414\n",
      "iteration 13670, loss: 0.0016385396011173725\n",
      "iteration 13671, loss: 0.001557655050419271\n",
      "iteration 13672, loss: 0.0014721662737429142\n",
      "iteration 13673, loss: 0.0015426232712343335\n",
      "iteration 13674, loss: 0.0016452890122309327\n",
      "iteration 13675, loss: 0.0019390351371839643\n",
      "iteration 13676, loss: 0.0018157637678086758\n",
      "iteration 13677, loss: 0.001579559175297618\n",
      "iteration 13678, loss: 0.001733412267640233\n",
      "iteration 13679, loss: 0.001850989880040288\n",
      "iteration 13680, loss: 0.0018712776945903897\n",
      "iteration 13681, loss: 0.0017468106234446168\n",
      "iteration 13682, loss: 0.0015300882514566183\n",
      "iteration 13683, loss: 0.0020498998928815126\n",
      "iteration 13684, loss: 0.0014780655037611723\n",
      "iteration 13685, loss: 0.0016386122442781925\n",
      "iteration 13686, loss: 0.0017716424772515893\n",
      "iteration 13687, loss: 0.0016524698585271835\n",
      "iteration 13688, loss: 0.0018848603358492255\n",
      "iteration 13689, loss: 0.0017915251664817333\n",
      "iteration 13690, loss: 0.0015876153483986855\n",
      "iteration 13691, loss: 0.0020457622595131397\n",
      "iteration 13692, loss: 0.0016793888062238693\n",
      "iteration 13693, loss: 0.0017580523854121566\n",
      "iteration 13694, loss: 0.0023030561860650778\n",
      "iteration 13695, loss: 0.0016203420236706734\n",
      "iteration 13696, loss: 0.0019444094505161047\n",
      "iteration 13697, loss: 0.0017251232638955116\n",
      "iteration 13698, loss: 0.0021519605070352554\n",
      "iteration 13699, loss: 0.0015226418618112803\n",
      "iteration 13700, loss: 0.0015025022439658642\n",
      "iteration 13701, loss: 0.0020031947642564774\n",
      "iteration 13702, loss: 0.0018355840584263206\n",
      "iteration 13703, loss: 0.0015329673187807202\n",
      "iteration 13704, loss: 0.0017544238362461329\n",
      "iteration 13705, loss: 0.0018642059294506907\n",
      "iteration 13706, loss: 0.001958332024514675\n",
      "iteration 13707, loss: 0.0019350475631654263\n",
      "iteration 13708, loss: 0.0021201339550316334\n",
      "iteration 13709, loss: 0.0016852987464517355\n",
      "iteration 13710, loss: 0.001940190326422453\n",
      "iteration 13711, loss: 0.001392147969454527\n",
      "iteration 13712, loss: 0.0019792169332504272\n",
      "iteration 13713, loss: 0.0018689368152990937\n",
      "iteration 13714, loss: 0.0018482964951545\n",
      "iteration 13715, loss: 0.0017842757515609264\n",
      "iteration 13716, loss: 0.0016888934187591076\n",
      "iteration 13717, loss: 0.001924762036651373\n",
      "iteration 13718, loss: 0.001712289871647954\n",
      "iteration 13719, loss: 0.0019608675502240658\n",
      "iteration 13720, loss: 0.0015870179049670696\n",
      "iteration 13721, loss: 0.0018224914092570543\n",
      "iteration 13722, loss: 0.0013999366201460361\n",
      "iteration 13723, loss: 0.001721840468235314\n",
      "iteration 13724, loss: 0.0015133252600207925\n",
      "iteration 13725, loss: 0.002044236520305276\n",
      "iteration 13726, loss: 0.0014220402808859944\n",
      "iteration 13727, loss: 0.0017675240524113178\n",
      "iteration 13728, loss: 0.0016499289777129889\n",
      "iteration 13729, loss: 0.0019740737043321133\n",
      "iteration 13730, loss: 0.0019866637885570526\n",
      "iteration 13731, loss: 0.002022014930844307\n",
      "iteration 13732, loss: 0.0016038118628785014\n",
      "iteration 13733, loss: 0.0015441696159541607\n",
      "iteration 13734, loss: 0.001626664074137807\n",
      "iteration 13735, loss: 0.0017630812944844365\n",
      "iteration 13736, loss: 0.0021771378815174103\n",
      "iteration 13737, loss: 0.0018326573772355914\n",
      "iteration 13738, loss: 0.0019446627702564\n",
      "iteration 13739, loss: 0.0017743951175361872\n",
      "iteration 13740, loss: 0.002263737376779318\n",
      "iteration 13741, loss: 0.0017350140260532498\n",
      "iteration 13742, loss: 0.001729303621686995\n",
      "iteration 13743, loss: 0.0018240802455693483\n",
      "iteration 13744, loss: 0.0017402908997610211\n",
      "iteration 13745, loss: 0.0018912950763478875\n",
      "iteration 13746, loss: 0.0017557155806571245\n",
      "iteration 13747, loss: 0.0019746418111026287\n",
      "iteration 13748, loss: 0.0015768252778798342\n",
      "iteration 13749, loss: 0.001602277858182788\n",
      "iteration 13750, loss: 0.0017121349228546023\n",
      "iteration 13751, loss: 0.001715517253614962\n",
      "iteration 13752, loss: 0.0016215466894209385\n",
      "iteration 13753, loss: 0.0013480628840625286\n",
      "iteration 13754, loss: 0.0015091755194589496\n",
      "iteration 13755, loss: 0.00188948935829103\n",
      "iteration 13756, loss: 0.0016656049992889166\n",
      "iteration 13757, loss: 0.0020129980985075235\n",
      "iteration 13758, loss: 0.0017207604832947254\n",
      "iteration 13759, loss: 0.0019100452773272991\n",
      "iteration 13760, loss: 0.0017901880200952291\n",
      "iteration 13761, loss: 0.0016630063764750957\n",
      "iteration 13762, loss: 0.0016073393635451794\n",
      "iteration 13763, loss: 0.001891910214908421\n",
      "iteration 13764, loss: 0.0016369451768696308\n",
      "iteration 13765, loss: 0.001981636742129922\n",
      "iteration 13766, loss: 0.0016419060993939638\n",
      "iteration 13767, loss: 0.0016909955302253366\n",
      "iteration 13768, loss: 0.002052098512649536\n",
      "iteration 13769, loss: 0.001982048386707902\n",
      "iteration 13770, loss: 0.001735256751999259\n",
      "iteration 13771, loss: 0.0017985622398555279\n",
      "iteration 13772, loss: 0.001543595688417554\n",
      "iteration 13773, loss: 0.0015119460877031088\n",
      "iteration 13774, loss: 0.0017186969053000212\n",
      "iteration 13775, loss: 0.0018362679984420538\n",
      "iteration 13776, loss: 0.0018440850544720888\n",
      "iteration 13777, loss: 0.0014798599295318127\n",
      "iteration 13778, loss: 0.0016487158136442304\n",
      "iteration 13779, loss: 0.0017322392668575048\n",
      "iteration 13780, loss: 0.0016518012853339314\n",
      "iteration 13781, loss: 0.0018041571602225304\n",
      "iteration 13782, loss: 0.0015208544209599495\n",
      "iteration 13783, loss: 0.0017969742184504867\n",
      "iteration 13784, loss: 0.0017291195690631866\n",
      "iteration 13785, loss: 0.0015297456411644816\n",
      "iteration 13786, loss: 0.0016658114036545157\n",
      "iteration 13787, loss: 0.001592990243807435\n",
      "iteration 13788, loss: 0.001856001908890903\n",
      "iteration 13789, loss: 0.0018917866982519627\n",
      "iteration 13790, loss: 0.0018241360085085034\n",
      "iteration 13791, loss: 0.001795897725969553\n",
      "iteration 13792, loss: 0.0017562611028552055\n",
      "iteration 13793, loss: 0.0014243004843592644\n",
      "iteration 13794, loss: 0.001913438318297267\n",
      "iteration 13795, loss: 0.001961770001798868\n",
      "iteration 13796, loss: 0.001946796546690166\n",
      "iteration 13797, loss: 0.0016066639218479395\n",
      "iteration 13798, loss: 0.0016700699925422668\n",
      "iteration 13799, loss: 0.0018339810194447637\n",
      "iteration 13800, loss: 0.001538286218419671\n",
      "iteration 13801, loss: 0.0020113540813326836\n",
      "iteration 13802, loss: 0.002094350289553404\n",
      "iteration 13803, loss: 0.002252449281513691\n",
      "iteration 13804, loss: 0.0017230687662959099\n",
      "iteration 13805, loss: 0.0016449075192213058\n",
      "iteration 13806, loss: 0.0017385941464453936\n",
      "iteration 13807, loss: 0.0018057699780911207\n",
      "iteration 13808, loss: 0.0016077213222160935\n",
      "iteration 13809, loss: 0.0015866928733885288\n",
      "iteration 13810, loss: 0.001708113239146769\n",
      "iteration 13811, loss: 0.0013808892108500004\n",
      "iteration 13812, loss: 0.0019386470085009933\n",
      "iteration 13813, loss: 0.0017591407522559166\n",
      "iteration 13814, loss: 0.0016630864702165127\n",
      "iteration 13815, loss: 0.001982540125027299\n",
      "iteration 13816, loss: 0.001520802266895771\n",
      "iteration 13817, loss: 0.0014994925586506724\n",
      "iteration 13818, loss: 0.0018357585649937391\n",
      "iteration 13819, loss: 0.0015068176435306668\n",
      "iteration 13820, loss: 0.0018847754690796137\n",
      "iteration 13821, loss: 0.0019428504165261984\n",
      "iteration 13822, loss: 0.0014683373738080263\n",
      "iteration 13823, loss: 0.0017292911652475595\n",
      "iteration 13824, loss: 0.0018268993590027094\n",
      "iteration 13825, loss: 0.0015031963121145964\n",
      "iteration 13826, loss: 0.0014833483146503568\n",
      "iteration 13827, loss: 0.001544724334962666\n",
      "iteration 13828, loss: 0.001907555852085352\n",
      "iteration 13829, loss: 0.0018592907581478357\n",
      "iteration 13830, loss: 0.0014901565155014396\n",
      "iteration 13831, loss: 0.0013697140384465456\n",
      "iteration 13832, loss: 0.001526850275695324\n",
      "iteration 13833, loss: 0.001667213044129312\n",
      "iteration 13834, loss: 0.0014359172200784087\n",
      "iteration 13835, loss: 0.0017453111940994859\n",
      "iteration 13836, loss: 0.0012801069533452392\n",
      "iteration 13837, loss: 0.001465231878682971\n",
      "iteration 13838, loss: 0.001543146325275302\n",
      "iteration 13839, loss: 0.0013900259509682655\n",
      "iteration 13840, loss: 0.0016867679078131914\n",
      "iteration 13841, loss: 0.001529243541881442\n",
      "iteration 13842, loss: 0.0015310579910874367\n",
      "iteration 13843, loss: 0.0014133506920188665\n",
      "iteration 13844, loss: 0.0015171844279393554\n",
      "iteration 13845, loss: 0.0016659209504723549\n",
      "iteration 13846, loss: 0.0018647832330316305\n",
      "iteration 13847, loss: 0.0016704648733139038\n",
      "iteration 13848, loss: 0.0017336646560579538\n",
      "iteration 13849, loss: 0.0018527110805734992\n",
      "iteration 13850, loss: 0.00210757739841938\n",
      "iteration 13851, loss: 0.001546407351270318\n",
      "iteration 13852, loss: 0.0019300340209156275\n",
      "iteration 13853, loss: 0.0017733685672283173\n",
      "iteration 13854, loss: 0.0014297472080215812\n",
      "iteration 13855, loss: 0.0014777278993278742\n",
      "iteration 13856, loss: 0.0015388077590614557\n",
      "iteration 13857, loss: 0.0016892347484827042\n",
      "iteration 13858, loss: 0.001573051093146205\n",
      "iteration 13859, loss: 0.0016080776695162058\n",
      "iteration 13860, loss: 0.0017295295838266611\n",
      "iteration 13861, loss: 0.0015406858874484897\n",
      "iteration 13862, loss: 0.0014207453932613134\n",
      "iteration 13863, loss: 0.0017062097322195768\n",
      "iteration 13864, loss: 0.0016792661044746637\n",
      "iteration 13865, loss: 0.0015788323944434524\n",
      "iteration 13866, loss: 0.0016694855876266956\n",
      "iteration 13867, loss: 0.0016659852117300034\n",
      "iteration 13868, loss: 0.0018557922448962927\n",
      "iteration 13869, loss: 0.0019863047637045383\n",
      "iteration 13870, loss: 0.0016135253245010972\n",
      "iteration 13871, loss: 0.0016077894251793623\n",
      "iteration 13872, loss: 0.0015778085216879845\n",
      "iteration 13873, loss: 0.0017244212795048952\n",
      "iteration 13874, loss: 0.001856903312727809\n",
      "iteration 13875, loss: 0.0015246368711814284\n",
      "iteration 13876, loss: 0.0014791951980441809\n",
      "iteration 13877, loss: 0.0015543272020295262\n",
      "iteration 13878, loss: 0.0013436977751553059\n",
      "iteration 13879, loss: 0.0016369346994906664\n",
      "iteration 13880, loss: 0.0018295234767720103\n",
      "iteration 13881, loss: 0.0019094444578513503\n",
      "iteration 13882, loss: 0.0017808426637202501\n",
      "iteration 13883, loss: 0.002053865697234869\n",
      "iteration 13884, loss: 0.002000358421355486\n",
      "iteration 13885, loss: 0.0019101407378911972\n",
      "iteration 13886, loss: 0.0018522192258387804\n",
      "iteration 13887, loss: 0.0016637725057080388\n",
      "iteration 13888, loss: 0.0017847322160378098\n",
      "iteration 13889, loss: 0.0015590130351483822\n",
      "iteration 13890, loss: 0.0018052918603643775\n",
      "iteration 13891, loss: 0.0014743086649104953\n",
      "iteration 13892, loss: 0.0018601252231746912\n",
      "iteration 13893, loss: 0.0015172362327575684\n",
      "iteration 13894, loss: 0.0017463881522417068\n",
      "iteration 13895, loss: 0.0016436483711004257\n",
      "iteration 13896, loss: 0.0016144062392413616\n",
      "iteration 13897, loss: 0.0014537000097334385\n",
      "iteration 13898, loss: 0.001377940527163446\n",
      "iteration 13899, loss: 0.002057828474789858\n",
      "iteration 13900, loss: 0.0015144427306950092\n",
      "iteration 13901, loss: 0.0021646199747920036\n",
      "iteration 13902, loss: 0.0013981382362544537\n",
      "iteration 13903, loss: 0.0015191341517493129\n",
      "iteration 13904, loss: 0.0015967159997671843\n",
      "iteration 13905, loss: 0.0018687916453927755\n",
      "iteration 13906, loss: 0.0019941104110330343\n",
      "iteration 13907, loss: 0.002096916316077113\n",
      "iteration 13908, loss: 0.0014767954126000404\n",
      "iteration 13909, loss: 0.0013242312707006931\n",
      "iteration 13910, loss: 0.001471279188990593\n",
      "iteration 13911, loss: 0.0013892811257392168\n",
      "iteration 13912, loss: 0.001621572533622384\n",
      "iteration 13913, loss: 0.001939099282026291\n",
      "iteration 13914, loss: 0.0018637425964698195\n",
      "iteration 13915, loss: 0.0017705188365653157\n",
      "iteration 13916, loss: 0.0016385169001296163\n",
      "iteration 13917, loss: 0.0013487108517438173\n",
      "iteration 13918, loss: 0.0015428930055350065\n",
      "iteration 13919, loss: 0.001732140895910561\n",
      "iteration 13920, loss: 0.0016195254866033792\n",
      "iteration 13921, loss: 0.001812587957829237\n",
      "iteration 13922, loss: 0.0016720021376386285\n",
      "iteration 13923, loss: 0.0018370812758803368\n",
      "iteration 13924, loss: 0.00174583843909204\n",
      "iteration 13925, loss: 0.0016404088819399476\n",
      "iteration 13926, loss: 0.0015127682127058506\n",
      "iteration 13927, loss: 0.0016569104045629501\n",
      "iteration 13928, loss: 0.0019995139446109533\n",
      "iteration 13929, loss: 0.0016346287447959185\n",
      "iteration 13930, loss: 0.0017095811199396849\n",
      "iteration 13931, loss: 0.0019784821197390556\n",
      "iteration 13932, loss: 0.0015432265354320407\n",
      "iteration 13933, loss: 0.0014866981655359268\n",
      "iteration 13934, loss: 0.0016511299181729555\n",
      "iteration 13935, loss: 0.001795899122953415\n",
      "iteration 13936, loss: 0.0017006947891786695\n",
      "iteration 13937, loss: 0.0015716655179858208\n",
      "iteration 13938, loss: 0.001717178849503398\n",
      "iteration 13939, loss: 0.0016459778416901827\n",
      "iteration 13940, loss: 0.0015804038848727942\n",
      "iteration 13941, loss: 0.001577278831973672\n",
      "iteration 13942, loss: 0.0018899263814091682\n",
      "iteration 13943, loss: 0.0014378734631463885\n",
      "iteration 13944, loss: 0.0018829376203939319\n",
      "iteration 13945, loss: 0.0018364863935858011\n",
      "iteration 13946, loss: 0.0014296218287199736\n",
      "iteration 13947, loss: 0.0013100446667522192\n",
      "iteration 13948, loss: 0.0017076475778594613\n",
      "iteration 13949, loss: 0.0016232054913416505\n",
      "iteration 13950, loss: 0.001605538884177804\n",
      "iteration 13951, loss: 0.0016466162633150816\n",
      "iteration 13952, loss: 0.0017043990083038807\n",
      "iteration 13953, loss: 0.0016123548848554492\n",
      "iteration 13954, loss: 0.0017373384907841682\n",
      "iteration 13955, loss: 0.0017895057098940015\n",
      "iteration 13956, loss: 0.002101079560816288\n",
      "iteration 13957, loss: 0.0020040429662913084\n",
      "iteration 13958, loss: 0.0015815888764336705\n",
      "iteration 13959, loss: 0.0017434876644983888\n",
      "iteration 13960, loss: 0.0016528996638953686\n",
      "iteration 13961, loss: 0.0017903696279972792\n",
      "iteration 13962, loss: 0.0014174825046211481\n",
      "iteration 13963, loss: 0.0014707555528730154\n",
      "iteration 13964, loss: 0.0016645141877233982\n",
      "iteration 13965, loss: 0.0017675540875643492\n",
      "iteration 13966, loss: 0.0016390597447752953\n",
      "iteration 13967, loss: 0.0016285376623272896\n",
      "iteration 13968, loss: 0.0016376322600990534\n",
      "iteration 13969, loss: 0.0016197185032069683\n",
      "iteration 13970, loss: 0.0014339222107082605\n",
      "iteration 13971, loss: 0.0017835594480857253\n",
      "iteration 13972, loss: 0.0014306397642940283\n",
      "iteration 13973, loss: 0.0019036096055060625\n",
      "iteration 13974, loss: 0.0018539425218477845\n",
      "iteration 13975, loss: 0.0017814994789659977\n",
      "iteration 13976, loss: 0.0016911551356315613\n",
      "iteration 13977, loss: 0.001554371090605855\n",
      "iteration 13978, loss: 0.0018473750678822398\n",
      "iteration 13979, loss: 0.0016016477020457387\n",
      "iteration 13980, loss: 0.0017871774034574628\n",
      "iteration 13981, loss: 0.001689703669399023\n",
      "iteration 13982, loss: 0.0016086521791294217\n",
      "iteration 13983, loss: 0.001967465039342642\n",
      "iteration 13984, loss: 0.0019260674016550183\n",
      "iteration 13985, loss: 0.0019949707202613354\n",
      "iteration 13986, loss: 0.0018384456634521484\n",
      "iteration 13987, loss: 0.0014472880866378546\n",
      "iteration 13988, loss: 0.0016930323326960206\n",
      "iteration 13989, loss: 0.00189988745842129\n",
      "iteration 13990, loss: 0.0017523991409689188\n",
      "iteration 13991, loss: 0.0018220862839370966\n",
      "iteration 13992, loss: 0.0017242231406271458\n",
      "iteration 13993, loss: 0.0014877854846417904\n",
      "iteration 13994, loss: 0.0020738262683153152\n",
      "iteration 13995, loss: 0.0018737680511549115\n",
      "iteration 13996, loss: 0.001488307025283575\n",
      "iteration 13997, loss: 0.001426036236807704\n",
      "iteration 13998, loss: 0.0014310786500573158\n",
      "iteration 13999, loss: 0.001560024218633771\n",
      "iteration 14000, loss: 0.0017003967659547925\n",
      "iteration 14001, loss: 0.001563653931953013\n",
      "iteration 14002, loss: 0.001791986171156168\n",
      "iteration 14003, loss: 0.0013703692238777876\n",
      "iteration 14004, loss: 0.0014856373891234398\n",
      "iteration 14005, loss: 0.0017252385150641203\n",
      "iteration 14006, loss: 0.0013306288747116923\n",
      "iteration 14007, loss: 0.0017783025978133082\n",
      "iteration 14008, loss: 0.001827210420742631\n",
      "iteration 14009, loss: 0.001705789822153747\n",
      "iteration 14010, loss: 0.0014376586768776178\n",
      "iteration 14011, loss: 0.0017326573142781854\n",
      "iteration 14012, loss: 0.0014940024120733142\n",
      "iteration 14013, loss: 0.0015824942383915186\n",
      "iteration 14014, loss: 0.0016633163904771209\n",
      "iteration 14015, loss: 0.0021288879215717316\n",
      "iteration 14016, loss: 0.0017467675497755408\n",
      "iteration 14017, loss: 0.0017841928638517857\n",
      "iteration 14018, loss: 0.0015600350452587008\n",
      "iteration 14019, loss: 0.0015880721621215343\n",
      "iteration 14020, loss: 0.0016105093527585268\n",
      "iteration 14021, loss: 0.0016644839197397232\n",
      "iteration 14022, loss: 0.0015859914710745215\n",
      "iteration 14023, loss: 0.0016602703835815191\n",
      "iteration 14024, loss: 0.0016077242325991392\n",
      "iteration 14025, loss: 0.0015011155046522617\n",
      "iteration 14026, loss: 0.0019276222446933389\n",
      "iteration 14027, loss: 0.0015205965610221028\n",
      "iteration 14028, loss: 0.0015802397392690182\n",
      "iteration 14029, loss: 0.0016074400627985597\n",
      "iteration 14030, loss: 0.001524199964478612\n",
      "iteration 14031, loss: 0.0019372910028323531\n",
      "iteration 14032, loss: 0.0014969035983085632\n",
      "iteration 14033, loss: 0.0014446028508245945\n",
      "iteration 14034, loss: 0.0014611708465963602\n",
      "iteration 14035, loss: 0.0015833443030714989\n",
      "iteration 14036, loss: 0.0017681539757177234\n",
      "iteration 14037, loss: 0.0017448040889576077\n",
      "iteration 14038, loss: 0.0014738081954419613\n",
      "iteration 14039, loss: 0.0017314518336206675\n",
      "iteration 14040, loss: 0.0019967402331531048\n",
      "iteration 14041, loss: 0.0016216023359447718\n",
      "iteration 14042, loss: 0.00138554023578763\n",
      "iteration 14043, loss: 0.001474555698223412\n",
      "iteration 14044, loss: 0.0018895030952990055\n",
      "iteration 14045, loss: 0.0015832006465643644\n",
      "iteration 14046, loss: 0.0016439177561551332\n",
      "iteration 14047, loss: 0.0015747888246551156\n",
      "iteration 14048, loss: 0.0014315241714939475\n",
      "iteration 14049, loss: 0.001693842699751258\n",
      "iteration 14050, loss: 0.0019047404639422894\n",
      "iteration 14051, loss: 0.001737135462462902\n",
      "iteration 14052, loss: 0.0014454140327870846\n",
      "iteration 14053, loss: 0.0016970019787549973\n",
      "iteration 14054, loss: 0.0018121973844245076\n",
      "iteration 14055, loss: 0.0016378606669604778\n",
      "iteration 14056, loss: 0.0014316908782348037\n",
      "iteration 14057, loss: 0.0015597057063132524\n",
      "iteration 14058, loss: 0.001729240408167243\n",
      "iteration 14059, loss: 0.0017501800321042538\n",
      "iteration 14060, loss: 0.0016370215453207493\n",
      "iteration 14061, loss: 0.001689739990979433\n",
      "iteration 14062, loss: 0.0015808753669261932\n",
      "iteration 14063, loss: 0.0016030239639803767\n",
      "iteration 14064, loss: 0.0017423880053684115\n",
      "iteration 14065, loss: 0.0017152470536530018\n",
      "iteration 14066, loss: 0.001799010788090527\n",
      "iteration 14067, loss: 0.002333794953301549\n",
      "iteration 14068, loss: 0.0018073732499033213\n",
      "iteration 14069, loss: 0.001692170393653214\n",
      "iteration 14070, loss: 0.0021841656416654587\n",
      "iteration 14071, loss: 0.0016632663318887353\n",
      "iteration 14072, loss: 0.0017398933414369822\n",
      "iteration 14073, loss: 0.0017819602508097887\n",
      "iteration 14074, loss: 0.0021137953735888004\n",
      "iteration 14075, loss: 0.0017780520720407367\n",
      "iteration 14076, loss: 0.0024342751130461693\n",
      "iteration 14077, loss: 0.001499297795817256\n",
      "iteration 14078, loss: 0.0017215998377650976\n",
      "iteration 14079, loss: 0.0016735694371163845\n",
      "iteration 14080, loss: 0.0016144930850714445\n",
      "iteration 14081, loss: 0.0017252409597858787\n",
      "iteration 14082, loss: 0.0018570655956864357\n",
      "iteration 14083, loss: 0.001688583754003048\n",
      "iteration 14084, loss: 0.0019691146444529295\n",
      "iteration 14085, loss: 0.0015410843770951033\n",
      "iteration 14086, loss: 0.001932507730089128\n",
      "iteration 14087, loss: 0.0017387652769684792\n",
      "iteration 14088, loss: 0.0018377802334725857\n",
      "iteration 14089, loss: 0.0015161375049501657\n",
      "iteration 14090, loss: 0.002069908194243908\n",
      "iteration 14091, loss: 0.0015726587735116482\n",
      "iteration 14092, loss: 0.001708446303382516\n",
      "iteration 14093, loss: 0.0020556803792715073\n",
      "iteration 14094, loss: 0.0018469432834535837\n",
      "iteration 14095, loss: 0.00164507154840976\n",
      "iteration 14096, loss: 0.001559833763167262\n",
      "iteration 14097, loss: 0.0017952935304492712\n",
      "iteration 14098, loss: 0.0015995167195796967\n",
      "iteration 14099, loss: 0.0017097587697207928\n",
      "iteration 14100, loss: 0.0017655554693192244\n",
      "iteration 14101, loss: 0.0016730763018131256\n",
      "iteration 14102, loss: 0.001663109753280878\n",
      "iteration 14103, loss: 0.0016903774812817574\n",
      "iteration 14104, loss: 0.0017734196735545993\n",
      "iteration 14105, loss: 0.0019106303807348013\n",
      "iteration 14106, loss: 0.0016264410223811865\n",
      "iteration 14107, loss: 0.001826911000534892\n",
      "iteration 14108, loss: 0.0015891678631305695\n",
      "iteration 14109, loss: 0.0017889479640871286\n",
      "iteration 14110, loss: 0.001684269867837429\n",
      "iteration 14111, loss: 0.0018948960350826383\n",
      "iteration 14112, loss: 0.0017645996995270252\n",
      "iteration 14113, loss: 0.001525041414424777\n",
      "iteration 14114, loss: 0.0014283459167927504\n",
      "iteration 14115, loss: 0.0016607008874416351\n",
      "iteration 14116, loss: 0.0018206846434623003\n",
      "iteration 14117, loss: 0.0020553215872496367\n",
      "iteration 14118, loss: 0.0017270706593990326\n",
      "iteration 14119, loss: 0.0012948475778102875\n",
      "iteration 14120, loss: 0.0018790174508467317\n",
      "iteration 14121, loss: 0.001455263001844287\n",
      "iteration 14122, loss: 0.0018912716768682003\n",
      "iteration 14123, loss: 0.0017323542851954699\n",
      "iteration 14124, loss: 0.0014009465230628848\n",
      "iteration 14125, loss: 0.001758266007527709\n",
      "iteration 14126, loss: 0.0014320728369057178\n",
      "iteration 14127, loss: 0.0016791940433904529\n",
      "iteration 14128, loss: 0.001372717204503715\n",
      "iteration 14129, loss: 0.0015942991012707353\n",
      "iteration 14130, loss: 0.0016229802276939154\n",
      "iteration 14131, loss: 0.0016801772871986032\n",
      "iteration 14132, loss: 0.001830959226936102\n",
      "iteration 14133, loss: 0.0012813904322683811\n",
      "iteration 14134, loss: 0.0016366564668715\n",
      "iteration 14135, loss: 0.001557540032081306\n",
      "iteration 14136, loss: 0.0015669218264520168\n",
      "iteration 14137, loss: 0.0017659328877925873\n",
      "iteration 14138, loss: 0.0015683943638578057\n",
      "iteration 14139, loss: 0.0014407560229301453\n",
      "iteration 14140, loss: 0.0014252616092562675\n",
      "iteration 14141, loss: 0.001347136334516108\n",
      "iteration 14142, loss: 0.0017534636426717043\n",
      "iteration 14143, loss: 0.0016293064691126347\n",
      "iteration 14144, loss: 0.0014109844341874123\n",
      "iteration 14145, loss: 0.0014979001134634018\n",
      "iteration 14146, loss: 0.0017332402057945728\n",
      "iteration 14147, loss: 0.0019042807398363948\n",
      "iteration 14148, loss: 0.0018698385683819652\n",
      "iteration 14149, loss: 0.0017808795673772693\n",
      "iteration 14150, loss: 0.0017107321182265878\n",
      "iteration 14151, loss: 0.0013671203050762415\n",
      "iteration 14152, loss: 0.0018467637710273266\n",
      "iteration 14153, loss: 0.0015991467516869307\n",
      "iteration 14154, loss: 0.001610123785212636\n",
      "iteration 14155, loss: 0.002037968020886183\n",
      "iteration 14156, loss: 0.0015244910027831793\n",
      "iteration 14157, loss: 0.0016151173040270805\n",
      "iteration 14158, loss: 0.0014143344014883041\n",
      "iteration 14159, loss: 0.001845052232965827\n",
      "iteration 14160, loss: 0.001658205408602953\n",
      "iteration 14161, loss: 0.0015947631327435374\n",
      "iteration 14162, loss: 0.0016990355215966702\n",
      "iteration 14163, loss: 0.0014252522960305214\n",
      "iteration 14164, loss: 0.001496422803029418\n",
      "iteration 14165, loss: 0.0018545619677752256\n",
      "iteration 14166, loss: 0.0015324519481509924\n",
      "iteration 14167, loss: 0.0018095617415383458\n",
      "iteration 14168, loss: 0.0016526459949091077\n",
      "iteration 14169, loss: 0.001759268343448639\n",
      "iteration 14170, loss: 0.0015756427310407162\n",
      "iteration 14171, loss: 0.0017728134989738464\n",
      "iteration 14172, loss: 0.0014528665924444795\n",
      "iteration 14173, loss: 0.0015229980926960707\n",
      "iteration 14174, loss: 0.0015264097601175308\n",
      "iteration 14175, loss: 0.0017524480354040861\n",
      "iteration 14176, loss: 0.0014298849273473024\n",
      "iteration 14177, loss: 0.0020426542032510042\n",
      "iteration 14178, loss: 0.001510406262241304\n",
      "iteration 14179, loss: 0.0017888813745230436\n",
      "iteration 14180, loss: 0.001563197816722095\n",
      "iteration 14181, loss: 0.0016442553605884314\n",
      "iteration 14182, loss: 0.0016470903065055609\n",
      "iteration 14183, loss: 0.0015774883795529604\n",
      "iteration 14184, loss: 0.0016209105961024761\n",
      "iteration 14185, loss: 0.0015121673932299018\n",
      "iteration 14186, loss: 0.0015091649256646633\n",
      "iteration 14187, loss: 0.0018167438684031367\n",
      "iteration 14188, loss: 0.0018395793158560991\n",
      "iteration 14189, loss: 0.0013384560588747263\n",
      "iteration 14190, loss: 0.0016448193928226829\n",
      "iteration 14191, loss: 0.001708437455818057\n",
      "iteration 14192, loss: 0.0013315774267539382\n",
      "iteration 14193, loss: 0.0012855406384915113\n",
      "iteration 14194, loss: 0.002002281602472067\n",
      "iteration 14195, loss: 0.0015881124418228865\n",
      "iteration 14196, loss: 0.001487651257775724\n",
      "iteration 14197, loss: 0.0016135411569848657\n",
      "iteration 14198, loss: 0.0013599793892353773\n",
      "iteration 14199, loss: 0.0013817944563925266\n",
      "iteration 14200, loss: 0.001489279791712761\n",
      "iteration 14201, loss: 0.0013990908628329635\n",
      "iteration 14202, loss: 0.0016658678650856018\n",
      "iteration 14203, loss: 0.001683513750322163\n",
      "iteration 14204, loss: 0.0015234231250360608\n",
      "iteration 14205, loss: 0.001694484381005168\n",
      "iteration 14206, loss: 0.0018239566124975681\n",
      "iteration 14207, loss: 0.0013589264126494527\n",
      "iteration 14208, loss: 0.0015248509589582682\n",
      "iteration 14209, loss: 0.001580157084390521\n",
      "iteration 14210, loss: 0.001665667281486094\n",
      "iteration 14211, loss: 0.0016594622284173965\n",
      "iteration 14212, loss: 0.0017149781342595816\n",
      "iteration 14213, loss: 0.0014027194119989872\n",
      "iteration 14214, loss: 0.0018328147707507014\n",
      "iteration 14215, loss: 0.0017585146706551313\n",
      "iteration 14216, loss: 0.0014633176615461707\n",
      "iteration 14217, loss: 0.0015301560051739216\n",
      "iteration 14218, loss: 0.0018517293501645327\n",
      "iteration 14219, loss: 0.0016020945040509105\n",
      "iteration 14220, loss: 0.0019299776758998632\n",
      "iteration 14221, loss: 0.0018040550639852881\n",
      "iteration 14222, loss: 0.001757386839017272\n",
      "iteration 14223, loss: 0.001847773790359497\n",
      "iteration 14224, loss: 0.001608851132914424\n",
      "iteration 14225, loss: 0.0015513538382947445\n",
      "iteration 14226, loss: 0.0016972325975075364\n",
      "iteration 14227, loss: 0.0018125746864825487\n",
      "iteration 14228, loss: 0.0018059640424326062\n",
      "iteration 14229, loss: 0.0019803717732429504\n",
      "iteration 14230, loss: 0.0015905543696135283\n",
      "iteration 14231, loss: 0.0017816381296142936\n",
      "iteration 14232, loss: 0.001736992970108986\n",
      "iteration 14233, loss: 0.0014744859654456377\n",
      "iteration 14234, loss: 0.001483703381381929\n",
      "iteration 14235, loss: 0.001647165510803461\n",
      "iteration 14236, loss: 0.0013965979451313615\n",
      "iteration 14237, loss: 0.001790195587091148\n",
      "iteration 14238, loss: 0.0016533720772713423\n",
      "iteration 14239, loss: 0.001601186115294695\n",
      "iteration 14240, loss: 0.0015920359874144197\n",
      "iteration 14241, loss: 0.001857008202932775\n",
      "iteration 14242, loss: 0.0018543492769822478\n",
      "iteration 14243, loss: 0.0014026421122252941\n",
      "iteration 14244, loss: 0.0015264630783349276\n",
      "iteration 14245, loss: 0.0014412616146728396\n",
      "iteration 14246, loss: 0.0013536993646994233\n",
      "iteration 14247, loss: 0.0015441373689100146\n",
      "iteration 14248, loss: 0.001560233999043703\n",
      "iteration 14249, loss: 0.0013670420739799738\n",
      "iteration 14250, loss: 0.001460716244764626\n",
      "iteration 14251, loss: 0.0020173578523099422\n",
      "iteration 14252, loss: 0.0015917678829282522\n",
      "iteration 14253, loss: 0.001283695106394589\n",
      "iteration 14254, loss: 0.001587703125551343\n",
      "iteration 14255, loss: 0.0014112740755081177\n",
      "iteration 14256, loss: 0.0017723098862916231\n",
      "iteration 14257, loss: 0.0018483359599485993\n",
      "iteration 14258, loss: 0.0017865769332274795\n",
      "iteration 14259, loss: 0.001733951037749648\n",
      "iteration 14260, loss: 0.00203639711253345\n",
      "iteration 14261, loss: 0.0016372964018955827\n",
      "iteration 14262, loss: 0.001466717105358839\n",
      "iteration 14263, loss: 0.0016866998048499227\n",
      "iteration 14264, loss: 0.0016706081805750728\n",
      "iteration 14265, loss: 0.0013734165113419294\n",
      "iteration 14266, loss: 0.0017114656511694193\n",
      "iteration 14267, loss: 0.0015893406234681606\n",
      "iteration 14268, loss: 0.001523152575828135\n",
      "iteration 14269, loss: 0.001455810503102839\n",
      "iteration 14270, loss: 0.0016413548728451133\n",
      "iteration 14271, loss: 0.0016401458997279406\n",
      "iteration 14272, loss: 0.0017177133122459054\n",
      "iteration 14273, loss: 0.0018575388239696622\n",
      "iteration 14274, loss: 0.0016773086972534657\n",
      "iteration 14275, loss: 0.001357813598588109\n",
      "iteration 14276, loss: 0.0014955223305150867\n",
      "iteration 14277, loss: 0.0015886941691860557\n",
      "iteration 14278, loss: 0.001704568276181817\n",
      "iteration 14279, loss: 0.0014973576180636883\n",
      "iteration 14280, loss: 0.001641938230022788\n",
      "iteration 14281, loss: 0.0015556017169728875\n",
      "iteration 14282, loss: 0.001445388188585639\n",
      "iteration 14283, loss: 0.0014826132683083415\n",
      "iteration 14284, loss: 0.0014881781535223126\n",
      "iteration 14285, loss: 0.001518942997790873\n",
      "iteration 14286, loss: 0.001678071916103363\n",
      "iteration 14287, loss: 0.0016301628202199936\n",
      "iteration 14288, loss: 0.001784018473699689\n",
      "iteration 14289, loss: 0.002036311663687229\n",
      "iteration 14290, loss: 0.002067365450784564\n",
      "iteration 14291, loss: 0.0017623910680413246\n",
      "iteration 14292, loss: 0.001744014909490943\n",
      "iteration 14293, loss: 0.0013942928053438663\n",
      "iteration 14294, loss: 0.0017954265931621194\n",
      "iteration 14295, loss: 0.001601377036422491\n",
      "iteration 14296, loss: 0.0013059598859399557\n",
      "iteration 14297, loss: 0.0016397427534684539\n",
      "iteration 14298, loss: 0.0015482793096452951\n",
      "iteration 14299, loss: 0.0016129892319440842\n",
      "iteration 14300, loss: 0.0016463829670101404\n",
      "iteration 14301, loss: 0.0015219958731904626\n",
      "iteration 14302, loss: 0.0014961836859583855\n",
      "iteration 14303, loss: 0.0016502460930496454\n",
      "iteration 14304, loss: 0.0018037918489426374\n",
      "iteration 14305, loss: 0.001696562161669135\n",
      "iteration 14306, loss: 0.0017745698569342494\n",
      "iteration 14307, loss: 0.0017626532353460789\n",
      "iteration 14308, loss: 0.001581311342306435\n",
      "iteration 14309, loss: 0.0017940402030944824\n",
      "iteration 14310, loss: 0.0017163939774036407\n",
      "iteration 14311, loss: 0.0013675427762791514\n",
      "iteration 14312, loss: 0.0017170403152704239\n",
      "iteration 14313, loss: 0.0014514353824779391\n",
      "iteration 14314, loss: 0.0014346633106470108\n",
      "iteration 14315, loss: 0.0015973233385011554\n",
      "iteration 14316, loss: 0.0015724916011095047\n",
      "iteration 14317, loss: 0.0017460384406149387\n",
      "iteration 14318, loss: 0.001737274811603129\n",
      "iteration 14319, loss: 0.0013915803283452988\n",
      "iteration 14320, loss: 0.001277454080991447\n",
      "iteration 14321, loss: 0.0016426737420260906\n",
      "iteration 14322, loss: 0.0016853056149557233\n",
      "iteration 14323, loss: 0.0018193110590800643\n",
      "iteration 14324, loss: 0.0015201529022306204\n",
      "iteration 14325, loss: 0.0018873759545385838\n",
      "iteration 14326, loss: 0.0016178672667592764\n",
      "iteration 14327, loss: 0.001426118309609592\n",
      "iteration 14328, loss: 0.0014905044808983803\n",
      "iteration 14329, loss: 0.0014776138123124838\n",
      "iteration 14330, loss: 0.0017524661961942911\n",
      "iteration 14331, loss: 0.0013547383714467287\n",
      "iteration 14332, loss: 0.0017414102330803871\n",
      "iteration 14333, loss: 0.0016271460335701704\n",
      "iteration 14334, loss: 0.001972602680325508\n",
      "iteration 14335, loss: 0.002090122550725937\n",
      "iteration 14336, loss: 0.0017989528132602572\n",
      "iteration 14337, loss: 0.0017871594754979014\n",
      "iteration 14338, loss: 0.0016159573569893837\n",
      "iteration 14339, loss: 0.0015118352603167295\n",
      "iteration 14340, loss: 0.001369012868963182\n",
      "iteration 14341, loss: 0.0016910226549953222\n",
      "iteration 14342, loss: 0.0020691894460469484\n",
      "iteration 14343, loss: 0.0016551960725337267\n",
      "iteration 14344, loss: 0.0016958039486780763\n",
      "iteration 14345, loss: 0.0021550331730395555\n",
      "iteration 14346, loss: 0.0016136971535161138\n",
      "iteration 14347, loss: 0.00156859727576375\n",
      "iteration 14348, loss: 0.0018273529130965471\n",
      "iteration 14349, loss: 0.0019011927070096135\n",
      "iteration 14350, loss: 0.0014820597134530544\n",
      "iteration 14351, loss: 0.0018381220288574696\n",
      "iteration 14352, loss: 0.0017575784586369991\n",
      "iteration 14353, loss: 0.0014980672858655453\n",
      "iteration 14354, loss: 0.0017471221508458257\n",
      "iteration 14355, loss: 0.002050677314400673\n",
      "iteration 14356, loss: 0.0018429741030558944\n",
      "iteration 14357, loss: 0.0015315923374146223\n",
      "iteration 14358, loss: 0.0016300021670758724\n",
      "iteration 14359, loss: 0.0017757248133420944\n",
      "iteration 14360, loss: 0.0015878182603046298\n",
      "iteration 14361, loss: 0.0016356552951037884\n",
      "iteration 14362, loss: 0.0018376472871750593\n",
      "iteration 14363, loss: 0.0016087866388261318\n",
      "iteration 14364, loss: 0.0017431247979402542\n",
      "iteration 14365, loss: 0.0020427072886377573\n",
      "iteration 14366, loss: 0.0020121848210692406\n",
      "iteration 14367, loss: 0.0013693899381905794\n",
      "iteration 14368, loss: 0.0015224474482238293\n",
      "iteration 14369, loss: 0.0014649902004748583\n",
      "iteration 14370, loss: 0.001941232243552804\n",
      "iteration 14371, loss: 0.0015775470528751612\n",
      "iteration 14372, loss: 0.0015666175168007612\n",
      "iteration 14373, loss: 0.002007879316806793\n",
      "iteration 14374, loss: 0.0016341018490493298\n",
      "iteration 14375, loss: 0.0015853855293244123\n",
      "iteration 14376, loss: 0.0017858243081718683\n",
      "iteration 14377, loss: 0.0017920674290508032\n",
      "iteration 14378, loss: 0.0018096647690981627\n",
      "iteration 14379, loss: 0.001610002014786005\n",
      "iteration 14380, loss: 0.0016966813709586859\n",
      "iteration 14381, loss: 0.001947868149727583\n",
      "iteration 14382, loss: 0.0017016238998621702\n",
      "iteration 14383, loss: 0.0016087844269350171\n",
      "iteration 14384, loss: 0.0018644240917637944\n",
      "iteration 14385, loss: 0.0012639460619539022\n",
      "iteration 14386, loss: 0.001598693197593093\n",
      "iteration 14387, loss: 0.0018814678769558668\n",
      "iteration 14388, loss: 0.001562317251227796\n",
      "iteration 14389, loss: 0.0015606377273797989\n",
      "iteration 14390, loss: 0.0014949212782084942\n",
      "iteration 14391, loss: 0.0013544405810534954\n",
      "iteration 14392, loss: 0.001681898022070527\n",
      "iteration 14393, loss: 0.0015548855299130082\n",
      "iteration 14394, loss: 0.0014655867125838995\n",
      "iteration 14395, loss: 0.0015587210655212402\n",
      "iteration 14396, loss: 0.001844180515035987\n",
      "iteration 14397, loss: 0.0017548413015902042\n",
      "iteration 14398, loss: 0.001618243521079421\n",
      "iteration 14399, loss: 0.0015765881398692727\n",
      "iteration 14400, loss: 0.0013358736177906394\n",
      "iteration 14401, loss: 0.0016566170379519463\n",
      "iteration 14402, loss: 0.0015675189206376672\n",
      "iteration 14403, loss: 0.001553265959955752\n",
      "iteration 14404, loss: 0.001832554000429809\n",
      "iteration 14405, loss: 0.0018109951633960009\n",
      "iteration 14406, loss: 0.0016604082193225622\n",
      "iteration 14407, loss: 0.002026171423494816\n",
      "iteration 14408, loss: 0.0015108947409316897\n",
      "iteration 14409, loss: 0.0013830027310177684\n",
      "iteration 14410, loss: 0.001607932848855853\n",
      "iteration 14411, loss: 0.0017135001253336668\n",
      "iteration 14412, loss: 0.0015368585009127855\n",
      "iteration 14413, loss: 0.0015846671303734183\n",
      "iteration 14414, loss: 0.001673673978075385\n",
      "iteration 14415, loss: 0.0017055381322279572\n",
      "iteration 14416, loss: 0.0016831940738484263\n",
      "iteration 14417, loss: 0.0014239819720387459\n",
      "iteration 14418, loss: 0.0018682752270251513\n",
      "iteration 14419, loss: 0.0014812008012086153\n",
      "iteration 14420, loss: 0.00171666219830513\n",
      "iteration 14421, loss: 0.0013835777062922716\n",
      "iteration 14422, loss: 0.0013593556359410286\n",
      "iteration 14423, loss: 0.00150611053686589\n",
      "iteration 14424, loss: 0.0016445962246507406\n",
      "iteration 14425, loss: 0.0015167813980951905\n",
      "iteration 14426, loss: 0.001498931902460754\n",
      "iteration 14427, loss: 0.0014741902705281973\n",
      "iteration 14428, loss: 0.001354527659714222\n",
      "iteration 14429, loss: 0.0014478150987997651\n",
      "iteration 14430, loss: 0.0017010285519063473\n",
      "iteration 14431, loss: 0.0016460916958749294\n",
      "iteration 14432, loss: 0.0019163170363754034\n",
      "iteration 14433, loss: 0.001902701100334525\n",
      "iteration 14434, loss: 0.0013559937942773104\n",
      "iteration 14435, loss: 0.0017963185673579574\n",
      "iteration 14436, loss: 0.0015951597597450018\n",
      "iteration 14437, loss: 0.001516316318884492\n",
      "iteration 14438, loss: 0.0016287433682009578\n",
      "iteration 14439, loss: 0.0013531051808968186\n",
      "iteration 14440, loss: 0.0017715266440063715\n",
      "iteration 14441, loss: 0.0017403140664100647\n",
      "iteration 14442, loss: 0.0016926341922953725\n",
      "iteration 14443, loss: 0.001893692766316235\n",
      "iteration 14444, loss: 0.0019114569295197725\n",
      "iteration 14445, loss: 0.001619604299776256\n",
      "iteration 14446, loss: 0.0015527063515037298\n",
      "iteration 14447, loss: 0.0011909327004104853\n",
      "iteration 14448, loss: 0.0016129824798554182\n",
      "iteration 14449, loss: 0.001720766187645495\n",
      "iteration 14450, loss: 0.0019545143004506826\n",
      "iteration 14451, loss: 0.001512414775788784\n",
      "iteration 14452, loss: 0.0015233836602419615\n",
      "iteration 14453, loss: 0.0018474140670150518\n",
      "iteration 14454, loss: 0.0013317944249138236\n",
      "iteration 14455, loss: 0.0016789359506219625\n",
      "iteration 14456, loss: 0.001941794529557228\n",
      "iteration 14457, loss: 0.0015239648055285215\n",
      "iteration 14458, loss: 0.0014745433581992984\n",
      "iteration 14459, loss: 0.0017967477906495333\n",
      "iteration 14460, loss: 0.0016426946967840195\n",
      "iteration 14461, loss: 0.0013856033328920603\n",
      "iteration 14462, loss: 0.0018414666410535574\n",
      "iteration 14463, loss: 0.0016439342871308327\n",
      "iteration 14464, loss: 0.0014903376577422023\n",
      "iteration 14465, loss: 0.0016610876191407442\n",
      "iteration 14466, loss: 0.00203474098816514\n",
      "iteration 14467, loss: 0.0018755928613245487\n",
      "iteration 14468, loss: 0.001311544212512672\n",
      "iteration 14469, loss: 0.0016931949649006128\n",
      "iteration 14470, loss: 0.0017845907714217901\n",
      "iteration 14471, loss: 0.0014428431168198586\n",
      "iteration 14472, loss: 0.0017353142611682415\n",
      "iteration 14473, loss: 0.0016071273712441325\n",
      "iteration 14474, loss: 0.002126614563167095\n",
      "iteration 14475, loss: 0.0016165258130058646\n",
      "iteration 14476, loss: 0.0017682760953903198\n",
      "iteration 14477, loss: 0.001468834700062871\n",
      "iteration 14478, loss: 0.001671167672611773\n",
      "iteration 14479, loss: 0.001711247838102281\n",
      "iteration 14480, loss: 0.0013461431954056025\n",
      "iteration 14481, loss: 0.0019171827007085085\n",
      "iteration 14482, loss: 0.0017545754089951515\n",
      "iteration 14483, loss: 0.0016360350418835878\n",
      "iteration 14484, loss: 0.0017755454173311591\n",
      "iteration 14485, loss: 0.001433218945749104\n",
      "iteration 14486, loss: 0.0015578522579744458\n",
      "iteration 14487, loss: 0.0018631445709615946\n",
      "iteration 14488, loss: 0.0016379435546696186\n",
      "iteration 14489, loss: 0.0015172753483057022\n",
      "iteration 14490, loss: 0.0014426365960389376\n",
      "iteration 14491, loss: 0.0017024808330461383\n",
      "iteration 14492, loss: 0.0015266824048012495\n",
      "iteration 14493, loss: 0.001483652275055647\n",
      "iteration 14494, loss: 0.0017487016739323735\n",
      "iteration 14495, loss: 0.0013436692534014583\n",
      "iteration 14496, loss: 0.0016721554566174746\n",
      "iteration 14497, loss: 0.0019221031107008457\n",
      "iteration 14498, loss: 0.0018663604278117418\n",
      "iteration 14499, loss: 0.0015958164585754275\n",
      "iteration 14500, loss: 0.00173663510940969\n",
      "iteration 14501, loss: 0.0020960757974535227\n",
      "iteration 14502, loss: 0.0014075885992497206\n",
      "iteration 14503, loss: 0.001618232810869813\n",
      "iteration 14504, loss: 0.0014710421673953533\n",
      "iteration 14505, loss: 0.0015480283182114363\n",
      "iteration 14506, loss: 0.0018343598349019885\n",
      "iteration 14507, loss: 0.001645196694880724\n",
      "iteration 14508, loss: 0.001865532249212265\n",
      "iteration 14509, loss: 0.001555491122417152\n",
      "iteration 14510, loss: 0.0013585846172645688\n",
      "iteration 14511, loss: 0.0011752571444958448\n",
      "iteration 14512, loss: 0.001342341653071344\n",
      "iteration 14513, loss: 0.0015588533133268356\n",
      "iteration 14514, loss: 0.0015976270660758018\n",
      "iteration 14515, loss: 0.0015911802183836699\n",
      "iteration 14516, loss: 0.002004161011427641\n",
      "iteration 14517, loss: 0.001444437075406313\n",
      "iteration 14518, loss: 0.0020059484522789717\n",
      "iteration 14519, loss: 0.0015540984459221363\n",
      "iteration 14520, loss: 0.001527908956632018\n",
      "iteration 14521, loss: 0.001742502790875733\n",
      "iteration 14522, loss: 0.0016350143123418093\n",
      "iteration 14523, loss: 0.0013623913982883096\n",
      "iteration 14524, loss: 0.0015792909543961287\n",
      "iteration 14525, loss: 0.0015387819148600101\n",
      "iteration 14526, loss: 0.0016231450717896223\n",
      "iteration 14527, loss: 0.0013692849315702915\n",
      "iteration 14528, loss: 0.0016452183481305838\n",
      "iteration 14529, loss: 0.0015946578932926059\n",
      "iteration 14530, loss: 0.0017074004281312227\n",
      "iteration 14531, loss: 0.0015234710881486535\n",
      "iteration 14532, loss: 0.0019889292307198048\n",
      "iteration 14533, loss: 0.001523258863016963\n",
      "iteration 14534, loss: 0.0016943515511229634\n",
      "iteration 14535, loss: 0.001556822215206921\n",
      "iteration 14536, loss: 0.0016178336227312684\n",
      "iteration 14537, loss: 0.001839975593611598\n",
      "iteration 14538, loss: 0.0011822179658338428\n",
      "iteration 14539, loss: 0.001417029183357954\n",
      "iteration 14540, loss: 0.0019171121530234814\n",
      "iteration 14541, loss: 0.001653601648285985\n",
      "iteration 14542, loss: 0.0019334800308570266\n",
      "iteration 14543, loss: 0.0014851068845018744\n",
      "iteration 14544, loss: 0.0016236724331974983\n",
      "iteration 14545, loss: 0.0012451866641640663\n",
      "iteration 14546, loss: 0.0016851574182510376\n",
      "iteration 14547, loss: 0.0014687664806842804\n",
      "iteration 14548, loss: 0.0012411595089361072\n",
      "iteration 14549, loss: 0.001304082339629531\n",
      "iteration 14550, loss: 0.0015963127370923758\n",
      "iteration 14551, loss: 0.0015521431341767311\n",
      "iteration 14552, loss: 0.001470829825848341\n",
      "iteration 14553, loss: 0.0016771180089563131\n",
      "iteration 14554, loss: 0.0018404421862214804\n",
      "iteration 14555, loss: 0.001732325879856944\n",
      "iteration 14556, loss: 0.0016162802930921316\n",
      "iteration 14557, loss: 0.002253087470307946\n",
      "iteration 14558, loss: 0.0019039611797779799\n",
      "iteration 14559, loss: 0.0016405258793383837\n",
      "iteration 14560, loss: 0.001898065209388733\n",
      "iteration 14561, loss: 0.0016111850272864103\n",
      "iteration 14562, loss: 0.0017189469654113054\n",
      "iteration 14563, loss: 0.0017858638893812895\n",
      "iteration 14564, loss: 0.0017965283477678895\n",
      "iteration 14565, loss: 0.001675296458415687\n",
      "iteration 14566, loss: 0.001530250534415245\n",
      "iteration 14567, loss: 0.0016351722879335284\n",
      "iteration 14568, loss: 0.0017337324097752571\n",
      "iteration 14569, loss: 0.001839878736063838\n",
      "iteration 14570, loss: 0.001746782218106091\n",
      "iteration 14571, loss: 0.0016460174228996038\n",
      "iteration 14572, loss: 0.0018178368918597698\n",
      "iteration 14573, loss: 0.0019325141329318285\n",
      "iteration 14574, loss: 0.001706205541267991\n",
      "iteration 14575, loss: 0.0015767075819894671\n",
      "iteration 14576, loss: 0.0016134176403284073\n",
      "iteration 14577, loss: 0.00151834636926651\n",
      "iteration 14578, loss: 0.0016678136307746172\n",
      "iteration 14579, loss: 0.0014949948526918888\n",
      "iteration 14580, loss: 0.0019355182303115726\n",
      "iteration 14581, loss: 0.0016449703834950924\n",
      "iteration 14582, loss: 0.00162532739341259\n",
      "iteration 14583, loss: 0.0015492539387196302\n",
      "iteration 14584, loss: 0.0017559482948854566\n",
      "iteration 14585, loss: 0.0016848873347043991\n",
      "iteration 14586, loss: 0.0015610158443450928\n",
      "iteration 14587, loss: 0.0012722252868115902\n",
      "iteration 14588, loss: 0.0019568060524761677\n",
      "iteration 14589, loss: 0.0015714921755716205\n",
      "iteration 14590, loss: 0.0014368249103426933\n",
      "iteration 14591, loss: 0.001526437234133482\n",
      "iteration 14592, loss: 0.0012899716384708881\n",
      "iteration 14593, loss: 0.0017761064227670431\n",
      "iteration 14594, loss: 0.0015625007217749953\n",
      "iteration 14595, loss: 0.0013827949296683073\n",
      "iteration 14596, loss: 0.0017597014084458351\n",
      "iteration 14597, loss: 0.0017268427181988955\n",
      "iteration 14598, loss: 0.0017204629257321358\n",
      "iteration 14599, loss: 0.0016031863633543253\n",
      "iteration 14600, loss: 0.0015110771637409925\n",
      "iteration 14601, loss: 0.0015854539815336466\n",
      "iteration 14602, loss: 0.0013573688920587301\n",
      "iteration 14603, loss: 0.001634670072235167\n",
      "iteration 14604, loss: 0.0015635108575224876\n",
      "iteration 14605, loss: 0.0015020851278677583\n",
      "iteration 14606, loss: 0.0011629723012447357\n",
      "iteration 14607, loss: 0.0019946666434407234\n",
      "iteration 14608, loss: 0.0016149856382980943\n",
      "iteration 14609, loss: 0.0015329765155911446\n",
      "iteration 14610, loss: 0.0017838181229308248\n",
      "iteration 14611, loss: 0.001538164564408362\n",
      "iteration 14612, loss: 0.0017087663291022182\n",
      "iteration 14613, loss: 0.0015192758291959763\n",
      "iteration 14614, loss: 0.001694855629466474\n",
      "iteration 14615, loss: 0.0016603590920567513\n",
      "iteration 14616, loss: 0.0017757286550477147\n",
      "iteration 14617, loss: 0.0018604400102049112\n",
      "iteration 14618, loss: 0.0016353004612028599\n",
      "iteration 14619, loss: 0.001651307102292776\n",
      "iteration 14620, loss: 0.0015694794710725546\n",
      "iteration 14621, loss: 0.0017194671090692282\n",
      "iteration 14622, loss: 0.0018930141814053059\n",
      "iteration 14623, loss: 0.0018774037016555667\n",
      "iteration 14624, loss: 0.0014345075469464064\n",
      "iteration 14625, loss: 0.0017127440078184009\n",
      "iteration 14626, loss: 0.0014561738353222609\n",
      "iteration 14627, loss: 0.0015473628882318735\n",
      "iteration 14628, loss: 0.0014855539193376899\n",
      "iteration 14629, loss: 0.0013886720407754183\n",
      "iteration 14630, loss: 0.0014912381302565336\n",
      "iteration 14631, loss: 0.0015852584037929773\n",
      "iteration 14632, loss: 0.0013682025019079447\n",
      "iteration 14633, loss: 0.0018169321119785309\n",
      "iteration 14634, loss: 0.0019053815631195903\n",
      "iteration 14635, loss: 0.0014868566067889333\n",
      "iteration 14636, loss: 0.0013691915664821863\n",
      "iteration 14637, loss: 0.0017395573668181896\n",
      "iteration 14638, loss: 0.0014764468651264906\n",
      "iteration 14639, loss: 0.0016352899838238955\n",
      "iteration 14640, loss: 0.001742518157698214\n",
      "iteration 14641, loss: 0.0016725504538044333\n",
      "iteration 14642, loss: 0.0018097169231623411\n",
      "iteration 14643, loss: 0.0015985684003680944\n",
      "iteration 14644, loss: 0.0015432062791660428\n",
      "iteration 14645, loss: 0.001291912980377674\n",
      "iteration 14646, loss: 0.0017025809502229095\n",
      "iteration 14647, loss: 0.0016718893311917782\n",
      "iteration 14648, loss: 0.0016587108839303255\n",
      "iteration 14649, loss: 0.0014763119397684932\n",
      "iteration 14650, loss: 0.0018512260867282748\n",
      "iteration 14651, loss: 0.001433875411748886\n",
      "iteration 14652, loss: 0.0018064938485622406\n",
      "iteration 14653, loss: 0.0014878899091854692\n",
      "iteration 14654, loss: 0.0016796983545646071\n",
      "iteration 14655, loss: 0.0014576079556718469\n",
      "iteration 14656, loss: 0.0014927180018275976\n",
      "iteration 14657, loss: 0.0015969270607456565\n",
      "iteration 14658, loss: 0.0018011925276368856\n",
      "iteration 14659, loss: 0.0017334792064502835\n",
      "iteration 14660, loss: 0.0019088629633188248\n",
      "iteration 14661, loss: 0.0014088625321164727\n",
      "iteration 14662, loss: 0.0016753969248384237\n",
      "iteration 14663, loss: 0.0018399497494101524\n",
      "iteration 14664, loss: 0.001829145709052682\n",
      "iteration 14665, loss: 0.0016586391720920801\n",
      "iteration 14666, loss: 0.0018983290065079927\n",
      "iteration 14667, loss: 0.0020027144346386194\n",
      "iteration 14668, loss: 0.0018465931061655283\n",
      "iteration 14669, loss: 0.0020612538792192936\n",
      "iteration 14670, loss: 0.001330839702859521\n",
      "iteration 14671, loss: 0.001441853935830295\n",
      "iteration 14672, loss: 0.0017710610991343856\n",
      "iteration 14673, loss: 0.0015614863950759172\n",
      "iteration 14674, loss: 0.00145910179708153\n",
      "iteration 14675, loss: 0.001645574695430696\n",
      "iteration 14676, loss: 0.0017030511517077684\n",
      "iteration 14677, loss: 0.0015432860236614943\n",
      "iteration 14678, loss: 0.0017696459544822574\n",
      "iteration 14679, loss: 0.0015871577197685838\n",
      "iteration 14680, loss: 0.0018973135156556964\n",
      "iteration 14681, loss: 0.0019147915299981833\n",
      "iteration 14682, loss: 0.0016838994342833757\n",
      "iteration 14683, loss: 0.0012761468533426523\n",
      "iteration 14684, loss: 0.0014565866440534592\n",
      "iteration 14685, loss: 0.0014298108872026205\n",
      "iteration 14686, loss: 0.0013825137866660953\n",
      "iteration 14687, loss: 0.0016408159863203764\n",
      "iteration 14688, loss: 0.0014146714238449931\n",
      "iteration 14689, loss: 0.0014403928071260452\n",
      "iteration 14690, loss: 0.0016386061906814575\n",
      "iteration 14691, loss: 0.0015457167755812407\n",
      "iteration 14692, loss: 0.001615369925275445\n",
      "iteration 14693, loss: 0.0013679462717846036\n",
      "iteration 14694, loss: 0.0014020936796441674\n",
      "iteration 14695, loss: 0.0015588904498144984\n",
      "iteration 14696, loss: 0.0014194461982697248\n",
      "iteration 14697, loss: 0.0016165458364412189\n",
      "iteration 14698, loss: 0.0014784191735088825\n",
      "iteration 14699, loss: 0.001254677539691329\n",
      "iteration 14700, loss: 0.0014572427608072758\n",
      "iteration 14701, loss: 0.0018087343778461218\n",
      "iteration 14702, loss: 0.001562794204801321\n",
      "iteration 14703, loss: 0.0015710304724052548\n",
      "iteration 14704, loss: 0.0013141484232619405\n",
      "iteration 14705, loss: 0.0014702561311423779\n",
      "iteration 14706, loss: 0.0014990478521212935\n",
      "iteration 14707, loss: 0.0015381437260657549\n",
      "iteration 14708, loss: 0.0013299863785505295\n",
      "iteration 14709, loss: 0.0015994294080883265\n",
      "iteration 14710, loss: 0.0013618948869407177\n",
      "iteration 14711, loss: 0.0015760338865220547\n",
      "iteration 14712, loss: 0.001816463889554143\n",
      "iteration 14713, loss: 0.0014620721340179443\n",
      "iteration 14714, loss: 0.001367564545944333\n",
      "iteration 14715, loss: 0.0017079213866963983\n",
      "iteration 14716, loss: 0.001393624348565936\n",
      "iteration 14717, loss: 0.001236849115230143\n",
      "iteration 14718, loss: 0.0015072007663547993\n",
      "iteration 14719, loss: 0.001799665275029838\n",
      "iteration 14720, loss: 0.0014437250792980194\n",
      "iteration 14721, loss: 0.001985137816518545\n",
      "iteration 14722, loss: 0.0017562059219926596\n",
      "iteration 14723, loss: 0.0015245635295286775\n",
      "iteration 14724, loss: 0.0013215377693995833\n",
      "iteration 14725, loss: 0.0015196367166936398\n",
      "iteration 14726, loss: 0.001594581175595522\n",
      "iteration 14727, loss: 0.001417308347299695\n",
      "iteration 14728, loss: 0.0021308462601155043\n",
      "iteration 14729, loss: 0.0016418511513620615\n",
      "iteration 14730, loss: 0.0013947164406999946\n",
      "iteration 14731, loss: 0.001944331917911768\n",
      "iteration 14732, loss: 0.0014083599671721458\n",
      "iteration 14733, loss: 0.0016303035663440824\n",
      "iteration 14734, loss: 0.0015606897650286555\n",
      "iteration 14735, loss: 0.0015780649846419692\n",
      "iteration 14736, loss: 0.001528632827103138\n",
      "iteration 14737, loss: 0.0014189609792083502\n",
      "iteration 14738, loss: 0.002006781753152609\n",
      "iteration 14739, loss: 0.0015784609131515026\n",
      "iteration 14740, loss: 0.0016197999939322472\n",
      "iteration 14741, loss: 0.0018585347570478916\n",
      "iteration 14742, loss: 0.001460435800254345\n",
      "iteration 14743, loss: 0.0016140115913003683\n",
      "iteration 14744, loss: 0.0014136029640212655\n",
      "iteration 14745, loss: 0.0017997215036302805\n",
      "iteration 14746, loss: 0.001466329675167799\n",
      "iteration 14747, loss: 0.0018017005641013384\n",
      "iteration 14748, loss: 0.0015396070666611195\n",
      "iteration 14749, loss: 0.0014995149103924632\n",
      "iteration 14750, loss: 0.0016048494726419449\n",
      "iteration 14751, loss: 0.0014531337656080723\n",
      "iteration 14752, loss: 0.0018158161547034979\n",
      "iteration 14753, loss: 0.0016935248859226704\n",
      "iteration 14754, loss: 0.0016808147775009274\n",
      "iteration 14755, loss: 0.0019559464417397976\n",
      "iteration 14756, loss: 0.001725316746160388\n",
      "iteration 14757, loss: 0.0018017279217019677\n",
      "iteration 14758, loss: 0.0015685745747759938\n",
      "iteration 14759, loss: 0.001439256127923727\n",
      "iteration 14760, loss: 0.0016276240348815918\n",
      "iteration 14761, loss: 0.0015329844318330288\n",
      "iteration 14762, loss: 0.001376620028167963\n",
      "iteration 14763, loss: 0.001568415085785091\n",
      "iteration 14764, loss: 0.001383377006277442\n",
      "iteration 14765, loss: 0.0016810798551887274\n",
      "iteration 14766, loss: 0.0017184237949550152\n",
      "iteration 14767, loss: 0.0014175456017255783\n",
      "iteration 14768, loss: 0.0017916273791342974\n",
      "iteration 14769, loss: 0.001796904718503356\n",
      "iteration 14770, loss: 0.001556829665787518\n",
      "iteration 14771, loss: 0.001519841724075377\n",
      "iteration 14772, loss: 0.0014944546855986118\n",
      "iteration 14773, loss: 0.0016317409463226795\n",
      "iteration 14774, loss: 0.00149696902371943\n",
      "iteration 14775, loss: 0.001992929959669709\n",
      "iteration 14776, loss: 0.0019661530386656523\n",
      "iteration 14777, loss: 0.001589252962730825\n",
      "iteration 14778, loss: 0.0014600163558498025\n",
      "iteration 14779, loss: 0.0016608965815976262\n",
      "iteration 14780, loss: 0.0016590575687587261\n",
      "iteration 14781, loss: 0.0014841258525848389\n",
      "iteration 14782, loss: 0.0015347856096923351\n",
      "iteration 14783, loss: 0.001684017712250352\n",
      "iteration 14784, loss: 0.0015296072233468294\n",
      "iteration 14785, loss: 0.0015311192255467176\n",
      "iteration 14786, loss: 0.0016349370125681162\n",
      "iteration 14787, loss: 0.0015039609279483557\n",
      "iteration 14788, loss: 0.0017944953870028257\n",
      "iteration 14789, loss: 0.0015143733471632004\n",
      "iteration 14790, loss: 0.0015384306898340583\n",
      "iteration 14791, loss: 0.0016641990514472127\n",
      "iteration 14792, loss: 0.0017342513892799616\n",
      "iteration 14793, loss: 0.0017220398876816034\n",
      "iteration 14794, loss: 0.0013757111737504601\n",
      "iteration 14795, loss: 0.0015365681611001492\n",
      "iteration 14796, loss: 0.0013113811146467924\n",
      "iteration 14797, loss: 0.0014039985835552216\n",
      "iteration 14798, loss: 0.0015174816362559795\n",
      "iteration 14799, loss: 0.0017913082847371697\n",
      "iteration 14800, loss: 0.0015553124248981476\n",
      "iteration 14801, loss: 0.0016321178991347551\n",
      "iteration 14802, loss: 0.001861141761764884\n",
      "iteration 14803, loss: 0.002031703945249319\n",
      "iteration 14804, loss: 0.0017351063434034586\n",
      "iteration 14805, loss: 0.0017605634639039636\n",
      "iteration 14806, loss: 0.0021390675101429224\n",
      "iteration 14807, loss: 0.0018433629302307963\n",
      "iteration 14808, loss: 0.0017658956348896027\n",
      "iteration 14809, loss: 0.0017226943746209145\n",
      "iteration 14810, loss: 0.0015921210870146751\n",
      "iteration 14811, loss: 0.001466142712160945\n",
      "iteration 14812, loss: 0.0016673703212291002\n",
      "iteration 14813, loss: 0.0012602320639416575\n",
      "iteration 14814, loss: 0.001387615455314517\n",
      "iteration 14815, loss: 0.00127161864656955\n",
      "iteration 14816, loss: 0.001720715663395822\n",
      "iteration 14817, loss: 0.00165953254327178\n",
      "iteration 14818, loss: 0.0016822346951812506\n",
      "iteration 14819, loss: 0.0015465368051081896\n",
      "iteration 14820, loss: 0.0016622297698631883\n",
      "iteration 14821, loss: 0.0018142028711736202\n",
      "iteration 14822, loss: 0.0016892647836357355\n",
      "iteration 14823, loss: 0.001910761813633144\n",
      "iteration 14824, loss: 0.0018424936570227146\n",
      "iteration 14825, loss: 0.0020069549791514874\n",
      "iteration 14826, loss: 0.0013647089945152402\n",
      "iteration 14827, loss: 0.0013912161812186241\n",
      "iteration 14828, loss: 0.0014837810304015875\n",
      "iteration 14829, loss: 0.001501415274105966\n",
      "iteration 14830, loss: 0.0015278385253623128\n",
      "iteration 14831, loss: 0.0017022937536239624\n",
      "iteration 14832, loss: 0.0015027213376015425\n",
      "iteration 14833, loss: 0.00161772221326828\n",
      "iteration 14834, loss: 0.0013807672075927258\n",
      "iteration 14835, loss: 0.001158106722868979\n",
      "iteration 14836, loss: 0.0015472857048735023\n",
      "iteration 14837, loss: 0.0022424496710300446\n",
      "iteration 14838, loss: 0.0013579678488895297\n",
      "iteration 14839, loss: 0.001748096663504839\n",
      "iteration 14840, loss: 0.0015330150490626693\n",
      "iteration 14841, loss: 0.0016458213794976473\n",
      "iteration 14842, loss: 0.0014281533658504486\n",
      "iteration 14843, loss: 0.0014188820496201515\n",
      "iteration 14844, loss: 0.001455698162317276\n",
      "iteration 14845, loss: 0.001835513859987259\n",
      "iteration 14846, loss: 0.0015784986317157745\n",
      "iteration 14847, loss: 0.0017321386840194464\n",
      "iteration 14848, loss: 0.0014971632044762373\n",
      "iteration 14849, loss: 0.0014657125575467944\n",
      "iteration 14850, loss: 0.0016318813432008028\n",
      "iteration 14851, loss: 0.0015101191820576787\n",
      "iteration 14852, loss: 0.0014508692547678947\n",
      "iteration 14853, loss: 0.001472468487918377\n",
      "iteration 14854, loss: 0.0013493715086951852\n",
      "iteration 14855, loss: 0.001581651857122779\n",
      "iteration 14856, loss: 0.0016521618235856295\n",
      "iteration 14857, loss: 0.0015942445024847984\n",
      "iteration 14858, loss: 0.0013802647590637207\n",
      "iteration 14859, loss: 0.0015264672692865133\n",
      "iteration 14860, loss: 0.0018001042772084475\n",
      "iteration 14861, loss: 0.0017712360713630915\n",
      "iteration 14862, loss: 0.0013760237488895655\n",
      "iteration 14863, loss: 0.0016113913152366877\n",
      "iteration 14864, loss: 0.001444124849513173\n",
      "iteration 14865, loss: 0.0016091996803879738\n",
      "iteration 14866, loss: 0.001256920164451003\n",
      "iteration 14867, loss: 0.0014119006227701902\n",
      "iteration 14868, loss: 0.0013823716435581446\n",
      "iteration 14869, loss: 0.0013401920441538095\n",
      "iteration 14870, loss: 0.001709713600575924\n",
      "iteration 14871, loss: 0.0017060404643416405\n",
      "iteration 14872, loss: 0.0013254617806524038\n",
      "iteration 14873, loss: 0.0013754555257037282\n",
      "iteration 14874, loss: 0.0014861240051686764\n",
      "iteration 14875, loss: 0.0016452256822958589\n",
      "iteration 14876, loss: 0.001581661170348525\n",
      "iteration 14877, loss: 0.001638796180486679\n",
      "iteration 14878, loss: 0.0015352766495198011\n",
      "iteration 14879, loss: 0.0014758416218683124\n",
      "iteration 14880, loss: 0.0011745456140488386\n",
      "iteration 14881, loss: 0.0015825084410607815\n",
      "iteration 14882, loss: 0.0014567363541573286\n",
      "iteration 14883, loss: 0.0014664935879409313\n",
      "iteration 14884, loss: 0.001566674909554422\n",
      "iteration 14885, loss: 0.0014766153180971742\n",
      "iteration 14886, loss: 0.0013551367446780205\n",
      "iteration 14887, loss: 0.0017730944091454148\n",
      "iteration 14888, loss: 0.0018753432668745518\n",
      "iteration 14889, loss: 0.0021497728303074837\n",
      "iteration 14890, loss: 0.0018145558424293995\n",
      "iteration 14891, loss: 0.0019804411567747593\n",
      "iteration 14892, loss: 0.0014363666996359825\n",
      "iteration 14893, loss: 0.0016199148958548903\n",
      "iteration 14894, loss: 0.0017077759839594364\n",
      "iteration 14895, loss: 0.0014445835258811712\n",
      "iteration 14896, loss: 0.001795127522200346\n",
      "iteration 14897, loss: 0.0013226538430899382\n",
      "iteration 14898, loss: 0.0015230851713567972\n",
      "iteration 14899, loss: 0.0019815629348158836\n",
      "iteration 14900, loss: 0.0016500158235430717\n",
      "iteration 14901, loss: 0.0012102797627449036\n",
      "iteration 14902, loss: 0.0018343008123338223\n",
      "iteration 14903, loss: 0.001446331269107759\n",
      "iteration 14904, loss: 0.0016896768938750029\n",
      "iteration 14905, loss: 0.0015670652501285076\n",
      "iteration 14906, loss: 0.0014821975491940975\n",
      "iteration 14907, loss: 0.0016495923046022654\n",
      "iteration 14908, loss: 0.002045962493866682\n",
      "iteration 14909, loss: 0.0014284319477155805\n",
      "iteration 14910, loss: 0.0014248609077185392\n",
      "iteration 14911, loss: 0.001665210584178567\n",
      "iteration 14912, loss: 0.001491936738602817\n",
      "iteration 14913, loss: 0.001712182885967195\n",
      "iteration 14914, loss: 0.00154304807074368\n",
      "iteration 14915, loss: 0.0013403735356405377\n",
      "iteration 14916, loss: 0.0016934206942096353\n",
      "iteration 14917, loss: 0.0015136438887566328\n",
      "iteration 14918, loss: 0.0016103631351143122\n",
      "iteration 14919, loss: 0.001720705535262823\n",
      "iteration 14920, loss: 0.0016243349527940154\n",
      "iteration 14921, loss: 0.0013540813233703375\n",
      "iteration 14922, loss: 0.001847561914473772\n",
      "iteration 14923, loss: 0.001462153042666614\n",
      "iteration 14924, loss: 0.0015793648781254888\n",
      "iteration 14925, loss: 0.0015166782541200519\n",
      "iteration 14926, loss: 0.0015587874222546816\n",
      "iteration 14927, loss: 0.0017348692053928971\n",
      "iteration 14928, loss: 0.0019283897709101439\n",
      "iteration 14929, loss: 0.001619757036678493\n",
      "iteration 14930, loss: 0.0017793870065361261\n",
      "iteration 14931, loss: 0.0014957892708480358\n",
      "iteration 14932, loss: 0.001621685572899878\n",
      "iteration 14933, loss: 0.0016985500697046518\n",
      "iteration 14934, loss: 0.001506879460066557\n",
      "iteration 14935, loss: 0.0016175147611647844\n",
      "iteration 14936, loss: 0.001513335737399757\n",
      "iteration 14937, loss: 0.0015390974003821611\n",
      "iteration 14938, loss: 0.0016398105071857572\n",
      "iteration 14939, loss: 0.0016168049769476056\n",
      "iteration 14940, loss: 0.0016229827888309956\n",
      "iteration 14941, loss: 0.001394380466081202\n",
      "iteration 14942, loss: 0.0011813212186098099\n",
      "iteration 14943, loss: 0.001672882353886962\n",
      "iteration 14944, loss: 0.0012110054958611727\n",
      "iteration 14945, loss: 0.001528942841105163\n",
      "iteration 14946, loss: 0.0015810863114893436\n",
      "iteration 14947, loss: 0.001570429652929306\n",
      "iteration 14948, loss: 0.0016688843024894595\n",
      "iteration 14949, loss: 0.001758068916387856\n",
      "iteration 14950, loss: 0.00161523069255054\n",
      "iteration 14951, loss: 0.0013174384366720915\n",
      "iteration 14952, loss: 0.0019234546925872564\n",
      "iteration 14953, loss: 0.001560203731060028\n",
      "iteration 14954, loss: 0.0017930397298187017\n",
      "iteration 14955, loss: 0.0019329339265823364\n",
      "iteration 14956, loss: 0.0015759202651679516\n",
      "iteration 14957, loss: 0.0018249310087412596\n",
      "iteration 14958, loss: 0.0015655489405617118\n",
      "iteration 14959, loss: 0.0016049542464315891\n",
      "iteration 14960, loss: 0.0015773733612149954\n",
      "iteration 14961, loss: 0.0013441084884107113\n",
      "iteration 14962, loss: 0.0013660741969943047\n",
      "iteration 14963, loss: 0.0015013585798442364\n",
      "iteration 14964, loss: 0.001662000548094511\n",
      "iteration 14965, loss: 0.001586748519912362\n",
      "iteration 14966, loss: 0.0018554091220721602\n",
      "iteration 14967, loss: 0.0015671723522245884\n",
      "iteration 14968, loss: 0.0014314509462565184\n",
      "iteration 14969, loss: 0.0013758230488747358\n",
      "iteration 14970, loss: 0.0014039495727047324\n",
      "iteration 14971, loss: 0.0022456629667431116\n",
      "iteration 14972, loss: 0.0014985392335802317\n",
      "iteration 14973, loss: 0.0012917021522298455\n",
      "iteration 14974, loss: 0.0016532973386347294\n",
      "iteration 14975, loss: 0.0012811899650841951\n",
      "iteration 14976, loss: 0.0016097989864647388\n",
      "iteration 14977, loss: 0.0012552133994176984\n",
      "iteration 14978, loss: 0.0013442090712487698\n",
      "iteration 14979, loss: 0.0015678309137001634\n",
      "iteration 14980, loss: 0.0015046456828713417\n",
      "iteration 14981, loss: 0.0015883146552368999\n",
      "iteration 14982, loss: 0.0014385322574526072\n",
      "iteration 14983, loss: 0.0014001366216689348\n",
      "iteration 14984, loss: 0.0015881668077781796\n",
      "iteration 14985, loss: 0.0014796963660046458\n",
      "iteration 14986, loss: 0.0011437975335866213\n",
      "iteration 14987, loss: 0.0015311851166188717\n",
      "iteration 14988, loss: 0.0015212214784696698\n",
      "iteration 14989, loss: 0.0012868019985035062\n",
      "iteration 14990, loss: 0.0013933397131040692\n",
      "iteration 14991, loss: 0.0012125445064157248\n",
      "iteration 14992, loss: 0.0013358313590288162\n",
      "iteration 14993, loss: 0.0017793490551412106\n",
      "iteration 14994, loss: 0.0015638836193829775\n",
      "iteration 14995, loss: 0.0013669333420693874\n",
      "iteration 14996, loss: 0.001676263753324747\n",
      "iteration 14997, loss: 0.001321654417552054\n",
      "iteration 14998, loss: 0.0016280196141451597\n",
      "iteration 14999, loss: 0.0014559748815372586\n",
      "iteration 15000, loss: 0.0013346716295927763\n",
      "iteration 15001, loss: 0.0013102866942062974\n",
      "iteration 15002, loss: 0.0015539935557171702\n",
      "iteration 15003, loss: 0.0013653116766363382\n",
      "iteration 15004, loss: 0.0018818886019289494\n",
      "iteration 15005, loss: 0.001437226077541709\n",
      "iteration 15006, loss: 0.0014355501625686884\n",
      "iteration 15007, loss: 0.0017977601382881403\n",
      "iteration 15008, loss: 0.0015016745310276747\n",
      "iteration 15009, loss: 0.0017060928512364626\n",
      "iteration 15010, loss: 0.0016591615276411176\n",
      "iteration 15011, loss: 0.001204553060233593\n",
      "iteration 15012, loss: 0.001604791497811675\n",
      "iteration 15013, loss: 0.00135601416695863\n",
      "iteration 15014, loss: 0.0014888488221913576\n",
      "iteration 15015, loss: 0.0014390636933967471\n",
      "iteration 15016, loss: 0.0016155452467501163\n",
      "iteration 15017, loss: 0.0016004093922674656\n",
      "iteration 15018, loss: 0.00154216552618891\n",
      "iteration 15019, loss: 0.001474839518778026\n",
      "iteration 15020, loss: 0.0015650315908715129\n",
      "iteration 15021, loss: 0.001795521005988121\n",
      "iteration 15022, loss: 0.0013898657634854317\n",
      "iteration 15023, loss: 0.0013049289118498564\n",
      "iteration 15024, loss: 0.0016916377935558558\n",
      "iteration 15025, loss: 0.0014127432368695736\n",
      "iteration 15026, loss: 0.0017903604311868548\n",
      "iteration 15027, loss: 0.0014343508519232273\n",
      "iteration 15028, loss: 0.0017886017449200153\n",
      "iteration 15029, loss: 0.0013884732034057379\n",
      "iteration 15030, loss: 0.001723590074107051\n",
      "iteration 15031, loss: 0.001953392755240202\n",
      "iteration 15032, loss: 0.0013666229788213968\n",
      "iteration 15033, loss: 0.001372650032863021\n",
      "iteration 15034, loss: 0.0014546066522598267\n",
      "iteration 15035, loss: 0.0013557396596297622\n",
      "iteration 15036, loss: 0.0015771646285429597\n",
      "iteration 15037, loss: 0.0017653405666351318\n",
      "iteration 15038, loss: 0.0015305152628570795\n",
      "iteration 15039, loss: 0.0015818588435649872\n",
      "iteration 15040, loss: 0.0013838582672178745\n",
      "iteration 15041, loss: 0.0015704546822234988\n",
      "iteration 15042, loss: 0.0013974580215290189\n",
      "iteration 15043, loss: 0.001352212275378406\n",
      "iteration 15044, loss: 0.0011502087581902742\n",
      "iteration 15045, loss: 0.0014164034510031343\n",
      "iteration 15046, loss: 0.0013554409379139543\n",
      "iteration 15047, loss: 0.001665914081968367\n",
      "iteration 15048, loss: 0.0013637840747833252\n",
      "iteration 15049, loss: 0.0014174389652907848\n",
      "iteration 15050, loss: 0.00205986388027668\n",
      "iteration 15051, loss: 0.001987203024327755\n",
      "iteration 15052, loss: 0.0015518219443038106\n",
      "iteration 15053, loss: 0.0016266864258795977\n",
      "iteration 15054, loss: 0.0017102565616369247\n",
      "iteration 15055, loss: 0.0013600311940535903\n",
      "iteration 15056, loss: 0.0019816849380731583\n",
      "iteration 15057, loss: 0.001821134821511805\n",
      "iteration 15058, loss: 0.001438297564163804\n",
      "iteration 15059, loss: 0.0016488311812281609\n",
      "iteration 15060, loss: 0.0017268594820052385\n",
      "iteration 15061, loss: 0.0017235905397683382\n",
      "iteration 15062, loss: 0.0015265829861164093\n",
      "iteration 15063, loss: 0.0016321020666509867\n",
      "iteration 15064, loss: 0.0014964130241423845\n",
      "iteration 15065, loss: 0.0016557929338887334\n",
      "iteration 15066, loss: 0.0011382449883967638\n",
      "iteration 15067, loss: 0.0014677508734166622\n",
      "iteration 15068, loss: 0.0018347385339438915\n",
      "iteration 15069, loss: 0.0015944500919431448\n",
      "iteration 15070, loss: 0.0012336110230535269\n",
      "iteration 15071, loss: 0.0015289457514882088\n",
      "iteration 15072, loss: 0.0016045502852648497\n",
      "iteration 15073, loss: 0.0014854086330160499\n",
      "iteration 15074, loss: 0.0012497000861912966\n",
      "iteration 15075, loss: 0.0016986578702926636\n",
      "iteration 15076, loss: 0.0017710566753521562\n",
      "iteration 15077, loss: 0.0013991984305903316\n",
      "iteration 15078, loss: 0.0013696711976081133\n",
      "iteration 15079, loss: 0.0013734926469624043\n",
      "iteration 15080, loss: 0.0015491076046600938\n",
      "iteration 15081, loss: 0.001394730294123292\n",
      "iteration 15082, loss: 0.001679725362919271\n",
      "iteration 15083, loss: 0.0011639241129159927\n",
      "iteration 15084, loss: 0.0014516018563881516\n",
      "iteration 15085, loss: 0.0014837234048172832\n",
      "iteration 15086, loss: 0.0013497418258339167\n",
      "iteration 15087, loss: 0.0016629262827336788\n",
      "iteration 15088, loss: 0.001408831449225545\n",
      "iteration 15089, loss: 0.0014779907651245594\n",
      "iteration 15090, loss: 0.0013669593026861548\n",
      "iteration 15091, loss: 0.0013469045516103506\n",
      "iteration 15092, loss: 0.0013339261058717966\n",
      "iteration 15093, loss: 0.001256968593224883\n",
      "iteration 15094, loss: 0.0015284758992493153\n",
      "iteration 15095, loss: 0.0013578195357695222\n",
      "iteration 15096, loss: 0.0012230277061462402\n",
      "iteration 15097, loss: 0.0015692940214648843\n",
      "iteration 15098, loss: 0.0016436438309028745\n",
      "iteration 15099, loss: 0.001289872918277979\n",
      "iteration 15100, loss: 0.0013257402461022139\n",
      "iteration 15101, loss: 0.0012881956063210964\n",
      "iteration 15102, loss: 0.0014974904479458928\n",
      "iteration 15103, loss: 0.0013193846680223942\n",
      "iteration 15104, loss: 0.0015826656017452478\n",
      "iteration 15105, loss: 0.0012795127695426345\n",
      "iteration 15106, loss: 0.0015355912037193775\n",
      "iteration 15107, loss: 0.0014333348954096437\n",
      "iteration 15108, loss: 0.001409216783940792\n",
      "iteration 15109, loss: 0.0014432216994464397\n",
      "iteration 15110, loss: 0.0012002205476164818\n",
      "iteration 15111, loss: 0.001691213808953762\n",
      "iteration 15112, loss: 0.0015915597323328257\n",
      "iteration 15113, loss: 0.0012564740609377623\n",
      "iteration 15114, loss: 0.001304087694734335\n",
      "iteration 15115, loss: 0.001367905642837286\n",
      "iteration 15116, loss: 0.0015208553522825241\n",
      "iteration 15117, loss: 0.0012500365264713764\n",
      "iteration 15118, loss: 0.001479366677813232\n",
      "iteration 15119, loss: 0.0014379911590367556\n",
      "iteration 15120, loss: 0.001370699843391776\n",
      "iteration 15121, loss: 0.0015651907306164503\n",
      "iteration 15122, loss: 0.001422221539542079\n",
      "iteration 15123, loss: 0.0014019932132214308\n",
      "iteration 15124, loss: 0.0012830269988626242\n",
      "iteration 15125, loss: 0.0013436262961477041\n",
      "iteration 15126, loss: 0.0013750921934843063\n",
      "iteration 15127, loss: 0.0015587599482387304\n",
      "iteration 15128, loss: 0.0014305933145806193\n",
      "iteration 15129, loss: 0.0015121244359761477\n",
      "iteration 15130, loss: 0.00141140294726938\n",
      "iteration 15131, loss: 0.0016071238787844777\n",
      "iteration 15132, loss: 0.0015236525796353817\n",
      "iteration 15133, loss: 0.0012809881009161472\n",
      "iteration 15134, loss: 0.0013788030482828617\n",
      "iteration 15135, loss: 0.0016612708568572998\n",
      "iteration 15136, loss: 0.001216335454955697\n",
      "iteration 15137, loss: 0.0017811211291700602\n",
      "iteration 15138, loss: 0.0012889057397842407\n",
      "iteration 15139, loss: 0.0014801069628447294\n",
      "iteration 15140, loss: 0.001321318093687296\n",
      "iteration 15141, loss: 0.0016609443118795753\n",
      "iteration 15142, loss: 0.0015287029091268778\n",
      "iteration 15143, loss: 0.0020002764649689198\n",
      "iteration 15144, loss: 0.0018269496504217386\n",
      "iteration 15145, loss: 0.001483138999901712\n",
      "iteration 15146, loss: 0.0015282323583960533\n",
      "iteration 15147, loss: 0.0012650517746806145\n",
      "iteration 15148, loss: 0.0017667401116341352\n",
      "iteration 15149, loss: 0.0015402985736727715\n",
      "iteration 15150, loss: 0.0016212030313909054\n",
      "iteration 15151, loss: 0.0015588465612381697\n",
      "iteration 15152, loss: 0.0016254197107627988\n",
      "iteration 15153, loss: 0.0014957734383642673\n",
      "iteration 15154, loss: 0.0014156355755403638\n",
      "iteration 15155, loss: 0.0013676818925887346\n",
      "iteration 15156, loss: 0.0015893615782260895\n",
      "iteration 15157, loss: 0.0014793843729421496\n",
      "iteration 15158, loss: 0.0016456376761198044\n",
      "iteration 15159, loss: 0.001422362169250846\n",
      "iteration 15160, loss: 0.0013448712415993214\n",
      "iteration 15161, loss: 0.001698772655799985\n",
      "iteration 15162, loss: 0.0012376006925478578\n",
      "iteration 15163, loss: 0.0013426471268758178\n",
      "iteration 15164, loss: 0.0014475169591605663\n",
      "iteration 15165, loss: 0.0013119233772158623\n",
      "iteration 15166, loss: 0.00150608504191041\n",
      "iteration 15167, loss: 0.001885830657556653\n",
      "iteration 15168, loss: 0.0014862781390547752\n",
      "iteration 15169, loss: 0.0012024944880977273\n",
      "iteration 15170, loss: 0.0015470597427338362\n",
      "iteration 15171, loss: 0.0016174614429473877\n",
      "iteration 15172, loss: 0.0012977785663679242\n",
      "iteration 15173, loss: 0.001331601059064269\n",
      "iteration 15174, loss: 0.0012453505769371986\n",
      "iteration 15175, loss: 0.0016550268046557903\n",
      "iteration 15176, loss: 0.0012798686511814594\n",
      "iteration 15177, loss: 0.0011988249607384205\n",
      "iteration 15178, loss: 0.001573188230395317\n",
      "iteration 15179, loss: 0.0014299624599516392\n",
      "iteration 15180, loss: 0.0016930520068854094\n",
      "iteration 15181, loss: 0.001411672797985375\n",
      "iteration 15182, loss: 0.001514302915893495\n",
      "iteration 15183, loss: 0.0017056944780051708\n",
      "iteration 15184, loss: 0.0015850942581892014\n",
      "iteration 15185, loss: 0.0014225628692656755\n",
      "iteration 15186, loss: 0.001547964639030397\n",
      "iteration 15187, loss: 0.0014560730196535587\n",
      "iteration 15188, loss: 0.0014058402739465237\n",
      "iteration 15189, loss: 0.0013903016224503517\n",
      "iteration 15190, loss: 0.002035662531852722\n",
      "iteration 15191, loss: 0.0015450522769242525\n",
      "iteration 15192, loss: 0.0014938642270863056\n",
      "iteration 15193, loss: 0.0017910996684804559\n",
      "iteration 15194, loss: 0.0013439881149679422\n",
      "iteration 15195, loss: 0.0013700665440410376\n",
      "iteration 15196, loss: 0.0011526655871421099\n",
      "iteration 15197, loss: 0.0014662696048617363\n",
      "iteration 15198, loss: 0.0016571711748838425\n",
      "iteration 15199, loss: 0.0018738765502348542\n",
      "iteration 15200, loss: 0.001764838001690805\n",
      "iteration 15201, loss: 0.0012750038877129555\n",
      "iteration 15202, loss: 0.0017321568448096514\n",
      "iteration 15203, loss: 0.001689733355306089\n",
      "iteration 15204, loss: 0.0013909849803894758\n",
      "iteration 15205, loss: 0.0018483847379684448\n",
      "iteration 15206, loss: 0.0014748602407053113\n",
      "iteration 15207, loss: 0.0015664915554225445\n",
      "iteration 15208, loss: 0.0018299393123015761\n",
      "iteration 15209, loss: 0.001340928371064365\n",
      "iteration 15210, loss: 0.0013257444370537996\n",
      "iteration 15211, loss: 0.0018870634958148003\n",
      "iteration 15212, loss: 0.0015913702081888914\n",
      "iteration 15213, loss: 0.0015583304921165109\n",
      "iteration 15214, loss: 0.001405044342391193\n",
      "iteration 15215, loss: 0.0018416251987218857\n",
      "iteration 15216, loss: 0.0015604258514940739\n",
      "iteration 15217, loss: 0.0016696269158273935\n",
      "iteration 15218, loss: 0.0018973244586959481\n",
      "iteration 15219, loss: 0.0019226104486733675\n",
      "iteration 15220, loss: 0.0015061565209180117\n",
      "iteration 15221, loss: 0.0016285302117466927\n",
      "iteration 15222, loss: 0.001523036160506308\n",
      "iteration 15223, loss: 0.0015288906870409846\n",
      "iteration 15224, loss: 0.0018914137035608292\n",
      "iteration 15225, loss: 0.0015135591384023428\n",
      "iteration 15226, loss: 0.0014855691697448492\n",
      "iteration 15227, loss: 0.0018746800487861037\n",
      "iteration 15228, loss: 0.001561719924211502\n",
      "iteration 15229, loss: 0.0017219414003193378\n",
      "iteration 15230, loss: 0.0013717461843043566\n",
      "iteration 15231, loss: 0.0017295659054070711\n",
      "iteration 15232, loss: 0.0017176565015688539\n",
      "iteration 15233, loss: 0.0015300940722227097\n",
      "iteration 15234, loss: 0.0014591189101338387\n",
      "iteration 15235, loss: 0.0018276347545906901\n",
      "iteration 15236, loss: 0.0013473120052367449\n",
      "iteration 15237, loss: 0.0020427864510565996\n",
      "iteration 15238, loss: 0.0014746654778718948\n",
      "iteration 15239, loss: 0.0018232805887237191\n",
      "iteration 15240, loss: 0.0018120311433449388\n",
      "iteration 15241, loss: 0.0017801423091441393\n",
      "iteration 15242, loss: 0.0013165399432182312\n",
      "iteration 15243, loss: 0.0017729545943439007\n",
      "iteration 15244, loss: 0.0014214920811355114\n",
      "iteration 15245, loss: 0.0016420013271272182\n",
      "iteration 15246, loss: 0.001166540547274053\n",
      "iteration 15247, loss: 0.0013903143117204309\n",
      "iteration 15248, loss: 0.0016379609005525708\n",
      "iteration 15249, loss: 0.0016702525317668915\n",
      "iteration 15250, loss: 0.001615792396478355\n",
      "iteration 15251, loss: 0.0012781477998942137\n",
      "iteration 15252, loss: 0.0016525364480912685\n",
      "iteration 15253, loss: 0.0014735352015122771\n",
      "iteration 15254, loss: 0.0016222046688199043\n",
      "iteration 15255, loss: 0.001534180250018835\n",
      "iteration 15256, loss: 0.0016098394989967346\n",
      "iteration 15257, loss: 0.0016581453382968903\n",
      "iteration 15258, loss: 0.0018387341406196356\n",
      "iteration 15259, loss: 0.0016029372345656157\n",
      "iteration 15260, loss: 0.001559551921673119\n",
      "iteration 15261, loss: 0.001552270958200097\n",
      "iteration 15262, loss: 0.0018580425530672073\n",
      "iteration 15263, loss: 0.0014989492483437061\n",
      "iteration 15264, loss: 0.0014844785910099745\n",
      "iteration 15265, loss: 0.001832808367908001\n",
      "iteration 15266, loss: 0.0011304280487820506\n",
      "iteration 15267, loss: 0.0016758725978434086\n",
      "iteration 15268, loss: 0.0014849151484668255\n",
      "iteration 15269, loss: 0.0015661853831261396\n",
      "iteration 15270, loss: 0.0015418309485539794\n",
      "iteration 15271, loss: 0.001441617263481021\n",
      "iteration 15272, loss: 0.0015512355603277683\n",
      "iteration 15273, loss: 0.00171937164850533\n",
      "iteration 15274, loss: 0.0013023905921727419\n",
      "iteration 15275, loss: 0.0015615870943292975\n",
      "iteration 15276, loss: 0.0016285276506096125\n",
      "iteration 15277, loss: 0.0014423173852264881\n",
      "iteration 15278, loss: 0.001510207075625658\n",
      "iteration 15279, loss: 0.001286227721720934\n",
      "iteration 15280, loss: 0.0012941722525283694\n",
      "iteration 15281, loss: 0.0016021456103771925\n",
      "iteration 15282, loss: 0.001215165015310049\n",
      "iteration 15283, loss: 0.0014956316445022821\n",
      "iteration 15284, loss: 0.0016202533151954412\n",
      "iteration 15285, loss: 0.0013528763083741069\n",
      "iteration 15286, loss: 0.0012673810124397278\n",
      "iteration 15287, loss: 0.0017567878821864724\n",
      "iteration 15288, loss: 0.0016425990033894777\n",
      "iteration 15289, loss: 0.0013271118514239788\n",
      "iteration 15290, loss: 0.0011487617157399654\n",
      "iteration 15291, loss: 0.001585907069966197\n",
      "iteration 15292, loss: 0.001279630116187036\n",
      "iteration 15293, loss: 0.0013997901696711779\n",
      "iteration 15294, loss: 0.0015063926111906767\n",
      "iteration 15295, loss: 0.0015314742922782898\n",
      "iteration 15296, loss: 0.0012960113817825913\n",
      "iteration 15297, loss: 0.0017560375854372978\n",
      "iteration 15298, loss: 0.0013748349156230688\n",
      "iteration 15299, loss: 0.001450729789212346\n",
      "iteration 15300, loss: 0.0012660883367061615\n",
      "iteration 15301, loss: 0.001749844173900783\n",
      "iteration 15302, loss: 0.0013912739232182503\n",
      "iteration 15303, loss: 0.0013256561942398548\n",
      "iteration 15304, loss: 0.0014428747817873955\n",
      "iteration 15305, loss: 0.0015977201983332634\n",
      "iteration 15306, loss: 0.0016620104433968663\n",
      "iteration 15307, loss: 0.001504444284364581\n",
      "iteration 15308, loss: 0.001521959202364087\n",
      "iteration 15309, loss: 0.001413164078257978\n",
      "iteration 15310, loss: 0.0014783914666622877\n",
      "iteration 15311, loss: 0.001325978897511959\n",
      "iteration 15312, loss: 0.0013174787163734436\n",
      "iteration 15313, loss: 0.0014274176210165024\n",
      "iteration 15314, loss: 0.0015886570326983929\n",
      "iteration 15315, loss: 0.0013186086434870958\n",
      "iteration 15316, loss: 0.0015454002423211932\n",
      "iteration 15317, loss: 0.0013703523436561227\n",
      "iteration 15318, loss: 0.001528444467112422\n",
      "iteration 15319, loss: 0.0013622097903862596\n",
      "iteration 15320, loss: 0.0013484368100762367\n",
      "iteration 15321, loss: 0.0014701515901833773\n",
      "iteration 15322, loss: 0.0016743664164096117\n",
      "iteration 15323, loss: 0.0013775653205811977\n",
      "iteration 15324, loss: 0.0015518044820055366\n",
      "iteration 15325, loss: 0.0016511189751327038\n",
      "iteration 15326, loss: 0.001466784393414855\n",
      "iteration 15327, loss: 0.0015225051902234554\n",
      "iteration 15328, loss: 0.0019513036822900176\n",
      "iteration 15329, loss: 0.0012148369569331408\n",
      "iteration 15330, loss: 0.0014114041114225984\n",
      "iteration 15331, loss: 0.0016189534217119217\n",
      "iteration 15332, loss: 0.0017372225411236286\n",
      "iteration 15333, loss: 0.0015148206148296595\n",
      "iteration 15334, loss: 0.0016848738305270672\n",
      "iteration 15335, loss: 0.0014688674127683043\n",
      "iteration 15336, loss: 0.001270423410460353\n",
      "iteration 15337, loss: 0.0016557739581912756\n",
      "iteration 15338, loss: 0.0016529217828065157\n",
      "iteration 15339, loss: 0.0014942861162126064\n",
      "iteration 15340, loss: 0.0014874502085149288\n",
      "iteration 15341, loss: 0.0014682544860988855\n",
      "iteration 15342, loss: 0.0016829632222652435\n",
      "iteration 15343, loss: 0.0015031632501631975\n",
      "iteration 15344, loss: 0.0013079862110316753\n",
      "iteration 15345, loss: 0.0013001908082515001\n",
      "iteration 15346, loss: 0.0013298643752932549\n",
      "iteration 15347, loss: 0.0015975884161889553\n",
      "iteration 15348, loss: 0.0018757181242108345\n",
      "iteration 15349, loss: 0.0013451578561216593\n",
      "iteration 15350, loss: 0.0019316081888973713\n",
      "iteration 15351, loss: 0.0014306817902252078\n",
      "iteration 15352, loss: 0.0015994594432413578\n",
      "iteration 15353, loss: 0.0017638364806771278\n",
      "iteration 15354, loss: 0.0016234327340498567\n",
      "iteration 15355, loss: 0.00143884913995862\n",
      "iteration 15356, loss: 0.0013589973095804453\n",
      "iteration 15357, loss: 0.001690294360741973\n",
      "iteration 15358, loss: 0.0015206225216388702\n",
      "iteration 15359, loss: 0.0012978415470570326\n",
      "iteration 15360, loss: 0.0017121366690844297\n",
      "iteration 15361, loss: 0.0015225261449813843\n",
      "iteration 15362, loss: 0.0011778795160353184\n",
      "iteration 15363, loss: 0.0018690197030082345\n",
      "iteration 15364, loss: 0.0011368616251274943\n",
      "iteration 15365, loss: 0.0013488500844687223\n",
      "iteration 15366, loss: 0.001597166177816689\n",
      "iteration 15367, loss: 0.0014796294271945953\n",
      "iteration 15368, loss: 0.001448169001378119\n",
      "iteration 15369, loss: 0.0010941778309643269\n",
      "iteration 15370, loss: 0.0013857516460120678\n",
      "iteration 15371, loss: 0.001205945503897965\n",
      "iteration 15372, loss: 0.0014637460699304938\n",
      "iteration 15373, loss: 0.0015898067504167557\n",
      "iteration 15374, loss: 0.0015395977534353733\n",
      "iteration 15375, loss: 0.0015186299569904804\n",
      "iteration 15376, loss: 0.001254906179383397\n",
      "iteration 15377, loss: 0.0018119768938049674\n",
      "iteration 15378, loss: 0.0018411247292533517\n",
      "iteration 15379, loss: 0.0016072201542556286\n",
      "iteration 15380, loss: 0.0014051453908905387\n",
      "iteration 15381, loss: 0.0016347533091902733\n",
      "iteration 15382, loss: 0.0014961586566641927\n",
      "iteration 15383, loss: 0.0012893339153379202\n",
      "iteration 15384, loss: 0.0017641413724049926\n",
      "iteration 15385, loss: 0.0012376941740512848\n",
      "iteration 15386, loss: 0.001672846730798483\n",
      "iteration 15387, loss: 0.001327618258073926\n",
      "iteration 15388, loss: 0.0014000991359353065\n",
      "iteration 15389, loss: 0.0014602812007069588\n",
      "iteration 15390, loss: 0.001697496511042118\n",
      "iteration 15391, loss: 0.001465627457946539\n",
      "iteration 15392, loss: 0.001333150896243751\n",
      "iteration 15393, loss: 0.0014165920438244939\n",
      "iteration 15394, loss: 0.001366663258522749\n",
      "iteration 15395, loss: 0.001167386886663735\n",
      "iteration 15396, loss: 0.0016393540427088737\n",
      "iteration 15397, loss: 0.0015903284074738622\n",
      "iteration 15398, loss: 0.0015828146133571863\n",
      "iteration 15399, loss: 0.001635512919165194\n",
      "iteration 15400, loss: 0.0014204711187630892\n",
      "iteration 15401, loss: 0.001336891669780016\n",
      "iteration 15402, loss: 0.0014088575262576342\n",
      "iteration 15403, loss: 0.0012488841312006116\n",
      "iteration 15404, loss: 0.0014217878924682736\n",
      "iteration 15405, loss: 0.0016506293322890997\n",
      "iteration 15406, loss: 0.001620881026610732\n",
      "iteration 15407, loss: 0.0015394832007586956\n",
      "iteration 15408, loss: 0.0017678453586995602\n",
      "iteration 15409, loss: 0.0012573293643072248\n",
      "iteration 15410, loss: 0.0012622240465134382\n",
      "iteration 15411, loss: 0.001498597557656467\n",
      "iteration 15412, loss: 0.0014699231833219528\n",
      "iteration 15413, loss: 0.001271628076210618\n",
      "iteration 15414, loss: 0.0017007035203278065\n",
      "iteration 15415, loss: 0.0013641421683132648\n",
      "iteration 15416, loss: 0.0014332743594422936\n",
      "iteration 15417, loss: 0.0016048888210207224\n",
      "iteration 15418, loss: 0.001422487199306488\n",
      "iteration 15419, loss: 0.0012448510387912393\n",
      "iteration 15420, loss: 0.0013945975806564093\n",
      "iteration 15421, loss: 0.001386892981827259\n",
      "iteration 15422, loss: 0.0016796857817098498\n",
      "iteration 15423, loss: 0.0013026939705014229\n",
      "iteration 15424, loss: 0.001283227582462132\n",
      "iteration 15425, loss: 0.0013013561256229877\n",
      "iteration 15426, loss: 0.0013400462921708822\n",
      "iteration 15427, loss: 0.0012149624526500702\n",
      "iteration 15428, loss: 0.0014274559216573834\n",
      "iteration 15429, loss: 0.0012843651929870248\n",
      "iteration 15430, loss: 0.0012213827576488256\n",
      "iteration 15431, loss: 0.0013767960481345654\n",
      "iteration 15432, loss: 0.0014704857021570206\n",
      "iteration 15433, loss: 0.0012931973906233907\n",
      "iteration 15434, loss: 0.0017190861981362104\n",
      "iteration 15435, loss: 0.0013270650524646044\n",
      "iteration 15436, loss: 0.0012878293637186289\n",
      "iteration 15437, loss: 0.0011892110342159867\n",
      "iteration 15438, loss: 0.0014876557979732752\n",
      "iteration 15439, loss: 0.0015056031988933682\n",
      "iteration 15440, loss: 0.0014933892525732517\n",
      "iteration 15441, loss: 0.0015428578481078148\n",
      "iteration 15442, loss: 0.0013055371819064021\n",
      "iteration 15443, loss: 0.0015083907637745142\n",
      "iteration 15444, loss: 0.0013388178776949644\n",
      "iteration 15445, loss: 0.0015426934696733952\n",
      "iteration 15446, loss: 0.0016448947135359049\n",
      "iteration 15447, loss: 0.001317505957558751\n",
      "iteration 15448, loss: 0.0013830732787027955\n",
      "iteration 15449, loss: 0.0016394777921959758\n",
      "iteration 15450, loss: 0.001662885071709752\n",
      "iteration 15451, loss: 0.001970999874174595\n",
      "iteration 15452, loss: 0.001506591448560357\n",
      "iteration 15453, loss: 0.0015987497754395008\n",
      "iteration 15454, loss: 0.0013775378465652466\n",
      "iteration 15455, loss: 0.0014193777460604906\n",
      "iteration 15456, loss: 0.0013965079560875893\n",
      "iteration 15457, loss: 0.0018017417751252651\n",
      "iteration 15458, loss: 0.0017693426925688982\n",
      "iteration 15459, loss: 0.0017016495112329721\n",
      "iteration 15460, loss: 0.0013527685077860951\n",
      "iteration 15461, loss: 0.0014845570549368858\n",
      "iteration 15462, loss: 0.0015252747107297182\n",
      "iteration 15463, loss: 0.0013593357289209962\n",
      "iteration 15464, loss: 0.0014447966823354363\n",
      "iteration 15465, loss: 0.0014896274078637362\n",
      "iteration 15466, loss: 0.001559064257889986\n",
      "iteration 15467, loss: 0.001622521784156561\n",
      "iteration 15468, loss: 0.0015268009155988693\n",
      "iteration 15469, loss: 0.0013393138069659472\n",
      "iteration 15470, loss: 0.0016325151082128286\n",
      "iteration 15471, loss: 0.0013934059534221888\n",
      "iteration 15472, loss: 0.0015501088928431273\n",
      "iteration 15473, loss: 0.0014010088052600622\n",
      "iteration 15474, loss: 0.001342572970315814\n",
      "iteration 15475, loss: 0.00198095734231174\n",
      "iteration 15476, loss: 0.0014800946228206158\n",
      "iteration 15477, loss: 0.001898172777146101\n",
      "iteration 15478, loss: 0.0014807423576712608\n",
      "iteration 15479, loss: 0.0014938447857275605\n",
      "iteration 15480, loss: 0.0015034291427582502\n",
      "iteration 15481, loss: 0.0016510983696207404\n",
      "iteration 15482, loss: 0.001406168914400041\n",
      "iteration 15483, loss: 0.0016092128353193402\n",
      "iteration 15484, loss: 0.0014244619524106383\n",
      "iteration 15485, loss: 0.0012030979851260781\n",
      "iteration 15486, loss: 0.0015953922411426902\n",
      "iteration 15487, loss: 0.0014574994565919042\n",
      "iteration 15488, loss: 0.001439884421415627\n",
      "iteration 15489, loss: 0.0016959344502538443\n",
      "iteration 15490, loss: 0.001601634779945016\n",
      "iteration 15491, loss: 0.001312711974605918\n",
      "iteration 15492, loss: 0.0013704490847885609\n",
      "iteration 15493, loss: 0.0017287004739046097\n",
      "iteration 15494, loss: 0.0012330921599641442\n",
      "iteration 15495, loss: 0.0014709983952343464\n",
      "iteration 15496, loss: 0.0014776121824979782\n",
      "iteration 15497, loss: 0.0015727977734059095\n",
      "iteration 15498, loss: 0.0014193773968145251\n",
      "iteration 15499, loss: 0.0015690014697611332\n",
      "iteration 15500, loss: 0.0012423527659848332\n",
      "iteration 15501, loss: 0.0014201172161847353\n",
      "iteration 15502, loss: 0.001698383130133152\n",
      "iteration 15503, loss: 0.001788238063454628\n",
      "iteration 15504, loss: 0.0014841093216091394\n",
      "iteration 15505, loss: 0.001104762894101441\n",
      "iteration 15506, loss: 0.0012598587200045586\n",
      "iteration 15507, loss: 0.0014418764039874077\n",
      "iteration 15508, loss: 0.0018359129317104816\n",
      "iteration 15509, loss: 0.0013114586472511292\n",
      "iteration 15510, loss: 0.0018409972544759512\n",
      "iteration 15511, loss: 0.0019535867031663656\n",
      "iteration 15512, loss: 0.0016654238570481539\n",
      "iteration 15513, loss: 0.0014203200116753578\n",
      "iteration 15514, loss: 0.0018512774258852005\n",
      "iteration 15515, loss: 0.001503107137978077\n",
      "iteration 15516, loss: 0.0017423660028725863\n",
      "iteration 15517, loss: 0.0016269421903416514\n",
      "iteration 15518, loss: 0.0016913939034566283\n",
      "iteration 15519, loss: 0.0018896585097536445\n",
      "iteration 15520, loss: 0.001571849687024951\n",
      "iteration 15521, loss: 0.0013332798844203353\n",
      "iteration 15522, loss: 0.0014497844967991114\n",
      "iteration 15523, loss: 0.0014430312439799309\n",
      "iteration 15524, loss: 0.0013576208148151636\n",
      "iteration 15525, loss: 0.001857565250247717\n",
      "iteration 15526, loss: 0.0012582190101966262\n",
      "iteration 15527, loss: 0.0016652543563395739\n",
      "iteration 15528, loss: 0.0013498460175469518\n",
      "iteration 15529, loss: 0.0015380200929939747\n",
      "iteration 15530, loss: 0.0013207856100052595\n",
      "iteration 15531, loss: 0.001525555970147252\n",
      "iteration 15532, loss: 0.0016221689293161035\n",
      "iteration 15533, loss: 0.0015929590445011854\n",
      "iteration 15534, loss: 0.0014587959740310907\n",
      "iteration 15535, loss: 0.0015152640407904983\n",
      "iteration 15536, loss: 0.0015715048648416996\n",
      "iteration 15537, loss: 0.0015315429773181677\n",
      "iteration 15538, loss: 0.001590242376551032\n",
      "iteration 15539, loss: 0.001466468907892704\n",
      "iteration 15540, loss: 0.0015182093484327197\n",
      "iteration 15541, loss: 0.0014586581382900476\n",
      "iteration 15542, loss: 0.001126570743508637\n",
      "iteration 15543, loss: 0.0014338017208501697\n",
      "iteration 15544, loss: 0.001375155639834702\n",
      "iteration 15545, loss: 0.0014052524929866195\n",
      "iteration 15546, loss: 0.001328207552433014\n",
      "iteration 15547, loss: 0.0015889208298176527\n",
      "iteration 15548, loss: 0.001291818218305707\n",
      "iteration 15549, loss: 0.0013207450974732637\n",
      "iteration 15550, loss: 0.0014606426702812314\n",
      "iteration 15551, loss: 0.0015032575465738773\n",
      "iteration 15552, loss: 0.001552922884002328\n",
      "iteration 15553, loss: 0.0013437443412840366\n",
      "iteration 15554, loss: 0.001527966931462288\n",
      "iteration 15555, loss: 0.0011861950624734163\n",
      "iteration 15556, loss: 0.0012521033640950918\n",
      "iteration 15557, loss: 0.001357044093310833\n",
      "iteration 15558, loss: 0.0016892185667529702\n",
      "iteration 15559, loss: 0.0012150330003350973\n",
      "iteration 15560, loss: 0.0014407123671844602\n",
      "iteration 15561, loss: 0.001548430067487061\n",
      "iteration 15562, loss: 0.0014183127786964178\n",
      "iteration 15563, loss: 0.0013370296219363809\n",
      "iteration 15564, loss: 0.0017086714506149292\n",
      "iteration 15565, loss: 0.0013083729427307844\n",
      "iteration 15566, loss: 0.0015866681933403015\n",
      "iteration 15567, loss: 0.0017360844649374485\n",
      "iteration 15568, loss: 0.00135640031658113\n",
      "iteration 15569, loss: 0.0012785930885002017\n",
      "iteration 15570, loss: 0.0016001013573259115\n",
      "iteration 15571, loss: 0.0014712223783135414\n",
      "iteration 15572, loss: 0.0018378645181655884\n",
      "iteration 15573, loss: 0.0015054034302011132\n",
      "iteration 15574, loss: 0.0015925040934234858\n",
      "iteration 15575, loss: 0.0016672293422743678\n",
      "iteration 15576, loss: 0.0014999512350186706\n",
      "iteration 15577, loss: 0.0013317960547283292\n",
      "iteration 15578, loss: 0.001487302710302174\n",
      "iteration 15579, loss: 0.0012860249262303114\n",
      "iteration 15580, loss: 0.001426419010385871\n",
      "iteration 15581, loss: 0.0013239877298474312\n",
      "iteration 15582, loss: 0.0015309141017496586\n",
      "iteration 15583, loss: 0.0014206403866410255\n",
      "iteration 15584, loss: 0.0014158878475427628\n",
      "iteration 15585, loss: 0.0014545212034136057\n",
      "iteration 15586, loss: 0.0014453292824327946\n",
      "iteration 15587, loss: 0.0012667879927903414\n",
      "iteration 15588, loss: 0.0015411500353366137\n",
      "iteration 15589, loss: 0.001851396169513464\n",
      "iteration 15590, loss: 0.0011742834467440844\n",
      "iteration 15591, loss: 0.0016607450088486075\n",
      "iteration 15592, loss: 0.0019396806601434946\n",
      "iteration 15593, loss: 0.0013233390636742115\n",
      "iteration 15594, loss: 0.0016362626338377595\n",
      "iteration 15595, loss: 0.0014202813617885113\n",
      "iteration 15596, loss: 0.001310683903284371\n",
      "iteration 15597, loss: 0.0015684256795793772\n",
      "iteration 15598, loss: 0.0014471603790298104\n",
      "iteration 15599, loss: 0.0014511467888951302\n",
      "iteration 15600, loss: 0.0013381014578044415\n",
      "iteration 15601, loss: 0.0016957309562712908\n",
      "iteration 15602, loss: 0.0017107089515775442\n",
      "iteration 15603, loss: 0.0012830314226448536\n",
      "iteration 15604, loss: 0.001680903136730194\n",
      "iteration 15605, loss: 0.0018219412304461002\n",
      "iteration 15606, loss: 0.0014492644695565104\n",
      "iteration 15607, loss: 0.00155039899982512\n",
      "iteration 15608, loss: 0.0017202759627252817\n",
      "iteration 15609, loss: 0.001267309533432126\n",
      "iteration 15610, loss: 0.001318093971349299\n",
      "iteration 15611, loss: 0.0013352127280086279\n",
      "iteration 15612, loss: 0.001436074497178197\n",
      "iteration 15613, loss: 0.0017075035721063614\n",
      "iteration 15614, loss: 0.0016524791717529297\n",
      "iteration 15615, loss: 0.001674898318015039\n",
      "iteration 15616, loss: 0.0017052621114999056\n",
      "iteration 15617, loss: 0.001262885401956737\n",
      "iteration 15618, loss: 0.0015357788652181625\n",
      "iteration 15619, loss: 0.0018343963893130422\n",
      "iteration 15620, loss: 0.0012873576488345861\n",
      "iteration 15621, loss: 0.0015531093813478947\n",
      "iteration 15622, loss: 0.00120727124158293\n",
      "iteration 15623, loss: 0.0013705469900742173\n",
      "iteration 15624, loss: 0.0013249232433736324\n",
      "iteration 15625, loss: 0.0013735396787524223\n",
      "iteration 15626, loss: 0.0015086920466274023\n",
      "iteration 15627, loss: 0.0014339983463287354\n",
      "iteration 15628, loss: 0.0015374863287433982\n",
      "iteration 15629, loss: 0.0019955006428062916\n",
      "iteration 15630, loss: 0.00161456607747823\n",
      "iteration 15631, loss: 0.0016194309573620558\n",
      "iteration 15632, loss: 0.0019717738032341003\n",
      "iteration 15633, loss: 0.0013624468119814992\n",
      "iteration 15634, loss: 0.002032727934420109\n",
      "iteration 15635, loss: 0.001506224274635315\n",
      "iteration 15636, loss: 0.0015853475779294968\n",
      "iteration 15637, loss: 0.0015095099806785583\n",
      "iteration 15638, loss: 0.0015027420595288277\n",
      "iteration 15639, loss: 0.0016945560928434134\n",
      "iteration 15640, loss: 0.00163130066357553\n",
      "iteration 15641, loss: 0.001435636542737484\n",
      "iteration 15642, loss: 0.001790583599358797\n",
      "iteration 15643, loss: 0.0015559541061520576\n",
      "iteration 15644, loss: 0.001492336392402649\n",
      "iteration 15645, loss: 0.0012690750882029533\n",
      "iteration 15646, loss: 0.0014286215882748365\n",
      "iteration 15647, loss: 0.0017891363240778446\n",
      "iteration 15648, loss: 0.0013747799675911665\n",
      "iteration 15649, loss: 0.0015786272706463933\n",
      "iteration 15650, loss: 0.0019525133538991213\n",
      "iteration 15651, loss: 0.001612736377865076\n",
      "iteration 15652, loss: 0.0014264052733778954\n",
      "iteration 15653, loss: 0.0014760715421289206\n",
      "iteration 15654, loss: 0.0012813415378332138\n",
      "iteration 15655, loss: 0.0012212777510285378\n",
      "iteration 15656, loss: 0.0012681290972977877\n",
      "iteration 15657, loss: 0.0013270543422549963\n",
      "iteration 15658, loss: 0.001467845169827342\n",
      "iteration 15659, loss: 0.0012032161466777325\n",
      "iteration 15660, loss: 0.0013756531989201903\n",
      "iteration 15661, loss: 0.0012444013264030218\n",
      "iteration 15662, loss: 0.0013597665820270777\n",
      "iteration 15663, loss: 0.0011987125035375357\n",
      "iteration 15664, loss: 0.0013254942605271935\n",
      "iteration 15665, loss: 0.0015183467185124755\n",
      "iteration 15666, loss: 0.0015851922798901796\n",
      "iteration 15667, loss: 0.0014360686764121056\n",
      "iteration 15668, loss: 0.0013205049326643348\n",
      "iteration 15669, loss: 0.001465104054659605\n",
      "iteration 15670, loss: 0.001536881667561829\n",
      "iteration 15671, loss: 0.0013384377816691995\n",
      "iteration 15672, loss: 0.0012945830821990967\n",
      "iteration 15673, loss: 0.001598075032234192\n",
      "iteration 15674, loss: 0.0012233757879585028\n",
      "iteration 15675, loss: 0.0014216669369488955\n",
      "iteration 15676, loss: 0.0012998138554394245\n",
      "iteration 15677, loss: 0.0013747685588896275\n",
      "iteration 15678, loss: 0.0015972349792718887\n",
      "iteration 15679, loss: 0.001193559030070901\n",
      "iteration 15680, loss: 0.0013858190504834056\n",
      "iteration 15681, loss: 0.0014007731806486845\n",
      "iteration 15682, loss: 0.0014329580590128899\n",
      "iteration 15683, loss: 0.001531435875222087\n",
      "iteration 15684, loss: 0.0017750284168869257\n",
      "iteration 15685, loss: 0.0013913513394072652\n",
      "iteration 15686, loss: 0.0013665909646078944\n",
      "iteration 15687, loss: 0.0016453301068395376\n",
      "iteration 15688, loss: 0.0016308564227074385\n",
      "iteration 15689, loss: 0.0014077594969421625\n",
      "iteration 15690, loss: 0.0013687412720173597\n",
      "iteration 15691, loss: 0.0014674814883619547\n",
      "iteration 15692, loss: 0.001264597987756133\n",
      "iteration 15693, loss: 0.0014044302515685558\n",
      "iteration 15694, loss: 0.0012650757562369108\n",
      "iteration 15695, loss: 0.0013652967754751444\n",
      "iteration 15696, loss: 0.0013996521010994911\n",
      "iteration 15697, loss: 0.0016448232345283031\n",
      "iteration 15698, loss: 0.0012180376797914505\n",
      "iteration 15699, loss: 0.00118781509809196\n",
      "iteration 15700, loss: 0.0011981285642832518\n",
      "iteration 15701, loss: 0.0016726674512028694\n",
      "iteration 15702, loss: 0.0013181683607399464\n",
      "iteration 15703, loss: 0.0013488242402672768\n",
      "iteration 15704, loss: 0.0015076056588441133\n",
      "iteration 15705, loss: 0.0013546134578064084\n",
      "iteration 15706, loss: 0.0012903765309602022\n",
      "iteration 15707, loss: 0.001262233592569828\n",
      "iteration 15708, loss: 0.0015558521263301373\n",
      "iteration 15709, loss: 0.0014213466783985496\n",
      "iteration 15710, loss: 0.0014335890300571918\n",
      "iteration 15711, loss: 0.0010122943203896284\n",
      "iteration 15712, loss: 0.001647591358050704\n",
      "iteration 15713, loss: 0.0012357579544186592\n",
      "iteration 15714, loss: 0.001537662697955966\n",
      "iteration 15715, loss: 0.0018414428923279047\n",
      "iteration 15716, loss: 0.001379669876769185\n",
      "iteration 15717, loss: 0.0013959980569779873\n",
      "iteration 15718, loss: 0.001581442542374134\n",
      "iteration 15719, loss: 0.0015199759509414434\n",
      "iteration 15720, loss: 0.0014472806360572577\n",
      "iteration 15721, loss: 0.0019652354530990124\n",
      "iteration 15722, loss: 0.0017863080138340592\n",
      "iteration 15723, loss: 0.0017474762862548232\n",
      "iteration 15724, loss: 0.001398218097165227\n",
      "iteration 15725, loss: 0.0013334883842617273\n",
      "iteration 15726, loss: 0.0016739633865654469\n",
      "iteration 15727, loss: 0.0018112905090674758\n",
      "iteration 15728, loss: 0.0016841921024024487\n",
      "iteration 15729, loss: 0.001393947983160615\n",
      "iteration 15730, loss: 0.001549260225147009\n",
      "iteration 15731, loss: 0.0015186958480626345\n",
      "iteration 15732, loss: 0.0014727097004652023\n",
      "iteration 15733, loss: 0.0016236379742622375\n",
      "iteration 15734, loss: 0.0012626309180632234\n",
      "iteration 15735, loss: 0.0014071946498006582\n",
      "iteration 15736, loss: 0.0013505836250260472\n",
      "iteration 15737, loss: 0.0013887891545891762\n",
      "iteration 15738, loss: 0.001596976537257433\n",
      "iteration 15739, loss: 0.001464991713874042\n",
      "iteration 15740, loss: 0.001389083219692111\n",
      "iteration 15741, loss: 0.0014201586600393057\n",
      "iteration 15742, loss: 0.0016412300756201148\n",
      "iteration 15743, loss: 0.0013801072491332889\n",
      "iteration 15744, loss: 0.001489871647208929\n",
      "iteration 15745, loss: 0.0012869363417848945\n",
      "iteration 15746, loss: 0.001322583993896842\n",
      "iteration 15747, loss: 0.001286227023229003\n",
      "iteration 15748, loss: 0.0016688414616510272\n",
      "iteration 15749, loss: 0.0015976824797689915\n",
      "iteration 15750, loss: 0.0017026775749400258\n",
      "iteration 15751, loss: 0.0013941905926913023\n",
      "iteration 15752, loss: 0.0014870921149849892\n",
      "iteration 15753, loss: 0.0014070842880755663\n",
      "iteration 15754, loss: 0.0016008478123694658\n",
      "iteration 15755, loss: 0.000887741451151669\n",
      "iteration 15756, loss: 0.0014479830861091614\n",
      "iteration 15757, loss: 0.0014952898491173983\n",
      "iteration 15758, loss: 0.001454797456972301\n",
      "iteration 15759, loss: 0.0013743704184889793\n",
      "iteration 15760, loss: 0.0014529353938996792\n",
      "iteration 15761, loss: 0.0013096865732222795\n",
      "iteration 15762, loss: 0.0015531571116298437\n",
      "iteration 15763, loss: 0.0013461934868246317\n",
      "iteration 15764, loss: 0.0014045297866687179\n",
      "iteration 15765, loss: 0.0016464560758322477\n",
      "iteration 15766, loss: 0.0017227852949872613\n",
      "iteration 15767, loss: 0.0014782476937398314\n",
      "iteration 15768, loss: 0.0013218800304457545\n",
      "iteration 15769, loss: 0.001471064635552466\n",
      "iteration 15770, loss: 0.0017388187116011977\n",
      "iteration 15771, loss: 0.0014786210376769304\n",
      "iteration 15772, loss: 0.0013905096566304564\n",
      "iteration 15773, loss: 0.0014941480476409197\n",
      "iteration 15774, loss: 0.0014060587855055928\n",
      "iteration 15775, loss: 0.0016919777262955904\n",
      "iteration 15776, loss: 0.0013712204527109861\n",
      "iteration 15777, loss: 0.0015035390388220549\n",
      "iteration 15778, loss: 0.001588624669238925\n",
      "iteration 15779, loss: 0.0013469373807311058\n",
      "iteration 15780, loss: 0.001621635747142136\n",
      "iteration 15781, loss: 0.0014444029657170177\n",
      "iteration 15782, loss: 0.0013476470485329628\n",
      "iteration 15783, loss: 0.00146729894913733\n",
      "iteration 15784, loss: 0.001511111855506897\n",
      "iteration 15785, loss: 0.00155452371109277\n",
      "iteration 15786, loss: 0.0013755873078480363\n",
      "iteration 15787, loss: 0.0018011531792581081\n",
      "iteration 15788, loss: 0.001331372419372201\n",
      "iteration 15789, loss: 0.0015491143567487597\n",
      "iteration 15790, loss: 0.001564766513183713\n",
      "iteration 15791, loss: 0.0018403108697384596\n",
      "iteration 15792, loss: 0.0015351719921454787\n",
      "iteration 15793, loss: 0.0016170006711035967\n",
      "iteration 15794, loss: 0.0013883677311241627\n",
      "iteration 15795, loss: 0.0016406162176281214\n",
      "iteration 15796, loss: 0.0013991680461913347\n",
      "iteration 15797, loss: 0.0012603756040334702\n",
      "iteration 15798, loss: 0.001682434929534793\n",
      "iteration 15799, loss: 0.0012830180348828435\n",
      "iteration 15800, loss: 0.001538679702207446\n",
      "iteration 15801, loss: 0.0017381369834765792\n",
      "iteration 15802, loss: 0.0014299801550805569\n",
      "iteration 15803, loss: 0.0014694464625790715\n",
      "iteration 15804, loss: 0.0011488305171951652\n",
      "iteration 15805, loss: 0.0016691687051206827\n",
      "iteration 15806, loss: 0.0013561018276959658\n",
      "iteration 15807, loss: 0.0016527525149285793\n",
      "iteration 15808, loss: 0.0016024635406211019\n",
      "iteration 15809, loss: 0.0013739312998950481\n",
      "iteration 15810, loss: 0.0014956688974052668\n",
      "iteration 15811, loss: 0.001912200590595603\n",
      "iteration 15812, loss: 0.0011734215077012777\n",
      "iteration 15813, loss: 0.00140570686198771\n",
      "iteration 15814, loss: 0.0016083747614175081\n",
      "iteration 15815, loss: 0.00126789475325495\n",
      "iteration 15816, loss: 0.0015425599412992597\n",
      "iteration 15817, loss: 0.0012072308454662561\n",
      "iteration 15818, loss: 0.0016229113098233938\n",
      "iteration 15819, loss: 0.0013873204588890076\n",
      "iteration 15820, loss: 0.001869586412794888\n",
      "iteration 15821, loss: 0.0016857314622029662\n",
      "iteration 15822, loss: 0.001169466064311564\n",
      "iteration 15823, loss: 0.0015529561787843704\n",
      "iteration 15824, loss: 0.0014893148327246308\n",
      "iteration 15825, loss: 0.00151668896432966\n",
      "iteration 15826, loss: 0.0015598400495946407\n",
      "iteration 15827, loss: 0.0014690037351101637\n",
      "iteration 15828, loss: 0.0016771260416135192\n",
      "iteration 15829, loss: 0.0012602126225829124\n",
      "iteration 15830, loss: 0.0014687265502288938\n",
      "iteration 15831, loss: 0.0011038280790671706\n",
      "iteration 15832, loss: 0.000967807078268379\n",
      "iteration 15833, loss: 0.0016921557253226638\n",
      "iteration 15834, loss: 0.0017436817288398743\n",
      "iteration 15835, loss: 0.0017831709701567888\n",
      "iteration 15836, loss: 0.001231034635566175\n",
      "iteration 15837, loss: 0.0013907402753829956\n",
      "iteration 15838, loss: 0.0011793994344770908\n",
      "iteration 15839, loss: 0.0013502861838787794\n",
      "iteration 15840, loss: 0.0012205648235976696\n",
      "iteration 15841, loss: 0.0014942236011847854\n",
      "iteration 15842, loss: 0.00159636908210814\n",
      "iteration 15843, loss: 0.0010894238948822021\n",
      "iteration 15844, loss: 0.0011878160294145346\n",
      "iteration 15845, loss: 0.001044858479872346\n",
      "iteration 15846, loss: 0.0012669309508055449\n",
      "iteration 15847, loss: 0.00115663418546319\n",
      "iteration 15848, loss: 0.0010841784533113241\n",
      "iteration 15849, loss: 0.0011763963848352432\n",
      "iteration 15850, loss: 0.0013555793557316065\n",
      "iteration 15851, loss: 0.0014041636604815722\n",
      "iteration 15852, loss: 0.0012453761883080006\n",
      "iteration 15853, loss: 0.0012185326777398586\n",
      "iteration 15854, loss: 0.0011194348335266113\n",
      "iteration 15855, loss: 0.0016983315581455827\n",
      "iteration 15856, loss: 0.0011461079120635986\n",
      "iteration 15857, loss: 0.001551118795759976\n",
      "iteration 15858, loss: 0.001328412676230073\n",
      "iteration 15859, loss: 0.0013892506249248981\n",
      "iteration 15860, loss: 0.0015626237727701664\n",
      "iteration 15861, loss: 0.0013008215464651585\n",
      "iteration 15862, loss: 0.0010999715887010098\n",
      "iteration 15863, loss: 0.0014385977992787957\n",
      "iteration 15864, loss: 0.001244069542735815\n",
      "iteration 15865, loss: 0.0013082334771752357\n",
      "iteration 15866, loss: 0.001634563785046339\n",
      "iteration 15867, loss: 0.0016327615594491363\n",
      "iteration 15868, loss: 0.0013878821628168225\n",
      "iteration 15869, loss: 0.0013106404803693295\n",
      "iteration 15870, loss: 0.0016273845685645938\n",
      "iteration 15871, loss: 0.0013537481427192688\n",
      "iteration 15872, loss: 0.0012986736837774515\n",
      "iteration 15873, loss: 0.0017446035053581\n",
      "iteration 15874, loss: 0.001630036043934524\n",
      "iteration 15875, loss: 0.0013779262080788612\n",
      "iteration 15876, loss: 0.0014891100581735373\n",
      "iteration 15877, loss: 0.001423026667907834\n",
      "iteration 15878, loss: 0.0015255038160830736\n",
      "iteration 15879, loss: 0.0013150913873687387\n",
      "iteration 15880, loss: 0.001158425584435463\n",
      "iteration 15881, loss: 0.001321213087067008\n",
      "iteration 15882, loss: 0.001302075106650591\n",
      "iteration 15883, loss: 0.0016061746282503009\n",
      "iteration 15884, loss: 0.0017972488421946764\n",
      "iteration 15885, loss: 0.0013483285438269377\n",
      "iteration 15886, loss: 0.0015265269903466105\n",
      "iteration 15887, loss: 0.0013671335764229298\n",
      "iteration 15888, loss: 0.0014847368001937866\n",
      "iteration 15889, loss: 0.0013808144722133875\n",
      "iteration 15890, loss: 0.001219901372678578\n",
      "iteration 15891, loss: 0.0015425768215209246\n",
      "iteration 15892, loss: 0.001403891947120428\n",
      "iteration 15893, loss: 0.0011929050087928772\n",
      "iteration 15894, loss: 0.0013909126864746213\n",
      "iteration 15895, loss: 0.0011939909309148788\n",
      "iteration 15896, loss: 0.0013494212180376053\n",
      "iteration 15897, loss: 0.001331806881353259\n",
      "iteration 15898, loss: 0.001385513343848288\n",
      "iteration 15899, loss: 0.0015863345470279455\n",
      "iteration 15900, loss: 0.0015964548802003264\n",
      "iteration 15901, loss: 0.0014485723804682493\n",
      "iteration 15902, loss: 0.001253744587302208\n",
      "iteration 15903, loss: 0.0015237894840538502\n",
      "iteration 15904, loss: 0.0015795142389833927\n",
      "iteration 15905, loss: 0.0014339006738737226\n",
      "iteration 15906, loss: 0.0014418037608265877\n",
      "iteration 15907, loss: 0.0016793184913694859\n",
      "iteration 15908, loss: 0.0015094542177394032\n",
      "iteration 15909, loss: 0.001406173687428236\n",
      "iteration 15910, loss: 0.001486464636400342\n",
      "iteration 15911, loss: 0.0012629693374037743\n",
      "iteration 15912, loss: 0.0013332609087228775\n",
      "iteration 15913, loss: 0.0016430686227977276\n",
      "iteration 15914, loss: 0.0016792058013379574\n",
      "iteration 15915, loss: 0.0015476616099476814\n",
      "iteration 15916, loss: 0.0015212864382192492\n",
      "iteration 15917, loss: 0.001367019023746252\n",
      "iteration 15918, loss: 0.001577202114276588\n",
      "iteration 15919, loss: 0.001740989275276661\n",
      "iteration 15920, loss: 0.0013802316971123219\n",
      "iteration 15921, loss: 0.0019032416166737676\n",
      "iteration 15922, loss: 0.0015533363912254572\n",
      "iteration 15923, loss: 0.0016064480878412724\n",
      "iteration 15924, loss: 0.0018550724489614367\n",
      "iteration 15925, loss: 0.0013684879522770643\n",
      "iteration 15926, loss: 0.0013565393164753914\n",
      "iteration 15927, loss: 0.0015998450107872486\n",
      "iteration 15928, loss: 0.0015407229075208306\n",
      "iteration 15929, loss: 0.0013697500107809901\n",
      "iteration 15930, loss: 0.0016477197641506791\n",
      "iteration 15931, loss: 0.0014141539577394724\n",
      "iteration 15932, loss: 0.0016243824502453208\n",
      "iteration 15933, loss: 0.0013837029691785574\n",
      "iteration 15934, loss: 0.0015285066328942776\n",
      "iteration 15935, loss: 0.0015130853280425072\n",
      "iteration 15936, loss: 0.0014093315694481134\n",
      "iteration 15937, loss: 0.0015247203409671783\n",
      "iteration 15938, loss: 0.0012381898704916239\n",
      "iteration 15939, loss: 0.0016060969792306423\n",
      "iteration 15940, loss: 0.0015355056384578347\n",
      "iteration 15941, loss: 0.0014343224465847015\n",
      "iteration 15942, loss: 0.0013786358758807182\n",
      "iteration 15943, loss: 0.0013239274267107248\n",
      "iteration 15944, loss: 0.001464408589527011\n",
      "iteration 15945, loss: 0.0018229696433991194\n",
      "iteration 15946, loss: 0.0013504890957847238\n",
      "iteration 15947, loss: 0.0013918993063271046\n",
      "iteration 15948, loss: 0.0012722420506179333\n",
      "iteration 15949, loss: 0.0012582268100231886\n",
      "iteration 15950, loss: 0.0011460313107818365\n",
      "iteration 15951, loss: 0.0012980027822777629\n",
      "iteration 15952, loss: 0.0014130811905488372\n",
      "iteration 15953, loss: 0.0013298599515110254\n",
      "iteration 15954, loss: 0.0012540658935904503\n",
      "iteration 15955, loss: 0.0013646574225276709\n",
      "iteration 15956, loss: 0.001081996364519\n",
      "iteration 15957, loss: 0.0013783928006887436\n",
      "iteration 15958, loss: 0.0013823361368849874\n",
      "iteration 15959, loss: 0.0015629344852641225\n",
      "iteration 15960, loss: 0.001541506266221404\n",
      "iteration 15961, loss: 0.000998685136437416\n",
      "iteration 15962, loss: 0.0014031038153916597\n",
      "iteration 15963, loss: 0.0013444905634969473\n",
      "iteration 15964, loss: 0.0014167304616421461\n",
      "iteration 15965, loss: 0.0011525691952556372\n",
      "iteration 15966, loss: 0.0015504807233810425\n",
      "iteration 15967, loss: 0.001595312962308526\n",
      "iteration 15968, loss: 0.0012465461622923613\n",
      "iteration 15969, loss: 0.0015182464849203825\n",
      "iteration 15970, loss: 0.001181541127152741\n",
      "iteration 15971, loss: 0.0014269697712734342\n",
      "iteration 15972, loss: 0.0012776274234056473\n",
      "iteration 15973, loss: 0.0014874304179102182\n",
      "iteration 15974, loss: 0.0016849550884217024\n",
      "iteration 15975, loss: 0.0015717183705419302\n",
      "iteration 15976, loss: 0.0012264094548299909\n",
      "iteration 15977, loss: 0.0014138834085315466\n",
      "iteration 15978, loss: 0.00129043054766953\n",
      "iteration 15979, loss: 0.0014441227540373802\n",
      "iteration 15980, loss: 0.001207654015161097\n",
      "iteration 15981, loss: 0.0014371802099049091\n",
      "iteration 15982, loss: 0.0014662655303254724\n",
      "iteration 15983, loss: 0.0013756996486335993\n",
      "iteration 15984, loss: 0.0011628677602857351\n",
      "iteration 15985, loss: 0.0013620276004076004\n",
      "iteration 15986, loss: 0.0012767039006575942\n",
      "iteration 15987, loss: 0.00188554753549397\n",
      "iteration 15988, loss: 0.0016485598171129823\n",
      "iteration 15989, loss: 0.0015423448057845235\n",
      "iteration 15990, loss: 0.001364435418508947\n",
      "iteration 15991, loss: 0.0013683863217011094\n",
      "iteration 15992, loss: 0.0015461173607036471\n",
      "iteration 15993, loss: 0.00193848367780447\n",
      "iteration 15994, loss: 0.0015489431098103523\n",
      "iteration 15995, loss: 0.00142037239857018\n",
      "iteration 15996, loss: 0.0016821506433188915\n",
      "iteration 15997, loss: 0.0014340502675622702\n",
      "iteration 15998, loss: 0.0015905145555734634\n",
      "iteration 15999, loss: 0.0013184405397623777\n",
      "iteration 16000, loss: 0.001800274127162993\n",
      "iteration 16001, loss: 0.0014328390825539827\n",
      "iteration 16002, loss: 0.0014105747686699033\n",
      "iteration 16003, loss: 0.0018354924395680428\n",
      "iteration 16004, loss: 0.001566705759614706\n",
      "iteration 16005, loss: 0.0015585471410304308\n",
      "iteration 16006, loss: 0.0017182871233671904\n",
      "iteration 16007, loss: 0.0014607778284698725\n",
      "iteration 16008, loss: 0.0012196074239909649\n",
      "iteration 16009, loss: 0.0014393124729394913\n",
      "iteration 16010, loss: 0.0018315367633476853\n",
      "iteration 16011, loss: 0.0012741279788315296\n",
      "iteration 16012, loss: 0.0016268924809992313\n",
      "iteration 16013, loss: 0.001524176448583603\n",
      "iteration 16014, loss: 0.0014957378152757883\n",
      "iteration 16015, loss: 0.001433514873497188\n",
      "iteration 16016, loss: 0.0017253939295187593\n",
      "iteration 16017, loss: 0.0017366387182846665\n",
      "iteration 16018, loss: 0.001191624323837459\n",
      "iteration 16019, loss: 0.0015342967817559838\n",
      "iteration 16020, loss: 0.0015463216695934534\n",
      "iteration 16021, loss: 0.0014717807061970234\n",
      "iteration 16022, loss: 0.001347264158539474\n",
      "iteration 16023, loss: 0.0012956608552485704\n",
      "iteration 16024, loss: 0.0014839450595900416\n",
      "iteration 16025, loss: 0.0015701986849308014\n",
      "iteration 16026, loss: 0.0014085413422435522\n",
      "iteration 16027, loss: 0.0018379285465925932\n",
      "iteration 16028, loss: 0.0014663358451798558\n",
      "iteration 16029, loss: 0.0011556139215826988\n",
      "iteration 16030, loss: 0.001408943673595786\n",
      "iteration 16031, loss: 0.0014530334156006575\n",
      "iteration 16032, loss: 0.0015431907959282398\n",
      "iteration 16033, loss: 0.0014006197452545166\n",
      "iteration 16034, loss: 0.0013599100057035685\n",
      "iteration 16035, loss: 0.0012965725036337972\n",
      "iteration 16036, loss: 0.0013586594723165035\n",
      "iteration 16037, loss: 0.0015044058673083782\n",
      "iteration 16038, loss: 0.0015249202260747552\n",
      "iteration 16039, loss: 0.0013662326382473111\n",
      "iteration 16040, loss: 0.0013097240589559078\n",
      "iteration 16041, loss: 0.0013481664936989546\n",
      "iteration 16042, loss: 0.0014717676676809788\n",
      "iteration 16043, loss: 0.0017657583812251687\n",
      "iteration 16044, loss: 0.0012444945750758052\n",
      "iteration 16045, loss: 0.001490424619987607\n",
      "iteration 16046, loss: 0.0015103531768545508\n",
      "iteration 16047, loss: 0.001492007402703166\n",
      "iteration 16048, loss: 0.0016344125615432858\n",
      "iteration 16049, loss: 0.0014842536766082048\n",
      "iteration 16050, loss: 0.001922029536217451\n",
      "iteration 16051, loss: 0.0014564604498445988\n",
      "iteration 16052, loss: 0.0015265792608261108\n",
      "iteration 16053, loss: 0.001207850407809019\n",
      "iteration 16054, loss: 0.0016037719324231148\n",
      "iteration 16055, loss: 0.001458107028156519\n",
      "iteration 16056, loss: 0.0014583573210984468\n",
      "iteration 16057, loss: 0.0015414913650602102\n",
      "iteration 16058, loss: 0.001348197227343917\n",
      "iteration 16059, loss: 0.0012933181133121252\n",
      "iteration 16060, loss: 0.0014577372930943966\n",
      "iteration 16061, loss: 0.001407003728672862\n",
      "iteration 16062, loss: 0.0018285883124917746\n",
      "iteration 16063, loss: 0.0014776693424209952\n",
      "iteration 16064, loss: 0.001274891896173358\n",
      "iteration 16065, loss: 0.0014308085665106773\n",
      "iteration 16066, loss: 0.0016679473919793963\n",
      "iteration 16067, loss: 0.0013730732025578618\n",
      "iteration 16068, loss: 0.0012933681719005108\n",
      "iteration 16069, loss: 0.0013729592319577932\n",
      "iteration 16070, loss: 0.0012120789615437388\n",
      "iteration 16071, loss: 0.001406901516020298\n",
      "iteration 16072, loss: 0.0014553944347426295\n",
      "iteration 16073, loss: 0.0013720912393182516\n",
      "iteration 16074, loss: 0.0011882537510246038\n",
      "iteration 16075, loss: 0.0012234440073370934\n",
      "iteration 16076, loss: 0.0011274940334260464\n",
      "iteration 16077, loss: 0.0013420736650004983\n",
      "iteration 16078, loss: 0.001411204575560987\n",
      "iteration 16079, loss: 0.0015530548989772797\n",
      "iteration 16080, loss: 0.0013930751010775566\n",
      "iteration 16081, loss: 0.0013213518541306257\n",
      "iteration 16082, loss: 0.0016611007740721107\n",
      "iteration 16083, loss: 0.0015878003323450685\n",
      "iteration 16084, loss: 0.001471501076593995\n",
      "iteration 16085, loss: 0.0017473724437877536\n",
      "iteration 16086, loss: 0.0012746737338602543\n",
      "iteration 16087, loss: 0.001100023277103901\n",
      "iteration 16088, loss: 0.0012674137251451612\n",
      "iteration 16089, loss: 0.0012828116305172443\n",
      "iteration 16090, loss: 0.0014203691389411688\n",
      "iteration 16091, loss: 0.0012527101207524538\n",
      "iteration 16092, loss: 0.0013744615716859698\n",
      "iteration 16093, loss: 0.0015224891249090433\n",
      "iteration 16094, loss: 0.0012474542018026114\n",
      "iteration 16095, loss: 0.0012343570124357939\n",
      "iteration 16096, loss: 0.0017905891872942448\n",
      "iteration 16097, loss: 0.0014446033164858818\n",
      "iteration 16098, loss: 0.0013376388233155012\n",
      "iteration 16099, loss: 0.001510613365098834\n",
      "iteration 16100, loss: 0.0015987118240445852\n",
      "iteration 16101, loss: 0.0013262364082038403\n",
      "iteration 16102, loss: 0.0014939631801098585\n",
      "iteration 16103, loss: 0.0011476846411824226\n",
      "iteration 16104, loss: 0.001523384591564536\n",
      "iteration 16105, loss: 0.0013651285553351045\n",
      "iteration 16106, loss: 0.0012832944048568606\n",
      "iteration 16107, loss: 0.0014661491150036454\n",
      "iteration 16108, loss: 0.0013500119093805552\n",
      "iteration 16109, loss: 0.0013053162256255746\n",
      "iteration 16110, loss: 0.0013654157519340515\n",
      "iteration 16111, loss: 0.0011308917310088873\n",
      "iteration 16112, loss: 0.001371813821606338\n",
      "iteration 16113, loss: 0.0014402630040422082\n",
      "iteration 16114, loss: 0.0012825445737689734\n",
      "iteration 16115, loss: 0.0012525530764833093\n",
      "iteration 16116, loss: 0.0011141174472868443\n",
      "iteration 16117, loss: 0.0012271733721718192\n",
      "iteration 16118, loss: 0.00104440376162529\n",
      "iteration 16119, loss: 0.0012720462400466204\n",
      "iteration 16120, loss: 0.0014164125313982368\n",
      "iteration 16121, loss: 0.0013936953619122505\n",
      "iteration 16122, loss: 0.0014166523469612002\n",
      "iteration 16123, loss: 0.001397002604790032\n",
      "iteration 16124, loss: 0.0013469015248119831\n",
      "iteration 16125, loss: 0.0014823747333139181\n",
      "iteration 16126, loss: 0.0013016995508223772\n",
      "iteration 16127, loss: 0.0018080517183989286\n",
      "iteration 16128, loss: 0.001585568767040968\n",
      "iteration 16129, loss: 0.0014193592360243201\n",
      "iteration 16130, loss: 0.001239689765498042\n",
      "iteration 16131, loss: 0.0014708428643643856\n",
      "iteration 16132, loss: 0.0012276830384507775\n",
      "iteration 16133, loss: 0.0017215919215232134\n",
      "iteration 16134, loss: 0.0017176632536575198\n",
      "iteration 16135, loss: 0.0016640801914036274\n",
      "iteration 16136, loss: 0.0014568964252248406\n",
      "iteration 16137, loss: 0.0015085269697010517\n",
      "iteration 16138, loss: 0.001519682235084474\n",
      "iteration 16139, loss: 0.0015372054185718298\n",
      "iteration 16140, loss: 0.0011312294518575072\n",
      "iteration 16141, loss: 0.001318318652920425\n",
      "iteration 16142, loss: 0.0012833698419854045\n",
      "iteration 16143, loss: 0.0014171362854540348\n",
      "iteration 16144, loss: 0.001589753432199359\n",
      "iteration 16145, loss: 0.0012437091208994389\n",
      "iteration 16146, loss: 0.0012972186086699367\n",
      "iteration 16147, loss: 0.0013549381401389837\n",
      "iteration 16148, loss: 0.001315413974225521\n",
      "iteration 16149, loss: 0.0015142261981964111\n",
      "iteration 16150, loss: 0.0014272801345214248\n",
      "iteration 16151, loss: 0.0013089361600577831\n",
      "iteration 16152, loss: 0.0011842178646475077\n",
      "iteration 16153, loss: 0.0013485918752849102\n",
      "iteration 16154, loss: 0.0013181074755266309\n",
      "iteration 16155, loss: 0.0013311044313013554\n",
      "iteration 16156, loss: 0.0011074321810156107\n",
      "iteration 16157, loss: 0.001738378661684692\n",
      "iteration 16158, loss: 0.0010700265411287546\n",
      "iteration 16159, loss: 0.0016019560862332582\n",
      "iteration 16160, loss: 0.001671049976721406\n",
      "iteration 16161, loss: 0.0014284842181950808\n",
      "iteration 16162, loss: 0.0010534871835261583\n",
      "iteration 16163, loss: 0.0013705024030059576\n",
      "iteration 16164, loss: 0.0014784112572669983\n",
      "iteration 16165, loss: 0.0013997710775583982\n",
      "iteration 16166, loss: 0.0012197919422760606\n",
      "iteration 16167, loss: 0.001470170565880835\n",
      "iteration 16168, loss: 0.0015160060720518231\n",
      "iteration 16169, loss: 0.001387624884955585\n",
      "iteration 16170, loss: 0.0015830319607630372\n",
      "iteration 16171, loss: 0.0016178032383322716\n",
      "iteration 16172, loss: 0.001355605898424983\n",
      "iteration 16173, loss: 0.0011504900176078081\n",
      "iteration 16174, loss: 0.0014132647775113583\n",
      "iteration 16175, loss: 0.0013023228384554386\n",
      "iteration 16176, loss: 0.0012621083296835423\n",
      "iteration 16177, loss: 0.0015433179214596748\n",
      "iteration 16178, loss: 0.001258618663996458\n",
      "iteration 16179, loss: 0.00153101806063205\n",
      "iteration 16180, loss: 0.0015450746286660433\n",
      "iteration 16181, loss: 0.0011185589246451855\n",
      "iteration 16182, loss: 0.0013936866307631135\n",
      "iteration 16183, loss: 0.0014548029284924269\n",
      "iteration 16184, loss: 0.0013652861816808581\n",
      "iteration 16185, loss: 0.001424104324541986\n",
      "iteration 16186, loss: 0.0015391791239380836\n",
      "iteration 16187, loss: 0.0013385271886363626\n",
      "iteration 16188, loss: 0.001336992485448718\n",
      "iteration 16189, loss: 0.0013184258714318275\n",
      "iteration 16190, loss: 0.0015385233564302325\n",
      "iteration 16191, loss: 0.0015794170321896672\n",
      "iteration 16192, loss: 0.001542833517305553\n",
      "iteration 16193, loss: 0.0014163234736770391\n",
      "iteration 16194, loss: 0.0015607635723426938\n",
      "iteration 16195, loss: 0.0015411449130624533\n",
      "iteration 16196, loss: 0.0016245604492723942\n",
      "iteration 16197, loss: 0.0015400366391986609\n",
      "iteration 16198, loss: 0.001479576574638486\n",
      "iteration 16199, loss: 0.0014114754740148783\n",
      "iteration 16200, loss: 0.001351500628516078\n",
      "iteration 16201, loss: 0.001240645768120885\n",
      "iteration 16202, loss: 0.00124378502368927\n",
      "iteration 16203, loss: 0.0012825079029425979\n",
      "iteration 16204, loss: 0.0015784363495185971\n",
      "iteration 16205, loss: 0.001139956060796976\n",
      "iteration 16206, loss: 0.0011887147556990385\n",
      "iteration 16207, loss: 0.0015641693025827408\n",
      "iteration 16208, loss: 0.001358482288196683\n",
      "iteration 16209, loss: 0.0012817570241168141\n",
      "iteration 16210, loss: 0.0014343119692057371\n",
      "iteration 16211, loss: 0.0010835487628355622\n",
      "iteration 16212, loss: 0.0014056015061214566\n",
      "iteration 16213, loss: 0.001290402957238257\n",
      "iteration 16214, loss: 0.0014248163206502795\n",
      "iteration 16215, loss: 0.0010423511266708374\n",
      "iteration 16216, loss: 0.0013966483529657125\n",
      "iteration 16217, loss: 0.0014923136914148927\n",
      "iteration 16218, loss: 0.0013244504807516932\n",
      "iteration 16219, loss: 0.0013884566724300385\n",
      "iteration 16220, loss: 0.0013244508299976587\n",
      "iteration 16221, loss: 0.0014641090529039502\n",
      "iteration 16222, loss: 0.0012557022273540497\n",
      "iteration 16223, loss: 0.0013908008113503456\n",
      "iteration 16224, loss: 0.001359220128506422\n",
      "iteration 16225, loss: 0.00154146330896765\n",
      "iteration 16226, loss: 0.001249035936780274\n",
      "iteration 16227, loss: 0.0011858821380883455\n",
      "iteration 16228, loss: 0.001266274368390441\n",
      "iteration 16229, loss: 0.00123058189637959\n",
      "iteration 16230, loss: 0.0015377011150121689\n",
      "iteration 16231, loss: 0.001035537221468985\n",
      "iteration 16232, loss: 0.001469527604058385\n",
      "iteration 16233, loss: 0.0014889024896547198\n",
      "iteration 16234, loss: 0.0013091883156448603\n",
      "iteration 16235, loss: 0.0019162795506417751\n",
      "iteration 16236, loss: 0.0013227984309196472\n",
      "iteration 16237, loss: 0.001165933208540082\n",
      "iteration 16238, loss: 0.0011735151056200266\n",
      "iteration 16239, loss: 0.001126463059335947\n",
      "iteration 16240, loss: 0.0014594468520954251\n",
      "iteration 16241, loss: 0.0013210192555561662\n",
      "iteration 16242, loss: 0.0013653667410835624\n",
      "iteration 16243, loss: 0.0012063556350767612\n",
      "iteration 16244, loss: 0.0012422928120940924\n",
      "iteration 16245, loss: 0.001204523490741849\n",
      "iteration 16246, loss: 0.0013052639551460743\n",
      "iteration 16247, loss: 0.001278022537007928\n",
      "iteration 16248, loss: 0.0012951514218002558\n",
      "iteration 16249, loss: 0.0011143845040351152\n",
      "iteration 16250, loss: 0.0011165810283273458\n",
      "iteration 16251, loss: 0.0012838782276958227\n",
      "iteration 16252, loss: 0.0012192160356789827\n",
      "iteration 16253, loss: 0.001166097354143858\n",
      "iteration 16254, loss: 0.0011418996145948768\n",
      "iteration 16255, loss: 0.0014686529757454991\n",
      "iteration 16256, loss: 0.0012607392854988575\n",
      "iteration 16257, loss: 0.0012260467046871781\n",
      "iteration 16258, loss: 0.001509054214693606\n",
      "iteration 16259, loss: 0.0012582323979586363\n",
      "iteration 16260, loss: 0.001532155554741621\n",
      "iteration 16261, loss: 0.0015606259694322944\n",
      "iteration 16262, loss: 0.0011803628876805305\n",
      "iteration 16263, loss: 0.0013927238760516047\n",
      "iteration 16264, loss: 0.001604245277121663\n",
      "iteration 16265, loss: 0.0015122812474146485\n",
      "iteration 16266, loss: 0.0011423190589994192\n",
      "iteration 16267, loss: 0.001425455091521144\n",
      "iteration 16268, loss: 0.0013850163668394089\n",
      "iteration 16269, loss: 0.001422676257789135\n",
      "iteration 16270, loss: 0.0010832077823579311\n",
      "iteration 16271, loss: 0.0017069458262994885\n",
      "iteration 16272, loss: 0.0013940343633294106\n",
      "iteration 16273, loss: 0.0014235288836061954\n",
      "iteration 16274, loss: 0.0011941753327846527\n",
      "iteration 16275, loss: 0.0013659654650837183\n",
      "iteration 16276, loss: 0.0017363473307341337\n",
      "iteration 16277, loss: 0.0015171266859397292\n",
      "iteration 16278, loss: 0.0013402904151007533\n",
      "iteration 16279, loss: 0.0011717290617525578\n",
      "iteration 16280, loss: 0.0013575924094766378\n",
      "iteration 16281, loss: 0.001505003310739994\n",
      "iteration 16282, loss: 0.001072937622666359\n",
      "iteration 16283, loss: 0.0016766299959272146\n",
      "iteration 16284, loss: 0.0011387222912162542\n",
      "iteration 16285, loss: 0.0014604919124394655\n",
      "iteration 16286, loss: 0.0012733961921185255\n",
      "iteration 16287, loss: 0.0013523049419745803\n",
      "iteration 16288, loss: 0.0014708888484165072\n",
      "iteration 16289, loss: 0.0014354097656905651\n",
      "iteration 16290, loss: 0.0013412808766588569\n",
      "iteration 16291, loss: 0.0016583295073360205\n",
      "iteration 16292, loss: 0.001318537164479494\n",
      "iteration 16293, loss: 0.0014517689123749733\n",
      "iteration 16294, loss: 0.0014048321172595024\n",
      "iteration 16295, loss: 0.0016142444219440222\n",
      "iteration 16296, loss: 0.0015419996343553066\n",
      "iteration 16297, loss: 0.0014860592782497406\n",
      "iteration 16298, loss: 0.00131519278511405\n",
      "iteration 16299, loss: 0.0015636656899005175\n",
      "iteration 16300, loss: 0.0014630341902375221\n",
      "iteration 16301, loss: 0.0015356730436906219\n",
      "iteration 16302, loss: 0.0013855888973921537\n",
      "iteration 16303, loss: 0.0012673723977059126\n",
      "iteration 16304, loss: 0.001694357255473733\n",
      "iteration 16305, loss: 0.0015698232455179095\n",
      "iteration 16306, loss: 0.001517455792054534\n",
      "iteration 16307, loss: 0.0016179537633433938\n",
      "iteration 16308, loss: 0.0013487490359693766\n",
      "iteration 16309, loss: 0.0021951969247311354\n",
      "iteration 16310, loss: 0.0012735212221741676\n",
      "iteration 16311, loss: 0.001275126589462161\n",
      "iteration 16312, loss: 0.0013711412902921438\n",
      "iteration 16313, loss: 0.0014823253732174635\n",
      "iteration 16314, loss: 0.001735088648274541\n",
      "iteration 16315, loss: 0.0011708175297826529\n",
      "iteration 16316, loss: 0.0014136182144284248\n",
      "iteration 16317, loss: 0.0012528556399047375\n",
      "iteration 16318, loss: 0.0010773541871458292\n",
      "iteration 16319, loss: 0.0015871970681473613\n",
      "iteration 16320, loss: 0.0013360611628741026\n",
      "iteration 16321, loss: 0.0012570321559906006\n",
      "iteration 16322, loss: 0.0016097004991024733\n",
      "iteration 16323, loss: 0.0016669726464897394\n",
      "iteration 16324, loss: 0.0014293343992903829\n",
      "iteration 16325, loss: 0.0017149302875623107\n",
      "iteration 16326, loss: 0.0015223219525068998\n",
      "iteration 16327, loss: 0.001318141701631248\n",
      "iteration 16328, loss: 0.0013660017866641283\n",
      "iteration 16329, loss: 0.0012634098529815674\n",
      "iteration 16330, loss: 0.001593498862348497\n",
      "iteration 16331, loss: 0.0014386525144800544\n",
      "iteration 16332, loss: 0.0014112882781773806\n",
      "iteration 16333, loss: 0.0013993533793836832\n",
      "iteration 16334, loss: 0.001244447659701109\n",
      "iteration 16335, loss: 0.001604791497811675\n",
      "iteration 16336, loss: 0.0011611662339419127\n",
      "iteration 16337, loss: 0.0013970534782856703\n",
      "iteration 16338, loss: 0.0016314740059897304\n",
      "iteration 16339, loss: 0.001292593078687787\n",
      "iteration 16340, loss: 0.001543831080198288\n",
      "iteration 16341, loss: 0.0015306449495255947\n",
      "iteration 16342, loss: 0.001486630761064589\n",
      "iteration 16343, loss: 0.0016473480500280857\n",
      "iteration 16344, loss: 0.00159969343803823\n",
      "iteration 16345, loss: 0.0011454516788944602\n",
      "iteration 16346, loss: 0.0011776568135246634\n",
      "iteration 16347, loss: 0.0012114476412534714\n",
      "iteration 16348, loss: 0.001339891110546887\n",
      "iteration 16349, loss: 0.0014539733529090881\n",
      "iteration 16350, loss: 0.0017087201122194529\n",
      "iteration 16351, loss: 0.00145028589759022\n",
      "iteration 16352, loss: 0.0016213373746722937\n",
      "iteration 16353, loss: 0.0014434035401791334\n",
      "iteration 16354, loss: 0.00114738498814404\n",
      "iteration 16355, loss: 0.001256043091416359\n",
      "iteration 16356, loss: 0.0016418169252574444\n",
      "iteration 16357, loss: 0.0012821720447391272\n",
      "iteration 16358, loss: 0.0014620787696912885\n",
      "iteration 16359, loss: 0.0011386537225916982\n",
      "iteration 16360, loss: 0.0014011247549206018\n",
      "iteration 16361, loss: 0.001039269263856113\n",
      "iteration 16362, loss: 0.0011902350233867764\n",
      "iteration 16363, loss: 0.001208793604746461\n",
      "iteration 16364, loss: 0.001345612108707428\n",
      "iteration 16365, loss: 0.0012424769811332226\n",
      "iteration 16366, loss: 0.001270528300665319\n",
      "iteration 16367, loss: 0.0013215134385973215\n",
      "iteration 16368, loss: 0.0013496733736246824\n",
      "iteration 16369, loss: 0.0013624958228319883\n",
      "iteration 16370, loss: 0.0011622144374996424\n",
      "iteration 16371, loss: 0.0017311873380094767\n",
      "iteration 16372, loss: 0.0015256591141223907\n",
      "iteration 16373, loss: 0.0017328488174825907\n",
      "iteration 16374, loss: 0.001547127729281783\n",
      "iteration 16375, loss: 0.0014757011085748672\n",
      "iteration 16376, loss: 0.0016024428186938167\n",
      "iteration 16377, loss: 0.00151282106526196\n",
      "iteration 16378, loss: 0.0016571453306823969\n",
      "iteration 16379, loss: 0.0017642102902755141\n",
      "iteration 16380, loss: 0.0017304087523370981\n",
      "iteration 16381, loss: 0.0013330216752365232\n",
      "iteration 16382, loss: 0.0013989536091685295\n",
      "iteration 16383, loss: 0.0010615942301228642\n",
      "iteration 16384, loss: 0.001417817547917366\n",
      "iteration 16385, loss: 0.0015289514558389783\n",
      "iteration 16386, loss: 0.0013108975253999233\n",
      "iteration 16387, loss: 0.00153620436321944\n",
      "iteration 16388, loss: 0.0012397922109812498\n",
      "iteration 16389, loss: 0.001372495898976922\n",
      "iteration 16390, loss: 0.0014872037572786212\n",
      "iteration 16391, loss: 0.001851979410275817\n",
      "iteration 16392, loss: 0.001217785058543086\n",
      "iteration 16393, loss: 0.001476256176829338\n",
      "iteration 16394, loss: 0.0013085617683827877\n",
      "iteration 16395, loss: 0.0014115053927525878\n",
      "iteration 16396, loss: 0.001609456492587924\n",
      "iteration 16397, loss: 0.0014010031009092927\n",
      "iteration 16398, loss: 0.0013256451347842813\n",
      "iteration 16399, loss: 0.001454700599424541\n",
      "iteration 16400, loss: 0.0014291612897068262\n",
      "iteration 16401, loss: 0.001302609103731811\n",
      "iteration 16402, loss: 0.0012891069054603577\n",
      "iteration 16403, loss: 0.001510349102318287\n",
      "iteration 16404, loss: 0.0011827002745121717\n",
      "iteration 16405, loss: 0.0013195377541705966\n",
      "iteration 16406, loss: 0.0013486756943166256\n",
      "iteration 16407, loss: 0.0012953293044120073\n",
      "iteration 16408, loss: 0.0012963144108653069\n",
      "iteration 16409, loss: 0.0014143005246296525\n",
      "iteration 16410, loss: 0.001554863527417183\n",
      "iteration 16411, loss: 0.001322553027421236\n",
      "iteration 16412, loss: 0.0013002335326746106\n",
      "iteration 16413, loss: 0.0010948129929602146\n",
      "iteration 16414, loss: 0.0013017694000154734\n",
      "iteration 16415, loss: 0.0011517514940351248\n",
      "iteration 16416, loss: 0.0011674612760543823\n",
      "iteration 16417, loss: 0.00137759605422616\n",
      "iteration 16418, loss: 0.0014534529764205217\n",
      "iteration 16419, loss: 0.002360170241445303\n",
      "iteration 16420, loss: 0.00117605144623667\n",
      "iteration 16421, loss: 0.001224722247570753\n",
      "iteration 16422, loss: 0.0013197781518101692\n",
      "iteration 16423, loss: 0.001407574862241745\n",
      "iteration 16424, loss: 0.0013803743058815598\n",
      "iteration 16425, loss: 0.0013554999604821205\n",
      "iteration 16426, loss: 0.001342644914984703\n",
      "iteration 16427, loss: 0.0015445718308910728\n",
      "iteration 16428, loss: 0.0016258666291832924\n",
      "iteration 16429, loss: 0.0014558837283402681\n",
      "iteration 16430, loss: 0.001261935569345951\n",
      "iteration 16431, loss: 0.0012317977380007505\n",
      "iteration 16432, loss: 0.0014382874360308051\n",
      "iteration 16433, loss: 0.0014099380932748318\n",
      "iteration 16434, loss: 0.0013803043402731419\n",
      "iteration 16435, loss: 0.0012689030263572931\n",
      "iteration 16436, loss: 0.0012370569165796041\n",
      "iteration 16437, loss: 0.0013143345713615417\n",
      "iteration 16438, loss: 0.0017634874675422907\n",
      "iteration 16439, loss: 0.0012235317844897509\n",
      "iteration 16440, loss: 0.0011817244812846184\n",
      "iteration 16441, loss: 0.0015737476060166955\n",
      "iteration 16442, loss: 0.001645565964281559\n",
      "iteration 16443, loss: 0.0013906043022871017\n",
      "iteration 16444, loss: 0.0014072495978325605\n",
      "iteration 16445, loss: 0.0014183155726641417\n",
      "iteration 16446, loss: 0.0013268714537844062\n",
      "iteration 16447, loss: 0.001406477065756917\n",
      "iteration 16448, loss: 0.0012231485452502966\n",
      "iteration 16449, loss: 0.001334591070190072\n",
      "iteration 16450, loss: 0.001422450179234147\n",
      "iteration 16451, loss: 0.0014844323741272092\n",
      "iteration 16452, loss: 0.0016168353613466024\n",
      "iteration 16453, loss: 0.0013086242834106088\n",
      "iteration 16454, loss: 0.0011872360482811928\n",
      "iteration 16455, loss: 0.001409187214449048\n",
      "iteration 16456, loss: 0.0014974732184782624\n",
      "iteration 16457, loss: 0.001216971897520125\n",
      "iteration 16458, loss: 0.001155172474682331\n",
      "iteration 16459, loss: 0.001474099582992494\n",
      "iteration 16460, loss: 0.001782208913937211\n",
      "iteration 16461, loss: 0.001375551801174879\n",
      "iteration 16462, loss: 0.00144219771027565\n",
      "iteration 16463, loss: 0.0012238502968102694\n",
      "iteration 16464, loss: 0.0011552309151738882\n",
      "iteration 16465, loss: 0.0011648095678538084\n",
      "iteration 16466, loss: 0.0014522677520290017\n",
      "iteration 16467, loss: 0.0014557025860995054\n",
      "iteration 16468, loss: 0.0015706582926213741\n",
      "iteration 16469, loss: 0.001545442035421729\n",
      "iteration 16470, loss: 0.0014338598120957613\n",
      "iteration 16471, loss: 0.001501955557614565\n",
      "iteration 16472, loss: 0.0013927711406722665\n",
      "iteration 16473, loss: 0.0018588420934975147\n",
      "iteration 16474, loss: 0.002004950540140271\n",
      "iteration 16475, loss: 0.0015387104358524084\n",
      "iteration 16476, loss: 0.0013348637148737907\n",
      "iteration 16477, loss: 0.0014939610846340656\n",
      "iteration 16478, loss: 0.0013595859054476023\n",
      "iteration 16479, loss: 0.0014978141989558935\n",
      "iteration 16480, loss: 0.0020408800337463617\n",
      "iteration 16481, loss: 0.0012908180942758918\n",
      "iteration 16482, loss: 0.0019251841586083174\n",
      "iteration 16483, loss: 0.001648251200094819\n",
      "iteration 16484, loss: 0.0012638481566682458\n",
      "iteration 16485, loss: 0.001093589817173779\n",
      "iteration 16486, loss: 0.001604075776413083\n",
      "iteration 16487, loss: 0.0013420763425529003\n",
      "iteration 16488, loss: 0.0011857678182423115\n",
      "iteration 16489, loss: 0.001468525268137455\n",
      "iteration 16490, loss: 0.001416813232935965\n",
      "iteration 16491, loss: 0.0013680916745215654\n",
      "iteration 16492, loss: 0.0014183877501636744\n",
      "iteration 16493, loss: 0.0013504123780876398\n",
      "iteration 16494, loss: 0.0013382381293922663\n",
      "iteration 16495, loss: 0.0014279806055128574\n",
      "iteration 16496, loss: 0.0012234699679538608\n",
      "iteration 16497, loss: 0.001372199272736907\n",
      "iteration 16498, loss: 0.0014299482572823763\n",
      "iteration 16499, loss: 0.0011375504545867443\n",
      "iteration 16500, loss: 0.0011987581383436918\n",
      "iteration 16501, loss: 0.0012410059571266174\n",
      "iteration 16502, loss: 0.0015925945481285453\n",
      "iteration 16503, loss: 0.0012977513251826167\n",
      "iteration 16504, loss: 0.0013376635033637285\n",
      "iteration 16505, loss: 0.0012804936850443482\n",
      "iteration 16506, loss: 0.0011110655032098293\n",
      "iteration 16507, loss: 0.0013680565170943737\n",
      "iteration 16508, loss: 0.0014031000901013613\n",
      "iteration 16509, loss: 0.001304352073930204\n",
      "iteration 16510, loss: 0.001396320411004126\n",
      "iteration 16511, loss: 0.0012528111692517996\n",
      "iteration 16512, loss: 0.0013789772056043148\n",
      "iteration 16513, loss: 0.0013530247379094362\n",
      "iteration 16514, loss: 0.0015376706141978502\n",
      "iteration 16515, loss: 0.0013352264650166035\n",
      "iteration 16516, loss: 0.0014683795161545277\n",
      "iteration 16517, loss: 0.0012708636932075024\n",
      "iteration 16518, loss: 0.0017250697128474712\n",
      "iteration 16519, loss: 0.0012936305720359087\n",
      "iteration 16520, loss: 0.0013070332352072\n",
      "iteration 16521, loss: 0.0013825464993715286\n",
      "iteration 16522, loss: 0.001289303065277636\n",
      "iteration 16523, loss: 0.0014516895171254873\n",
      "iteration 16524, loss: 0.0010223968420177698\n",
      "iteration 16525, loss: 0.0013458714820444584\n",
      "iteration 16526, loss: 0.001302225049585104\n",
      "iteration 16527, loss: 0.0013051893329247832\n",
      "iteration 16528, loss: 0.0015698946081101894\n",
      "iteration 16529, loss: 0.001196188386529684\n",
      "iteration 16530, loss: 0.0011798234190791845\n",
      "iteration 16531, loss: 0.0013634685892611742\n",
      "iteration 16532, loss: 0.0012321877293288708\n",
      "iteration 16533, loss: 0.0016345883486792445\n",
      "iteration 16534, loss: 0.0014603105373680592\n",
      "iteration 16535, loss: 0.0013270406052470207\n",
      "iteration 16536, loss: 0.0011928138555958867\n",
      "iteration 16537, loss: 0.001293044537305832\n",
      "iteration 16538, loss: 0.001295828027650714\n",
      "iteration 16539, loss: 0.0013929724227637053\n",
      "iteration 16540, loss: 0.0016233535716310143\n",
      "iteration 16541, loss: 0.001227189670316875\n",
      "iteration 16542, loss: 0.001385426614433527\n",
      "iteration 16543, loss: 0.0012831399217247963\n",
      "iteration 16544, loss: 0.0016530496068298817\n",
      "iteration 16545, loss: 0.0016453068237751722\n",
      "iteration 16546, loss: 0.0015222334768623114\n",
      "iteration 16547, loss: 0.0010655089281499386\n",
      "iteration 16548, loss: 0.0015096404822543263\n",
      "iteration 16549, loss: 0.0015599748585373163\n",
      "iteration 16550, loss: 0.0016746653709560633\n",
      "iteration 16551, loss: 0.0011102986754849553\n",
      "iteration 16552, loss: 0.0014644914772361517\n",
      "iteration 16553, loss: 0.0012134084245190024\n",
      "iteration 16554, loss: 0.0012566563673317432\n",
      "iteration 16555, loss: 0.0014919384848326445\n",
      "iteration 16556, loss: 0.0012787916930392385\n",
      "iteration 16557, loss: 0.0012019241694360971\n",
      "iteration 16558, loss: 0.0012306333519518375\n",
      "iteration 16559, loss: 0.0011367637198418379\n",
      "iteration 16560, loss: 0.0014427019050344825\n",
      "iteration 16561, loss: 0.0015420536510646343\n",
      "iteration 16562, loss: 0.0013969943393021822\n",
      "iteration 16563, loss: 0.001463835476897657\n",
      "iteration 16564, loss: 0.0016564937541261315\n",
      "iteration 16565, loss: 0.001595228211954236\n",
      "iteration 16566, loss: 0.001266987295821309\n",
      "iteration 16567, loss: 0.0013343098107725382\n",
      "iteration 16568, loss: 0.001331547973677516\n",
      "iteration 16569, loss: 0.00121679634321481\n",
      "iteration 16570, loss: 0.001415290404111147\n",
      "iteration 16571, loss: 0.001462097279727459\n",
      "iteration 16572, loss: 0.0010965336114168167\n",
      "iteration 16573, loss: 0.001270232256501913\n",
      "iteration 16574, loss: 0.0012857015244662762\n",
      "iteration 16575, loss: 0.0012893585953861475\n",
      "iteration 16576, loss: 0.0011383863165974617\n",
      "iteration 16577, loss: 0.0013845410430803895\n",
      "iteration 16578, loss: 0.0012854887172579765\n",
      "iteration 16579, loss: 0.0011416308116167784\n",
      "iteration 16580, loss: 0.0013869295362383127\n",
      "iteration 16581, loss: 0.0014005652628839016\n",
      "iteration 16582, loss: 0.0013102602679282427\n",
      "iteration 16583, loss: 0.0013168016448616982\n",
      "iteration 16584, loss: 0.0013096585171297193\n",
      "iteration 16585, loss: 0.0013512871228158474\n",
      "iteration 16586, loss: 0.001312000211328268\n",
      "iteration 16587, loss: 0.0016961960354819894\n",
      "iteration 16588, loss: 0.001550582586787641\n",
      "iteration 16589, loss: 0.0013664650032296777\n",
      "iteration 16590, loss: 0.001332661951892078\n",
      "iteration 16591, loss: 0.0013518317136913538\n",
      "iteration 16592, loss: 0.0012198694748803973\n",
      "iteration 16593, loss: 0.0015181726776063442\n",
      "iteration 16594, loss: 0.0017303808126598597\n",
      "iteration 16595, loss: 0.001503936480730772\n",
      "iteration 16596, loss: 0.0012700618244707584\n",
      "iteration 16597, loss: 0.0013681930722668767\n",
      "iteration 16598, loss: 0.001132019329816103\n",
      "iteration 16599, loss: 0.0012193607399240136\n",
      "iteration 16600, loss: 0.0015113487606868148\n",
      "iteration 16601, loss: 0.001552532077766955\n",
      "iteration 16602, loss: 0.0013597989454865456\n",
      "iteration 16603, loss: 0.0017852510791271925\n",
      "iteration 16604, loss: 0.0013667531311511993\n",
      "iteration 16605, loss: 0.0012987235095351934\n",
      "iteration 16606, loss: 0.0015252982266247272\n",
      "iteration 16607, loss: 0.0009835244854912162\n",
      "iteration 16608, loss: 0.0016697278479114175\n",
      "iteration 16609, loss: 0.0014059452805668116\n",
      "iteration 16610, loss: 0.0012082129251211882\n",
      "iteration 16611, loss: 0.0014192467788234353\n",
      "iteration 16612, loss: 0.0010688964975997806\n",
      "iteration 16613, loss: 0.0011070969048887491\n",
      "iteration 16614, loss: 0.0010250234045088291\n",
      "iteration 16615, loss: 0.0013543039094656706\n",
      "iteration 16616, loss: 0.0014426381094381213\n",
      "iteration 16617, loss: 0.0013842503540217876\n",
      "iteration 16618, loss: 0.0011804425157606602\n",
      "iteration 16619, loss: 0.001104176975786686\n",
      "iteration 16620, loss: 0.0015982750337570906\n",
      "iteration 16621, loss: 0.0013621372636407614\n",
      "iteration 16622, loss: 0.0015369716566056013\n",
      "iteration 16623, loss: 0.00160818244330585\n",
      "iteration 16624, loss: 0.0013503024820238352\n",
      "iteration 16625, loss: 0.0014506315346807241\n",
      "iteration 16626, loss: 0.0011774145532399416\n",
      "iteration 16627, loss: 0.001189370988868177\n",
      "iteration 16628, loss: 0.001217791810631752\n",
      "iteration 16629, loss: 0.0015860549174249172\n",
      "iteration 16630, loss: 0.0012486728373914957\n",
      "iteration 16631, loss: 0.0011203984031453729\n",
      "iteration 16632, loss: 0.0012006836477667093\n",
      "iteration 16633, loss: 0.0011114247608929873\n",
      "iteration 16634, loss: 0.0011934314388781786\n",
      "iteration 16635, loss: 0.0012582391500473022\n",
      "iteration 16636, loss: 0.0012783203274011612\n",
      "iteration 16637, loss: 0.0011693047126755118\n",
      "iteration 16638, loss: 0.0011827547568827868\n",
      "iteration 16639, loss: 0.001275181770324707\n",
      "iteration 16640, loss: 0.0011694824788719416\n",
      "iteration 16641, loss: 0.0013960858341306448\n",
      "iteration 16642, loss: 0.001230464200489223\n",
      "iteration 16643, loss: 0.0013789244694635272\n",
      "iteration 16644, loss: 0.0013727882178500295\n",
      "iteration 16645, loss: 0.001356479711830616\n",
      "iteration 16646, loss: 0.0014055505162104964\n",
      "iteration 16647, loss: 0.001377068692818284\n",
      "iteration 16648, loss: 0.0011884907726198435\n",
      "iteration 16649, loss: 0.001435928395949304\n",
      "iteration 16650, loss: 0.0010911659337580204\n",
      "iteration 16651, loss: 0.0011584931053221226\n",
      "iteration 16652, loss: 0.0011340740602463484\n",
      "iteration 16653, loss: 0.001224386040121317\n",
      "iteration 16654, loss: 0.0013792826794087887\n",
      "iteration 16655, loss: 0.0012232842855155468\n",
      "iteration 16656, loss: 0.0016274936497211456\n",
      "iteration 16657, loss: 0.0013084009988233447\n",
      "iteration 16658, loss: 0.0009649242856539786\n",
      "iteration 16659, loss: 0.0009872636292129755\n",
      "iteration 16660, loss: 0.0011487678857520223\n",
      "iteration 16661, loss: 0.0009792924392968416\n",
      "iteration 16662, loss: 0.0013275840319693089\n",
      "iteration 16663, loss: 0.0015258851926773787\n",
      "iteration 16664, loss: 0.0012901690788567066\n",
      "iteration 16665, loss: 0.0011229707160964608\n",
      "iteration 16666, loss: 0.0011467505246400833\n",
      "iteration 16667, loss: 0.0011848476715385914\n",
      "iteration 16668, loss: 0.0010673098731786013\n",
      "iteration 16669, loss: 0.0013000675244256854\n",
      "iteration 16670, loss: 0.0011931773042306304\n",
      "iteration 16671, loss: 0.0011404725955799222\n",
      "iteration 16672, loss: 0.001375923166051507\n",
      "iteration 16673, loss: 0.0016876638401299715\n",
      "iteration 16674, loss: 0.0011438899673521519\n",
      "iteration 16675, loss: 0.0011066868901252747\n",
      "iteration 16676, loss: 0.0014036365319043398\n",
      "iteration 16677, loss: 0.0014852398307994008\n",
      "iteration 16678, loss: 0.0013883741339668632\n",
      "iteration 16679, loss: 0.0012621082132682204\n",
      "iteration 16680, loss: 0.00128950085490942\n",
      "iteration 16681, loss: 0.0013449336402118206\n",
      "iteration 16682, loss: 0.001303875120356679\n",
      "iteration 16683, loss: 0.0013429713435471058\n",
      "iteration 16684, loss: 0.0012675401521846652\n",
      "iteration 16685, loss: 0.0010253076907247305\n",
      "iteration 16686, loss: 0.001812082133255899\n",
      "iteration 16687, loss: 0.0012988776434212923\n",
      "iteration 16688, loss: 0.0012305369600653648\n",
      "iteration 16689, loss: 0.0013128893915563822\n",
      "iteration 16690, loss: 0.001223843777552247\n",
      "iteration 16691, loss: 0.0013825948117300868\n",
      "iteration 16692, loss: 0.0012156274169683456\n",
      "iteration 16693, loss: 0.0013047188986092806\n",
      "iteration 16694, loss: 0.0013651613844558597\n",
      "iteration 16695, loss: 0.0014524159487336874\n",
      "iteration 16696, loss: 0.0014543943107128143\n",
      "iteration 16697, loss: 0.0013742148876190186\n",
      "iteration 16698, loss: 0.001501864055171609\n",
      "iteration 16699, loss: 0.0014546967577189207\n",
      "iteration 16700, loss: 0.0012424045708030462\n",
      "iteration 16701, loss: 0.0014692828990519047\n",
      "iteration 16702, loss: 0.0012926022754982114\n",
      "iteration 16703, loss: 0.0013316369149833918\n",
      "iteration 16704, loss: 0.0014763842336833477\n",
      "iteration 16705, loss: 0.0012742899125441909\n",
      "iteration 16706, loss: 0.0016000354662537575\n",
      "iteration 16707, loss: 0.0014222681056708097\n",
      "iteration 16708, loss: 0.0012477678246796131\n",
      "iteration 16709, loss: 0.0013220289256423712\n",
      "iteration 16710, loss: 0.0013222198467701674\n",
      "iteration 16711, loss: 0.0013222674606367946\n",
      "iteration 16712, loss: 0.0012566791847348213\n",
      "iteration 16713, loss: 0.0011613962706178427\n",
      "iteration 16714, loss: 0.0013767103664577007\n",
      "iteration 16715, loss: 0.0012352882185950875\n",
      "iteration 16716, loss: 0.0012947091599926353\n",
      "iteration 16717, loss: 0.001121064997278154\n",
      "iteration 16718, loss: 0.001213560113683343\n",
      "iteration 16719, loss: 0.001568180276080966\n",
      "iteration 16720, loss: 0.0011846029665321112\n",
      "iteration 16721, loss: 0.001034609042108059\n",
      "iteration 16722, loss: 0.0012097079306840897\n",
      "iteration 16723, loss: 0.001201916136778891\n",
      "iteration 16724, loss: 0.0012465034378692508\n",
      "iteration 16725, loss: 0.0014509634347632527\n",
      "iteration 16726, loss: 0.0012912688544020057\n",
      "iteration 16727, loss: 0.0017388936830684543\n",
      "iteration 16728, loss: 0.0010609312448650599\n",
      "iteration 16729, loss: 0.0013296243268996477\n",
      "iteration 16730, loss: 0.001087757060304284\n",
      "iteration 16731, loss: 0.0014064591377973557\n",
      "iteration 16732, loss: 0.0012465637410059571\n",
      "iteration 16733, loss: 0.0014533764915540814\n",
      "iteration 16734, loss: 0.0012962501496076584\n",
      "iteration 16735, loss: 0.0014893754851073027\n",
      "iteration 16736, loss: 0.0012721586972475052\n",
      "iteration 16737, loss: 0.001472741598263383\n",
      "iteration 16738, loss: 0.0014984161825850606\n",
      "iteration 16739, loss: 0.001186041859909892\n",
      "iteration 16740, loss: 0.0013886411907151341\n",
      "iteration 16741, loss: 0.0010951876174658537\n",
      "iteration 16742, loss: 0.001344246556982398\n",
      "iteration 16743, loss: 0.0012838684488087893\n",
      "iteration 16744, loss: 0.0012067976640537381\n",
      "iteration 16745, loss: 0.00117995566688478\n",
      "iteration 16746, loss: 0.0011132943909615278\n",
      "iteration 16747, loss: 0.0013958060881122947\n",
      "iteration 16748, loss: 0.00126843701582402\n",
      "iteration 16749, loss: 0.0012031167279928923\n",
      "iteration 16750, loss: 0.0014261696487665176\n",
      "iteration 16751, loss: 0.0012603944633156061\n",
      "iteration 16752, loss: 0.0013491297140717506\n",
      "iteration 16753, loss: 0.0012631069403141737\n",
      "iteration 16754, loss: 0.0009994952706620097\n",
      "iteration 16755, loss: 0.0012001823633909225\n",
      "iteration 16756, loss: 0.0010686126770451665\n",
      "iteration 16757, loss: 0.001284469966776669\n",
      "iteration 16758, loss: 0.0015873154625296593\n",
      "iteration 16759, loss: 0.0013222857378423214\n",
      "iteration 16760, loss: 0.001226062886416912\n",
      "iteration 16761, loss: 0.0012865627650171518\n",
      "iteration 16762, loss: 0.001359785906970501\n",
      "iteration 16763, loss: 0.0010798326693475246\n",
      "iteration 16764, loss: 0.0011124277953058481\n",
      "iteration 16765, loss: 0.0013222219422459602\n",
      "iteration 16766, loss: 0.0012239485513418913\n",
      "iteration 16767, loss: 0.0012254504254087806\n",
      "iteration 16768, loss: 0.0014492367627099156\n",
      "iteration 16769, loss: 0.0011857536155730486\n",
      "iteration 16770, loss: 0.00112923514097929\n",
      "iteration 16771, loss: 0.0011826384579762816\n",
      "iteration 16772, loss: 0.0012415102683007717\n",
      "iteration 16773, loss: 0.0011699663009494543\n",
      "iteration 16774, loss: 0.001206833403557539\n",
      "iteration 16775, loss: 0.0013720440911129117\n",
      "iteration 16776, loss: 0.001230380148626864\n",
      "iteration 16777, loss: 0.0013114393223077059\n",
      "iteration 16778, loss: 0.0015503597678616643\n",
      "iteration 16779, loss: 0.0011447526048868895\n",
      "iteration 16780, loss: 0.0012932781828567386\n",
      "iteration 16781, loss: 0.0013265677262097597\n",
      "iteration 16782, loss: 0.0012929250951856375\n",
      "iteration 16783, loss: 0.0012432613875716925\n",
      "iteration 16784, loss: 0.0010726130567491055\n",
      "iteration 16785, loss: 0.0016310333739966154\n",
      "iteration 16786, loss: 0.0010900050401687622\n",
      "iteration 16787, loss: 0.0014290474355220795\n",
      "iteration 16788, loss: 0.0010336406994611025\n",
      "iteration 16789, loss: 0.0012000754941254854\n",
      "iteration 16790, loss: 0.0012995573924854398\n",
      "iteration 16791, loss: 0.0013222855050116777\n",
      "iteration 16792, loss: 0.0014204011531546712\n",
      "iteration 16793, loss: 0.001280604861676693\n",
      "iteration 16794, loss: 0.0010392576223239303\n",
      "iteration 16795, loss: 0.0011796022299677134\n",
      "iteration 16796, loss: 0.0013160353992134333\n",
      "iteration 16797, loss: 0.0009822775609791279\n",
      "iteration 16798, loss: 0.001060138689354062\n",
      "iteration 16799, loss: 0.0012601724592968822\n",
      "iteration 16800, loss: 0.001314951223321259\n",
      "iteration 16801, loss: 0.0012382245622575283\n",
      "iteration 16802, loss: 0.0010681765852496028\n",
      "iteration 16803, loss: 0.0014856104971840978\n",
      "iteration 16804, loss: 0.0011480071116238832\n",
      "iteration 16805, loss: 0.0013547567650675774\n",
      "iteration 16806, loss: 0.001453356584534049\n",
      "iteration 16807, loss: 0.0015537638682872057\n",
      "iteration 16808, loss: 0.0012750474270433187\n",
      "iteration 16809, loss: 0.0013754961546510458\n",
      "iteration 16810, loss: 0.0013745911419391632\n",
      "iteration 16811, loss: 0.0012613122817128897\n",
      "iteration 16812, loss: 0.0012831512140110135\n",
      "iteration 16813, loss: 0.0015774019993841648\n",
      "iteration 16814, loss: 0.0013551919255405664\n",
      "iteration 16815, loss: 0.0012953882105648518\n",
      "iteration 16816, loss: 0.0010242881253361702\n",
      "iteration 16817, loss: 0.001318244612775743\n",
      "iteration 16818, loss: 0.001042020507156849\n",
      "iteration 16819, loss: 0.0012803629506379366\n",
      "iteration 16820, loss: 0.001637711189687252\n",
      "iteration 16821, loss: 0.0011045915307477117\n",
      "iteration 16822, loss: 0.001558821415528655\n",
      "iteration 16823, loss: 0.0013195087667554617\n",
      "iteration 16824, loss: 0.0013554263859987259\n",
      "iteration 16825, loss: 0.001180443912744522\n",
      "iteration 16826, loss: 0.0014138494152575731\n",
      "iteration 16827, loss: 0.001532656722702086\n",
      "iteration 16828, loss: 0.0014887816505506635\n",
      "iteration 16829, loss: 0.001549535896629095\n",
      "iteration 16830, loss: 0.0012034544488415122\n",
      "iteration 16831, loss: 0.0014285852666944265\n",
      "iteration 16832, loss: 0.0011557887773960829\n",
      "iteration 16833, loss: 0.0015158102614805102\n",
      "iteration 16834, loss: 0.0013388448860496283\n",
      "iteration 16835, loss: 0.001439346931874752\n",
      "iteration 16836, loss: 0.001178464270196855\n",
      "iteration 16837, loss: 0.0013452740386128426\n",
      "iteration 16838, loss: 0.0011324286460876465\n",
      "iteration 16839, loss: 0.0013485942035913467\n",
      "iteration 16840, loss: 0.0012126509100198746\n",
      "iteration 16841, loss: 0.0013126835692673922\n",
      "iteration 16842, loss: 0.0013568461872637272\n",
      "iteration 16843, loss: 0.0014213637914508581\n",
      "iteration 16844, loss: 0.001508096931502223\n",
      "iteration 16845, loss: 0.0014221533201634884\n",
      "iteration 16846, loss: 0.0013458931352943182\n",
      "iteration 16847, loss: 0.0011824374087154865\n",
      "iteration 16848, loss: 0.0013544601388275623\n",
      "iteration 16849, loss: 0.0012608810793608427\n",
      "iteration 16850, loss: 0.0012648091651499271\n",
      "iteration 16851, loss: 0.0011958092218264937\n",
      "iteration 16852, loss: 0.0014551846543326974\n",
      "iteration 16853, loss: 0.001631840830668807\n",
      "iteration 16854, loss: 0.0012095377314835787\n",
      "iteration 16855, loss: 0.0011522757122293115\n",
      "iteration 16856, loss: 0.0014463334809988737\n",
      "iteration 16857, loss: 0.0013819988816976547\n",
      "iteration 16858, loss: 0.0011652642861008644\n",
      "iteration 16859, loss: 0.0012845247983932495\n",
      "iteration 16860, loss: 0.0013466367963701487\n",
      "iteration 16861, loss: 0.0014911903999745846\n",
      "iteration 16862, loss: 0.0012886634794995189\n",
      "iteration 16863, loss: 0.0011370002757757902\n",
      "iteration 16864, loss: 0.0014144249726086855\n",
      "iteration 16865, loss: 0.0014113474171608686\n",
      "iteration 16866, loss: 0.0014838951174169779\n",
      "iteration 16867, loss: 0.0011238624574616551\n",
      "iteration 16868, loss: 0.001563406316563487\n",
      "iteration 16869, loss: 0.0013725028838962317\n",
      "iteration 16870, loss: 0.0011427372228354216\n",
      "iteration 16871, loss: 0.0014942842535674572\n",
      "iteration 16872, loss: 0.0015117088332772255\n",
      "iteration 16873, loss: 0.0014245586935430765\n",
      "iteration 16874, loss: 0.0013908431865274906\n",
      "iteration 16875, loss: 0.0013877474702894688\n",
      "iteration 16876, loss: 0.0010336311534047127\n",
      "iteration 16877, loss: 0.0011593493400141597\n",
      "iteration 16878, loss: 0.0013397806324064732\n",
      "iteration 16879, loss: 0.001389272860251367\n",
      "iteration 16880, loss: 0.0015904181636869907\n",
      "iteration 16881, loss: 0.0011691993568092585\n",
      "iteration 16882, loss: 0.0013615534408017993\n",
      "iteration 16883, loss: 0.001329285791143775\n",
      "iteration 16884, loss: 0.0016004365170374513\n",
      "iteration 16885, loss: 0.0015037518460303545\n",
      "iteration 16886, loss: 0.0011876702774316072\n",
      "iteration 16887, loss: 0.0015552106779068708\n",
      "iteration 16888, loss: 0.0013510816497728229\n",
      "iteration 16889, loss: 0.0011993220541626215\n",
      "iteration 16890, loss: 0.0015005414607003331\n",
      "iteration 16891, loss: 0.0017491958569735289\n",
      "iteration 16892, loss: 0.0013022879138588905\n",
      "iteration 16893, loss: 0.0013306839391589165\n",
      "iteration 16894, loss: 0.0015667869010940194\n",
      "iteration 16895, loss: 0.0014424757100641727\n",
      "iteration 16896, loss: 0.0017193254316225648\n",
      "iteration 16897, loss: 0.0013270886847749352\n",
      "iteration 16898, loss: 0.0013506977120414376\n",
      "iteration 16899, loss: 0.001259020995348692\n",
      "iteration 16900, loss: 0.0015459442511200905\n",
      "iteration 16901, loss: 0.0012554679997265339\n",
      "iteration 16902, loss: 0.001592360669746995\n",
      "iteration 16903, loss: 0.0012649314012378454\n",
      "iteration 16904, loss: 0.0014348450349643826\n",
      "iteration 16905, loss: 0.0013523814268410206\n",
      "iteration 16906, loss: 0.0014394968748092651\n",
      "iteration 16907, loss: 0.0014769479166716337\n",
      "iteration 16908, loss: 0.0012140448670834303\n",
      "iteration 16909, loss: 0.0014360402710735798\n",
      "iteration 16910, loss: 0.0014197237323969603\n",
      "iteration 16911, loss: 0.0014275769935920835\n",
      "iteration 16912, loss: 0.0014909268356859684\n",
      "iteration 16913, loss: 0.0013056006282567978\n",
      "iteration 16914, loss: 0.0015739335212856531\n",
      "iteration 16915, loss: 0.0014107818715274334\n",
      "iteration 16916, loss: 0.0015236255712807178\n",
      "iteration 16917, loss: 0.0018465023022145033\n",
      "iteration 16918, loss: 0.001388794626109302\n",
      "iteration 16919, loss: 0.001208535861223936\n",
      "iteration 16920, loss: 0.0013438831083476543\n",
      "iteration 16921, loss: 0.0012962252367287874\n",
      "iteration 16922, loss: 0.001372748170979321\n",
      "iteration 16923, loss: 0.0017094665672630072\n",
      "iteration 16924, loss: 0.0014129579067230225\n",
      "iteration 16925, loss: 0.0018699748907238245\n",
      "iteration 16926, loss: 0.0012937840074300766\n",
      "iteration 16927, loss: 0.0014702121261507273\n",
      "iteration 16928, loss: 0.0013564067194238305\n",
      "iteration 16929, loss: 0.0015286742709577084\n",
      "iteration 16930, loss: 0.0013902619248256087\n",
      "iteration 16931, loss: 0.001498212805017829\n",
      "iteration 16932, loss: 0.0012936288258060813\n",
      "iteration 16933, loss: 0.00143117131665349\n",
      "iteration 16934, loss: 0.0012032787781208754\n",
      "iteration 16935, loss: 0.001486054970882833\n",
      "iteration 16936, loss: 0.0015836602542549372\n",
      "iteration 16937, loss: 0.0012974832206964493\n",
      "iteration 16938, loss: 0.001624644035473466\n",
      "iteration 16939, loss: 0.0012685451656579971\n",
      "iteration 16940, loss: 0.0014355286257341504\n",
      "iteration 16941, loss: 0.0013373452238738537\n",
      "iteration 16942, loss: 0.0012285895645618439\n",
      "iteration 16943, loss: 0.0013654606882482767\n",
      "iteration 16944, loss: 0.0010855842847377062\n",
      "iteration 16945, loss: 0.0012951251119375229\n",
      "iteration 16946, loss: 0.0016650853212922812\n",
      "iteration 16947, loss: 0.0012113729026168585\n",
      "iteration 16948, loss: 0.001336916582658887\n",
      "iteration 16949, loss: 0.001154449419118464\n",
      "iteration 16950, loss: 0.0012201644713059068\n",
      "iteration 16951, loss: 0.0012378846295177937\n",
      "iteration 16952, loss: 0.001459430088289082\n",
      "iteration 16953, loss: 0.0012747776927426457\n",
      "iteration 16954, loss: 0.0015465449541807175\n",
      "iteration 16955, loss: 0.0014287005178630352\n",
      "iteration 16956, loss: 0.0012957181315869093\n",
      "iteration 16957, loss: 0.0014475340722128749\n",
      "iteration 16958, loss: 0.0011783505324274302\n",
      "iteration 16959, loss: 0.0016207827720791101\n",
      "iteration 16960, loss: 0.001301573938690126\n",
      "iteration 16961, loss: 0.0015409068437293172\n",
      "iteration 16962, loss: 0.001480382401496172\n",
      "iteration 16963, loss: 0.001455588499084115\n",
      "iteration 16964, loss: 0.0012642210349440575\n",
      "iteration 16965, loss: 0.0013401603791862726\n",
      "iteration 16966, loss: 0.0013773621758446097\n",
      "iteration 16967, loss: 0.0011644024634733796\n",
      "iteration 16968, loss: 0.001257655443623662\n",
      "iteration 16969, loss: 0.0017273164121434093\n",
      "iteration 16970, loss: 0.0012523718178272247\n",
      "iteration 16971, loss: 0.0010504587553441525\n",
      "iteration 16972, loss: 0.0015750605380162597\n",
      "iteration 16973, loss: 0.0018097630236297846\n",
      "iteration 16974, loss: 0.0012538048904389143\n",
      "iteration 16975, loss: 0.0013115822803229094\n",
      "iteration 16976, loss: 0.0012776812072843313\n",
      "iteration 16977, loss: 0.0014594022650271654\n",
      "iteration 16978, loss: 0.001397985266521573\n",
      "iteration 16979, loss: 0.0010816834401339293\n",
      "iteration 16980, loss: 0.001322546973824501\n",
      "iteration 16981, loss: 0.0012475521070882678\n",
      "iteration 16982, loss: 0.0013520722277462482\n",
      "iteration 16983, loss: 0.0012904583709314466\n",
      "iteration 16984, loss: 0.001542390207760036\n",
      "iteration 16985, loss: 0.0014525786973536015\n",
      "iteration 16986, loss: 0.0012430449714884162\n",
      "iteration 16987, loss: 0.0015464385505765676\n",
      "iteration 16988, loss: 0.0014850145671516657\n",
      "iteration 16989, loss: 0.0011467773001641035\n",
      "iteration 16990, loss: 0.0013564546825364232\n",
      "iteration 16991, loss: 0.001407861476764083\n",
      "iteration 16992, loss: 0.001396401785314083\n",
      "iteration 16993, loss: 0.0013199927052482963\n",
      "iteration 16994, loss: 0.00126247713342309\n",
      "iteration 16995, loss: 0.001571109751239419\n",
      "iteration 16996, loss: 0.0014130978379398584\n",
      "iteration 16997, loss: 0.0013566174311563373\n",
      "iteration 16998, loss: 0.0012005786411464214\n",
      "iteration 16999, loss: 0.001340375398285687\n",
      "iteration 17000, loss: 0.0012697557685896754\n",
      "iteration 17001, loss: 0.0014209849759936333\n",
      "iteration 17002, loss: 0.0013923731166869402\n",
      "iteration 17003, loss: 0.0012128276284784079\n",
      "iteration 17004, loss: 0.001277439994737506\n",
      "iteration 17005, loss: 0.0010732633527368307\n",
      "iteration 17006, loss: 0.001315695932134986\n",
      "iteration 17007, loss: 0.0012342981062829494\n",
      "iteration 17008, loss: 0.0013052591821178794\n",
      "iteration 17009, loss: 0.0014317133463919163\n",
      "iteration 17010, loss: 0.0018260239157825708\n",
      "iteration 17011, loss: 0.0011151426006108522\n",
      "iteration 17012, loss: 0.0018536755815148354\n",
      "iteration 17013, loss: 0.0014570318162441254\n",
      "iteration 17014, loss: 0.001230214606039226\n",
      "iteration 17015, loss: 0.0014890083111822605\n",
      "iteration 17016, loss: 0.0015814779326319695\n",
      "iteration 17017, loss: 0.0014896828215569258\n",
      "iteration 17018, loss: 0.0014869027072563767\n",
      "iteration 17019, loss: 0.0014559247065335512\n",
      "iteration 17020, loss: 0.0016700064297765493\n",
      "iteration 17021, loss: 0.0015479899011552334\n",
      "iteration 17022, loss: 0.0014432776952162385\n",
      "iteration 17023, loss: 0.0014569838531315327\n",
      "iteration 17024, loss: 0.0013025513617321849\n",
      "iteration 17025, loss: 0.0014463728293776512\n",
      "iteration 17026, loss: 0.0015482096932828426\n",
      "iteration 17027, loss: 0.0016096375184133649\n",
      "iteration 17028, loss: 0.0014919841196388006\n",
      "iteration 17029, loss: 0.0013700249837711453\n",
      "iteration 17030, loss: 0.0012384086148813367\n",
      "iteration 17031, loss: 0.0016888543032109737\n",
      "iteration 17032, loss: 0.0013992206659168005\n",
      "iteration 17033, loss: 0.0014528058236464858\n",
      "iteration 17034, loss: 0.0015283952234312892\n",
      "iteration 17035, loss: 0.0013346796622499824\n",
      "iteration 17036, loss: 0.0012311080936342478\n",
      "iteration 17037, loss: 0.0014316230081021786\n",
      "iteration 17038, loss: 0.001494940952397883\n",
      "iteration 17039, loss: 0.0012649896088987589\n",
      "iteration 17040, loss: 0.0013034779112786055\n",
      "iteration 17041, loss: 0.0017507270677015185\n",
      "iteration 17042, loss: 0.001422099070623517\n",
      "iteration 17043, loss: 0.001404631999321282\n",
      "iteration 17044, loss: 0.0014547226019203663\n",
      "iteration 17045, loss: 0.001480037928558886\n",
      "iteration 17046, loss: 0.0014085880247876048\n",
      "iteration 17047, loss: 0.001608910271897912\n",
      "iteration 17048, loss: 0.0013764568138867617\n",
      "iteration 17049, loss: 0.001407185452990234\n",
      "iteration 17050, loss: 0.0015205279923975468\n",
      "iteration 17051, loss: 0.001518971985206008\n",
      "iteration 17052, loss: 0.0014934607315808535\n",
      "iteration 17053, loss: 0.001581047661602497\n",
      "iteration 17054, loss: 0.0015919508878141642\n",
      "iteration 17055, loss: 0.0012838554102927446\n",
      "iteration 17056, loss: 0.0016257016686722636\n",
      "iteration 17057, loss: 0.0012614419683814049\n",
      "iteration 17058, loss: 0.0011768529657274485\n",
      "iteration 17059, loss: 0.001347178127616644\n",
      "iteration 17060, loss: 0.001082992646843195\n",
      "iteration 17061, loss: 0.0012312063481658697\n",
      "iteration 17062, loss: 0.0014395348262041807\n",
      "iteration 17063, loss: 0.0012834067456424236\n",
      "iteration 17064, loss: 0.0019284390145912766\n",
      "iteration 17065, loss: 0.0011604728642851114\n",
      "iteration 17066, loss: 0.0013221377739682794\n",
      "iteration 17067, loss: 0.001053028623573482\n",
      "iteration 17068, loss: 0.0014947287272661924\n",
      "iteration 17069, loss: 0.0013600883539766073\n",
      "iteration 17070, loss: 0.0014106585877016187\n",
      "iteration 17071, loss: 0.0012304577976465225\n",
      "iteration 17072, loss: 0.0011062612757086754\n",
      "iteration 17073, loss: 0.0013041713973507285\n",
      "iteration 17074, loss: 0.0011929592583328485\n",
      "iteration 17075, loss: 0.001259215292520821\n",
      "iteration 17076, loss: 0.0014896341599524021\n",
      "iteration 17077, loss: 0.001024991855956614\n",
      "iteration 17078, loss: 0.002000376582145691\n",
      "iteration 17079, loss: 0.0014165143948048353\n",
      "iteration 17080, loss: 0.00151741411536932\n",
      "iteration 17081, loss: 0.0013128253631293774\n",
      "iteration 17082, loss: 0.0013501509092748165\n",
      "iteration 17083, loss: 0.0013475025771185756\n",
      "iteration 17084, loss: 0.0014089217875152826\n",
      "iteration 17085, loss: 0.0014419581275433302\n",
      "iteration 17086, loss: 0.001160953426733613\n",
      "iteration 17087, loss: 0.001107190502807498\n",
      "iteration 17088, loss: 0.0012245598481968045\n",
      "iteration 17089, loss: 0.001354346051812172\n",
      "iteration 17090, loss: 0.0013852568808943033\n",
      "iteration 17091, loss: 0.0013452563434839249\n",
      "iteration 17092, loss: 0.0012037436245009303\n",
      "iteration 17093, loss: 0.0011711338302120566\n",
      "iteration 17094, loss: 0.0011805418180301785\n",
      "iteration 17095, loss: 0.0010875961743295193\n",
      "iteration 17096, loss: 0.0013749359641224146\n",
      "iteration 17097, loss: 0.001656379085034132\n",
      "iteration 17098, loss: 0.0012799021787941456\n",
      "iteration 17099, loss: 0.0010602414840832353\n",
      "iteration 17100, loss: 0.0013738236157223582\n",
      "iteration 17101, loss: 0.0013117075432091951\n",
      "iteration 17102, loss: 0.0011178008280694485\n",
      "iteration 17103, loss: 0.001203124294988811\n",
      "iteration 17104, loss: 0.001337510533630848\n",
      "iteration 17105, loss: 0.00142126705031842\n",
      "iteration 17106, loss: 0.0010806680656969547\n",
      "iteration 17107, loss: 0.0011656181886792183\n",
      "iteration 17108, loss: 0.0009972882689908147\n",
      "iteration 17109, loss: 0.0012773738708347082\n",
      "iteration 17110, loss: 0.0015042395098134875\n",
      "iteration 17111, loss: 0.0012287416029721498\n",
      "iteration 17112, loss: 0.0010527310660108924\n",
      "iteration 17113, loss: 0.001052901498042047\n",
      "iteration 17114, loss: 0.0011072843335568905\n",
      "iteration 17115, loss: 0.0013809485826641321\n",
      "iteration 17116, loss: 0.0012913134414702654\n",
      "iteration 17117, loss: 0.0013707822654396296\n",
      "iteration 17118, loss: 0.0013309766072779894\n",
      "iteration 17119, loss: 0.0011310481932014227\n",
      "iteration 17120, loss: 0.0013927369145676494\n",
      "iteration 17121, loss: 0.0014861109666526318\n",
      "iteration 17122, loss: 0.0011542446445673704\n",
      "iteration 17123, loss: 0.0017318549798801541\n",
      "iteration 17124, loss: 0.0015154472785070539\n",
      "iteration 17125, loss: 0.000912434421479702\n",
      "iteration 17126, loss: 0.0013536359183490276\n",
      "iteration 17127, loss: 0.0012446838663890958\n",
      "iteration 17128, loss: 0.0015700662042945623\n",
      "iteration 17129, loss: 0.0016692699864506721\n",
      "iteration 17130, loss: 0.001314446097239852\n",
      "iteration 17131, loss: 0.0014171241782605648\n",
      "iteration 17132, loss: 0.0016337534179911017\n",
      "iteration 17133, loss: 0.0011895477073267102\n",
      "iteration 17134, loss: 0.0014117116807028651\n",
      "iteration 17135, loss: 0.001331685809418559\n",
      "iteration 17136, loss: 0.001285614212974906\n",
      "iteration 17137, loss: 0.0014231768436729908\n",
      "iteration 17138, loss: 0.0014507145388051867\n",
      "iteration 17139, loss: 0.0011556518729776144\n",
      "iteration 17140, loss: 0.0012082550674676895\n",
      "iteration 17141, loss: 0.001335024950094521\n",
      "iteration 17142, loss: 0.0012535485439002514\n",
      "iteration 17143, loss: 0.0014317876193672419\n",
      "iteration 17144, loss: 0.0015123303746804595\n",
      "iteration 17145, loss: 0.0013959694188088179\n",
      "iteration 17146, loss: 0.0012631108984351158\n",
      "iteration 17147, loss: 0.0014826833503320813\n",
      "iteration 17148, loss: 0.0013961552176624537\n",
      "iteration 17149, loss: 0.0013120475923642516\n",
      "iteration 17150, loss: 0.00144139863550663\n",
      "iteration 17151, loss: 0.0013972106389701366\n",
      "iteration 17152, loss: 0.001248920219950378\n",
      "iteration 17153, loss: 0.00119233806617558\n",
      "iteration 17154, loss: 0.0011964065488427877\n",
      "iteration 17155, loss: 0.001031734747812152\n",
      "iteration 17156, loss: 0.0014158247504383326\n",
      "iteration 17157, loss: 0.0013386139180511236\n",
      "iteration 17158, loss: 0.001091144047677517\n",
      "iteration 17159, loss: 0.001266509760171175\n",
      "iteration 17160, loss: 0.0011531515046954155\n",
      "iteration 17161, loss: 0.0012765810824930668\n",
      "iteration 17162, loss: 0.001125456765294075\n",
      "iteration 17163, loss: 0.0012660830980166793\n",
      "iteration 17164, loss: 0.0011375377653166652\n",
      "iteration 17165, loss: 0.0011153561063110828\n",
      "iteration 17166, loss: 0.0015417856629937887\n",
      "iteration 17167, loss: 0.0013338024728000164\n",
      "iteration 17168, loss: 0.001329585094936192\n",
      "iteration 17169, loss: 0.0015161435585469007\n",
      "iteration 17170, loss: 0.0012064739130437374\n",
      "iteration 17171, loss: 0.001230896799825132\n",
      "iteration 17172, loss: 0.0011949815088883042\n",
      "iteration 17173, loss: 0.0014335528248921037\n",
      "iteration 17174, loss: 0.0011561978608369827\n",
      "iteration 17175, loss: 0.0011249934323132038\n",
      "iteration 17176, loss: 0.0017303824424743652\n",
      "iteration 17177, loss: 0.0017943333368748426\n",
      "iteration 17178, loss: 0.0013421395560726523\n",
      "iteration 17179, loss: 0.0010914818849414587\n",
      "iteration 17180, loss: 0.00113145902287215\n",
      "iteration 17181, loss: 0.0010802913457155228\n",
      "iteration 17182, loss: 0.0012527488870546222\n",
      "iteration 17183, loss: 0.0010811234824359417\n",
      "iteration 17184, loss: 0.0012570250546559691\n",
      "iteration 17185, loss: 0.0009603442740626633\n",
      "iteration 17186, loss: 0.001362664275802672\n",
      "iteration 17187, loss: 0.0010863824281841516\n",
      "iteration 17188, loss: 0.0011412145104259253\n",
      "iteration 17189, loss: 0.0016860852483659983\n",
      "iteration 17190, loss: 0.001548282103613019\n",
      "iteration 17191, loss: 0.0010907098185271025\n",
      "iteration 17192, loss: 0.0013911679852753878\n",
      "iteration 17193, loss: 0.001223157742060721\n",
      "iteration 17194, loss: 0.0013221267145127058\n",
      "iteration 17195, loss: 0.0011908203596249223\n",
      "iteration 17196, loss: 0.0011660633608698845\n",
      "iteration 17197, loss: 0.0011260114843025804\n",
      "iteration 17198, loss: 0.0016316674882546067\n",
      "iteration 17199, loss: 0.0013570893788710237\n",
      "iteration 17200, loss: 0.001562111428938806\n",
      "iteration 17201, loss: 0.0010961706284433603\n",
      "iteration 17202, loss: 0.001215709955431521\n",
      "iteration 17203, loss: 0.001431059092283249\n",
      "iteration 17204, loss: 0.0010336927371099591\n",
      "iteration 17205, loss: 0.001507103443145752\n",
      "iteration 17206, loss: 0.0013488966505974531\n",
      "iteration 17207, loss: 0.0011940710246562958\n",
      "iteration 17208, loss: 0.00134688476100564\n",
      "iteration 17209, loss: 0.0016488829860463738\n",
      "iteration 17210, loss: 0.0013343128375709057\n",
      "iteration 17211, loss: 0.0011916117509827018\n",
      "iteration 17212, loss: 0.001474146032705903\n",
      "iteration 17213, loss: 0.0014858603244647384\n",
      "iteration 17214, loss: 0.0013368817744776607\n",
      "iteration 17215, loss: 0.001015939051285386\n",
      "iteration 17216, loss: 0.0013884295476600528\n",
      "iteration 17217, loss: 0.0013529750285670161\n",
      "iteration 17218, loss: 0.001373213017359376\n",
      "iteration 17219, loss: 0.0013354073744267225\n",
      "iteration 17220, loss: 0.0012617988977581263\n",
      "iteration 17221, loss: 0.0016712457872927189\n",
      "iteration 17222, loss: 0.001577948802150786\n",
      "iteration 17223, loss: 0.0011562781874090433\n",
      "iteration 17224, loss: 0.0013602364342659712\n",
      "iteration 17225, loss: 0.0009987200610339642\n",
      "iteration 17226, loss: 0.0014815337490290403\n",
      "iteration 17227, loss: 0.0015109475934877992\n",
      "iteration 17228, loss: 0.0013831165852025151\n",
      "iteration 17229, loss: 0.001370748272165656\n",
      "iteration 17230, loss: 0.001184734283015132\n",
      "iteration 17231, loss: 0.0016625996213406324\n",
      "iteration 17232, loss: 0.0015129081439226866\n",
      "iteration 17233, loss: 0.0014628996141254902\n",
      "iteration 17234, loss: 0.0012124202912673354\n",
      "iteration 17235, loss: 0.0012876118998974562\n",
      "iteration 17236, loss: 0.001270944601856172\n",
      "iteration 17237, loss: 0.0010923412628471851\n",
      "iteration 17238, loss: 0.0016742965672165155\n",
      "iteration 17239, loss: 0.001060106442309916\n",
      "iteration 17240, loss: 0.0010618835221976042\n",
      "iteration 17241, loss: 0.0014123732689768076\n",
      "iteration 17242, loss: 0.0011693993583321571\n",
      "iteration 17243, loss: 0.0015155337750911713\n",
      "iteration 17244, loss: 0.0016882636118680239\n",
      "iteration 17245, loss: 0.0016142677050083876\n",
      "iteration 17246, loss: 0.001557965879328549\n",
      "iteration 17247, loss: 0.0015233132289722562\n",
      "iteration 17248, loss: 0.0013985764235258102\n",
      "iteration 17249, loss: 0.001480714650824666\n",
      "iteration 17250, loss: 0.0013114649336785078\n",
      "iteration 17251, loss: 0.0012520963791757822\n",
      "iteration 17252, loss: 0.001363898627460003\n",
      "iteration 17253, loss: 0.0016522123478353024\n",
      "iteration 17254, loss: 0.0011241682805120945\n",
      "iteration 17255, loss: 0.0012096278369426727\n",
      "iteration 17256, loss: 0.0013066113460808992\n",
      "iteration 17257, loss: 0.0011787326075136662\n",
      "iteration 17258, loss: 0.0014332212740555406\n",
      "iteration 17259, loss: 0.0013524973765015602\n",
      "iteration 17260, loss: 0.0013943155063316226\n",
      "iteration 17261, loss: 0.0016980016371235251\n",
      "iteration 17262, loss: 0.0013799371663480997\n",
      "iteration 17263, loss: 0.0014478174271062016\n",
      "iteration 17264, loss: 0.0016411510296165943\n",
      "iteration 17265, loss: 0.001461503910832107\n",
      "iteration 17266, loss: 0.001494385302066803\n",
      "iteration 17267, loss: 0.001114201731979847\n",
      "iteration 17268, loss: 0.0011839257786050439\n",
      "iteration 17269, loss: 0.0012535747373476624\n",
      "iteration 17270, loss: 0.001611740910448134\n",
      "iteration 17271, loss: 0.0014450204325839877\n",
      "iteration 17272, loss: 0.0013012939598411322\n",
      "iteration 17273, loss: 0.0013240856351330876\n",
      "iteration 17274, loss: 0.001461787847802043\n",
      "iteration 17275, loss: 0.0010923458030447364\n",
      "iteration 17276, loss: 0.0015004086308181286\n",
      "iteration 17277, loss: 0.00171720702201128\n",
      "iteration 17278, loss: 0.0013162416871637106\n",
      "iteration 17279, loss: 0.0012738903751596808\n",
      "iteration 17280, loss: 0.001063779229298234\n",
      "iteration 17281, loss: 0.0013139883521944284\n",
      "iteration 17282, loss: 0.001465746434405446\n",
      "iteration 17283, loss: 0.0013775021070614457\n",
      "iteration 17284, loss: 0.0012606356758624315\n",
      "iteration 17285, loss: 0.0013408936792984605\n",
      "iteration 17286, loss: 0.0012736397329717875\n",
      "iteration 17287, loss: 0.0012979917228221893\n",
      "iteration 17288, loss: 0.0013233714271336794\n",
      "iteration 17289, loss: 0.0016517086187377572\n",
      "iteration 17290, loss: 0.0010497987968847156\n",
      "iteration 17291, loss: 0.0011545078596100211\n",
      "iteration 17292, loss: 0.0013441058108583093\n",
      "iteration 17293, loss: 0.001270372187718749\n",
      "iteration 17294, loss: 0.0012720113154500723\n",
      "iteration 17295, loss: 0.0011634042020887136\n",
      "iteration 17296, loss: 0.0015938059659674764\n",
      "iteration 17297, loss: 0.0012396455276757479\n",
      "iteration 17298, loss: 0.0014949229080229998\n",
      "iteration 17299, loss: 0.0012620786437764764\n",
      "iteration 17300, loss: 0.0017330001574009657\n",
      "iteration 17301, loss: 0.0014216455165296793\n",
      "iteration 17302, loss: 0.001471597352065146\n",
      "iteration 17303, loss: 0.0013634180650115013\n",
      "iteration 17304, loss: 0.0012001516297459602\n",
      "iteration 17305, loss: 0.0016568719875067472\n",
      "iteration 17306, loss: 0.00104913383256644\n",
      "iteration 17307, loss: 0.0012738832738250494\n",
      "iteration 17308, loss: 0.0016936580650508404\n",
      "iteration 17309, loss: 0.001734885387122631\n",
      "iteration 17310, loss: 0.001277620205655694\n",
      "iteration 17311, loss: 0.002005770104005933\n",
      "iteration 17312, loss: 0.001574202673509717\n",
      "iteration 17313, loss: 0.001365357544273138\n",
      "iteration 17314, loss: 0.001182708889245987\n",
      "iteration 17315, loss: 0.0013942817458882928\n",
      "iteration 17316, loss: 0.001300597796216607\n",
      "iteration 17317, loss: 0.001186719280667603\n",
      "iteration 17318, loss: 0.0011608358472585678\n",
      "iteration 17319, loss: 0.0010865022195503116\n",
      "iteration 17320, loss: 0.0012937793508172035\n",
      "iteration 17321, loss: 0.001442279084585607\n",
      "iteration 17322, loss: 0.0012639322085306048\n",
      "iteration 17323, loss: 0.0010264449520036578\n",
      "iteration 17324, loss: 0.00120551825966686\n",
      "iteration 17325, loss: 0.0013002068735659122\n",
      "iteration 17326, loss: 0.001362839131616056\n",
      "iteration 17327, loss: 0.0010365615598857403\n",
      "iteration 17328, loss: 0.0011366801336407661\n",
      "iteration 17329, loss: 0.0010278797708451748\n",
      "iteration 17330, loss: 0.001232703449204564\n",
      "iteration 17331, loss: 0.001016985042952001\n",
      "iteration 17332, loss: 0.0015045835170894861\n",
      "iteration 17333, loss: 0.0011786645045503974\n",
      "iteration 17334, loss: 0.001059396774508059\n",
      "iteration 17335, loss: 0.00122350687161088\n",
      "iteration 17336, loss: 0.0012442779261618853\n",
      "iteration 17337, loss: 0.0012724697589874268\n",
      "iteration 17338, loss: 0.0012174976291134953\n",
      "iteration 17339, loss: 0.0011963675497099757\n",
      "iteration 17340, loss: 0.0013823672197759151\n",
      "iteration 17341, loss: 0.0011581488652154803\n",
      "iteration 17342, loss: 0.001087239827029407\n",
      "iteration 17343, loss: 0.0014853233005851507\n",
      "iteration 17344, loss: 0.0011221581371501088\n",
      "iteration 17345, loss: 0.0012151224073022604\n",
      "iteration 17346, loss: 0.0012278985232114792\n",
      "iteration 17347, loss: 0.0014378370251506567\n",
      "iteration 17348, loss: 0.0010503544472157955\n",
      "iteration 17349, loss: 0.0011385632678866386\n",
      "iteration 17350, loss: 0.0012511508539319038\n",
      "iteration 17351, loss: 0.0011697323061525822\n",
      "iteration 17352, loss: 0.0014551961794495583\n",
      "iteration 17353, loss: 0.0011317215394228697\n",
      "iteration 17354, loss: 0.0013415063731372356\n",
      "iteration 17355, loss: 0.0017767077079042792\n",
      "iteration 17356, loss: 0.0012162972707301378\n",
      "iteration 17357, loss: 0.00121210515499115\n",
      "iteration 17358, loss: 0.000976758310571313\n",
      "iteration 17359, loss: 0.0011722660856321454\n",
      "iteration 17360, loss: 0.0013951958389952779\n",
      "iteration 17361, loss: 0.001279839314520359\n",
      "iteration 17362, loss: 0.001398145337589085\n",
      "iteration 17363, loss: 0.0010845200158655643\n",
      "iteration 17364, loss: 0.0011246924987062812\n",
      "iteration 17365, loss: 0.001530150999315083\n",
      "iteration 17366, loss: 0.001179905142635107\n",
      "iteration 17367, loss: 0.0009668968850746751\n",
      "iteration 17368, loss: 0.0012761682737618685\n",
      "iteration 17369, loss: 0.0012823768192902207\n",
      "iteration 17370, loss: 0.001965864561498165\n",
      "iteration 17371, loss: 0.0011405766708776355\n",
      "iteration 17372, loss: 0.0012728391448035836\n",
      "iteration 17373, loss: 0.0010538175702095032\n",
      "iteration 17374, loss: 0.0011412810999900103\n",
      "iteration 17375, loss: 0.001639544265344739\n",
      "iteration 17376, loss: 0.00118088792078197\n",
      "iteration 17377, loss: 0.0015972231049090624\n",
      "iteration 17378, loss: 0.0012817552778869867\n",
      "iteration 17379, loss: 0.001240248209796846\n",
      "iteration 17380, loss: 0.0011659495066851377\n",
      "iteration 17381, loss: 0.0013021428603678942\n",
      "iteration 17382, loss: 0.0012899328721687198\n",
      "iteration 17383, loss: 0.0013767617056146264\n",
      "iteration 17384, loss: 0.0011674643028527498\n",
      "iteration 17385, loss: 0.001817989512346685\n",
      "iteration 17386, loss: 0.0018923291936516762\n",
      "iteration 17387, loss: 0.001296857139095664\n",
      "iteration 17388, loss: 0.0013752111699432135\n",
      "iteration 17389, loss: 0.0013498137705028057\n",
      "iteration 17390, loss: 0.0017014879267662764\n",
      "iteration 17391, loss: 0.0013235111255198717\n",
      "iteration 17392, loss: 0.0013683236902579665\n",
      "iteration 17393, loss: 0.0015772015321999788\n",
      "iteration 17394, loss: 0.0014158571138978004\n",
      "iteration 17395, loss: 0.0019139450741931796\n",
      "iteration 17396, loss: 0.0015455740503966808\n",
      "iteration 17397, loss: 0.0014883799012750387\n",
      "iteration 17398, loss: 0.0015031725633889437\n",
      "iteration 17399, loss: 0.0012556730071082711\n",
      "iteration 17400, loss: 0.001537015661597252\n",
      "iteration 17401, loss: 0.0013661147095263004\n",
      "iteration 17402, loss: 0.0013623738195747137\n",
      "iteration 17403, loss: 0.0014424151740968227\n",
      "iteration 17404, loss: 0.001520861522294581\n",
      "iteration 17405, loss: 0.0012852717190980911\n",
      "iteration 17406, loss: 0.0011714424472302198\n",
      "iteration 17407, loss: 0.0011766433017328382\n",
      "iteration 17408, loss: 0.0010042262729257345\n",
      "iteration 17409, loss: 0.0013030902482569218\n",
      "iteration 17410, loss: 0.001503832172602415\n",
      "iteration 17411, loss: 0.0011659609153866768\n",
      "iteration 17412, loss: 0.0016030003316700459\n",
      "iteration 17413, loss: 0.0010869500692933798\n",
      "iteration 17414, loss: 0.0013090273132547736\n",
      "iteration 17415, loss: 0.0012308894656598568\n",
      "iteration 17416, loss: 0.0012504906626418233\n",
      "iteration 17417, loss: 0.0010853090789169073\n",
      "iteration 17418, loss: 0.0014084773138165474\n",
      "iteration 17419, loss: 0.0014172489754855633\n",
      "iteration 17420, loss: 0.0014885966666042805\n",
      "iteration 17421, loss: 0.0012455318355932832\n",
      "iteration 17422, loss: 0.001279181451536715\n",
      "iteration 17423, loss: 0.001129291020333767\n",
      "iteration 17424, loss: 0.0011612818343564868\n",
      "iteration 17425, loss: 0.0011202145833522081\n",
      "iteration 17426, loss: 0.0013613628689199686\n",
      "iteration 17427, loss: 0.001181512139737606\n",
      "iteration 17428, loss: 0.0009567296365275979\n",
      "iteration 17429, loss: 0.0016092280857264996\n",
      "iteration 17430, loss: 0.0012681244406849146\n",
      "iteration 17431, loss: 0.0013809052761644125\n",
      "iteration 17432, loss: 0.001038956455886364\n",
      "iteration 17433, loss: 0.0011208199430257082\n",
      "iteration 17434, loss: 0.0015384202124550939\n",
      "iteration 17435, loss: 0.0010863007046282291\n",
      "iteration 17436, loss: 0.001406111754477024\n",
      "iteration 17437, loss: 0.0010525246616452932\n",
      "iteration 17438, loss: 0.0011096495436504483\n",
      "iteration 17439, loss: 0.0010761125013232231\n",
      "iteration 17440, loss: 0.0011498862877488136\n",
      "iteration 17441, loss: 0.0012764987768605351\n",
      "iteration 17442, loss: 0.001375153660774231\n",
      "iteration 17443, loss: 0.0012102203909307718\n",
      "iteration 17444, loss: 0.0013527608243748546\n",
      "iteration 17445, loss: 0.0014853846514597535\n",
      "iteration 17446, loss: 0.001276775961741805\n",
      "iteration 17447, loss: 0.0014302919153124094\n",
      "iteration 17448, loss: 0.0012962794862687588\n",
      "iteration 17449, loss: 0.0013169205049052835\n",
      "iteration 17450, loss: 0.001034460961818695\n",
      "iteration 17451, loss: 0.0012014531530439854\n",
      "iteration 17452, loss: 0.0014280115719884634\n",
      "iteration 17453, loss: 0.0014644262846559286\n",
      "iteration 17454, loss: 0.0011042369296774268\n",
      "iteration 17455, loss: 0.001090070465579629\n",
      "iteration 17456, loss: 0.0014351606369018555\n",
      "iteration 17457, loss: 0.0011840630322694778\n",
      "iteration 17458, loss: 0.0012668345589190722\n",
      "iteration 17459, loss: 0.001381403999403119\n",
      "iteration 17460, loss: 0.001114749233238399\n",
      "iteration 17461, loss: 0.001490605529397726\n",
      "iteration 17462, loss: 0.0014827961567789316\n",
      "iteration 17463, loss: 0.0012425457825884223\n",
      "iteration 17464, loss: 0.0011155041866004467\n",
      "iteration 17465, loss: 0.0011979166883975267\n",
      "iteration 17466, loss: 0.0012937781866639853\n",
      "iteration 17467, loss: 0.0012713533360511065\n",
      "iteration 17468, loss: 0.0011780065251514316\n",
      "iteration 17469, loss: 0.0012568916426971555\n",
      "iteration 17470, loss: 0.0011533201904967427\n",
      "iteration 17471, loss: 0.0012685984838753939\n",
      "iteration 17472, loss: 0.0013081117067486048\n",
      "iteration 17473, loss: 0.001297030015848577\n",
      "iteration 17474, loss: 0.0011277084704488516\n",
      "iteration 17475, loss: 0.0014210650697350502\n",
      "iteration 17476, loss: 0.0011907238513231277\n",
      "iteration 17477, loss: 0.0011464905692264438\n",
      "iteration 17478, loss: 0.0011846451088786125\n",
      "iteration 17479, loss: 0.0012302312534302473\n",
      "iteration 17480, loss: 0.0011299109319224954\n",
      "iteration 17481, loss: 0.0012700734660029411\n",
      "iteration 17482, loss: 0.0013626681175082922\n",
      "iteration 17483, loss: 0.0014621928567066789\n",
      "iteration 17484, loss: 0.0012130208779126406\n",
      "iteration 17485, loss: 0.0011807963019236922\n",
      "iteration 17486, loss: 0.0013643207494169474\n",
      "iteration 17487, loss: 0.0011028808075934649\n",
      "iteration 17488, loss: 0.001429485040716827\n",
      "iteration 17489, loss: 0.0010469394037500024\n",
      "iteration 17490, loss: 0.0010929242707788944\n",
      "iteration 17491, loss: 0.001210907706990838\n",
      "iteration 17492, loss: 0.0013265577144920826\n",
      "iteration 17493, loss: 0.001283849822357297\n",
      "iteration 17494, loss: 0.0012357898522168398\n",
      "iteration 17495, loss: 0.0014626176562160254\n",
      "iteration 17496, loss: 0.0011469116434454918\n",
      "iteration 17497, loss: 0.0010012979619204998\n",
      "iteration 17498, loss: 0.0014477935619652271\n",
      "iteration 17499, loss: 0.0014336114982143044\n",
      "iteration 17500, loss: 0.00157125573605299\n",
      "iteration 17501, loss: 0.0012855560053139925\n",
      "iteration 17502, loss: 0.0012565363431349397\n",
      "iteration 17503, loss: 0.0012064563343301415\n",
      "iteration 17504, loss: 0.0011413241736590862\n",
      "iteration 17505, loss: 0.001177153317257762\n",
      "iteration 17506, loss: 0.0015026236651465297\n",
      "iteration 17507, loss: 0.0014475020579993725\n",
      "iteration 17508, loss: 0.0012296705972403288\n",
      "iteration 17509, loss: 0.0011960947886109352\n",
      "iteration 17510, loss: 0.0012158895842731\n",
      "iteration 17511, loss: 0.0013873737771064043\n",
      "iteration 17512, loss: 0.0013578280340880156\n",
      "iteration 17513, loss: 0.0014849735889583826\n",
      "iteration 17514, loss: 0.0014523835852742195\n",
      "iteration 17515, loss: 0.0013499505585059524\n",
      "iteration 17516, loss: 0.0014101064298301935\n",
      "iteration 17517, loss: 0.001364989671856165\n",
      "iteration 17518, loss: 0.0011713790008798242\n",
      "iteration 17519, loss: 0.0011683760676532984\n",
      "iteration 17520, loss: 0.0010273510124534369\n",
      "iteration 17521, loss: 0.001098911976441741\n",
      "iteration 17522, loss: 0.001255947514437139\n",
      "iteration 17523, loss: 0.0012156997108832002\n",
      "iteration 17524, loss: 0.0011655655689537525\n",
      "iteration 17525, loss: 0.0012670350261032581\n",
      "iteration 17526, loss: 0.0016183065017685294\n",
      "iteration 17527, loss: 0.0014580634888261557\n",
      "iteration 17528, loss: 0.0015828211326152086\n",
      "iteration 17529, loss: 0.0016147217247635126\n",
      "iteration 17530, loss: 0.0011198108550161123\n",
      "iteration 17531, loss: 0.0012489936780184507\n",
      "iteration 17532, loss: 0.0011748973047360778\n",
      "iteration 17533, loss: 0.001160214887931943\n",
      "iteration 17534, loss: 0.0013665262376889586\n",
      "iteration 17535, loss: 0.001380599569529295\n",
      "iteration 17536, loss: 0.001103537972085178\n",
      "iteration 17537, loss: 0.0013231700286269188\n",
      "iteration 17538, loss: 0.0013514175079762936\n",
      "iteration 17539, loss: 0.001476737903431058\n",
      "iteration 17540, loss: 0.0013414245331659913\n",
      "iteration 17541, loss: 0.001189869362860918\n",
      "iteration 17542, loss: 0.0014888785080984235\n",
      "iteration 17543, loss: 0.001236743526533246\n",
      "iteration 17544, loss: 0.0014967998722568154\n",
      "iteration 17545, loss: 0.0013266984606161714\n",
      "iteration 17546, loss: 0.00113203888759017\n",
      "iteration 17547, loss: 0.0012184894876554608\n",
      "iteration 17548, loss: 0.0010255908127874136\n",
      "iteration 17549, loss: 0.0011640615994110703\n",
      "iteration 17550, loss: 0.0012773426715284586\n",
      "iteration 17551, loss: 0.0012834726367145777\n",
      "iteration 17552, loss: 0.0013558731880038977\n",
      "iteration 17553, loss: 0.0012080231681466103\n",
      "iteration 17554, loss: 0.0013184871058911085\n",
      "iteration 17555, loss: 0.0009285332635045052\n",
      "iteration 17556, loss: 0.00122163281776011\n",
      "iteration 17557, loss: 0.0013223595451563597\n",
      "iteration 17558, loss: 0.0011156892869621515\n",
      "iteration 17559, loss: 0.0010311381192877889\n",
      "iteration 17560, loss: 0.001166107482276857\n",
      "iteration 17561, loss: 0.0014118673279881477\n",
      "iteration 17562, loss: 0.0012694435426965356\n",
      "iteration 17563, loss: 0.0015531991375610232\n",
      "iteration 17564, loss: 0.0015359660610556602\n",
      "iteration 17565, loss: 0.0013184791896492243\n",
      "iteration 17566, loss: 0.0012495804112404585\n",
      "iteration 17567, loss: 0.0017924733692780137\n",
      "iteration 17568, loss: 0.001363818533718586\n",
      "iteration 17569, loss: 0.001092017744667828\n",
      "iteration 17570, loss: 0.0011999844573438168\n",
      "iteration 17571, loss: 0.0015241913497447968\n",
      "iteration 17572, loss: 0.0013055289164185524\n",
      "iteration 17573, loss: 0.001315925968810916\n",
      "iteration 17574, loss: 0.0011728700483217835\n",
      "iteration 17575, loss: 0.0014956278027966619\n",
      "iteration 17576, loss: 0.001750434748828411\n",
      "iteration 17577, loss: 0.0013784441398456693\n",
      "iteration 17578, loss: 0.0014433879405260086\n",
      "iteration 17579, loss: 0.0015956817660480738\n",
      "iteration 17580, loss: 0.00169998942874372\n",
      "iteration 17581, loss: 0.0015593887073919177\n",
      "iteration 17582, loss: 0.0015821070410311222\n",
      "iteration 17583, loss: 0.0013148346915841103\n",
      "iteration 17584, loss: 0.0014593431260436773\n",
      "iteration 17585, loss: 0.0013120389776304364\n",
      "iteration 17586, loss: 0.001130517921410501\n",
      "iteration 17587, loss: 0.001367663498967886\n",
      "iteration 17588, loss: 0.0013053934089839458\n",
      "iteration 17589, loss: 0.0011778782354667783\n",
      "iteration 17590, loss: 0.0013627761509269476\n",
      "iteration 17591, loss: 0.0014558243565261364\n",
      "iteration 17592, loss: 0.001323738950304687\n",
      "iteration 17593, loss: 0.0012140455655753613\n",
      "iteration 17594, loss: 0.0010902124922722578\n",
      "iteration 17595, loss: 0.0016116665210574865\n",
      "iteration 17596, loss: 0.0013238020474091172\n",
      "iteration 17597, loss: 0.0010736917611211538\n",
      "iteration 17598, loss: 0.0014178880956023932\n",
      "iteration 17599, loss: 0.0011136210523545742\n",
      "iteration 17600, loss: 0.0014034457271918654\n",
      "iteration 17601, loss: 0.001257977681234479\n",
      "iteration 17602, loss: 0.0013790985103696585\n",
      "iteration 17603, loss: 0.001152530312538147\n",
      "iteration 17604, loss: 0.001147935283370316\n",
      "iteration 17605, loss: 0.001407315954566002\n",
      "iteration 17606, loss: 0.001227670582011342\n",
      "iteration 17607, loss: 0.001370221609249711\n",
      "iteration 17608, loss: 0.001287075225263834\n",
      "iteration 17609, loss: 0.0016132948221638799\n",
      "iteration 17610, loss: 0.0012128227390348911\n",
      "iteration 17611, loss: 0.0010489745764061809\n",
      "iteration 17612, loss: 0.001599688082933426\n",
      "iteration 17613, loss: 0.0012480895966291428\n",
      "iteration 17614, loss: 0.0014356583124026656\n",
      "iteration 17615, loss: 0.0010288828052580357\n",
      "iteration 17616, loss: 0.0014376790495589375\n",
      "iteration 17617, loss: 0.00128404819406569\n",
      "iteration 17618, loss: 0.0012658031191676855\n",
      "iteration 17619, loss: 0.0012480372097343206\n",
      "iteration 17620, loss: 0.0012469264911487699\n",
      "iteration 17621, loss: 0.0012066360795870423\n",
      "iteration 17622, loss: 0.0011156119871884584\n",
      "iteration 17623, loss: 0.0013165426207706332\n",
      "iteration 17624, loss: 0.001238152151927352\n",
      "iteration 17625, loss: 0.0014232818502932787\n",
      "iteration 17626, loss: 0.0011495298240333796\n",
      "iteration 17627, loss: 0.0011817098129540682\n",
      "iteration 17628, loss: 0.001330571249127388\n",
      "iteration 17629, loss: 0.0012224537786096334\n",
      "iteration 17630, loss: 0.0012656647013500333\n",
      "iteration 17631, loss: 0.001054700231179595\n",
      "iteration 17632, loss: 0.0015165862860158086\n",
      "iteration 17633, loss: 0.001218247227370739\n",
      "iteration 17634, loss: 0.0014629615470767021\n",
      "iteration 17635, loss: 0.001540502067655325\n",
      "iteration 17636, loss: 0.0017776833847165108\n",
      "iteration 17637, loss: 0.0011605098843574524\n",
      "iteration 17638, loss: 0.0013461000053212047\n",
      "iteration 17639, loss: 0.0013155511114746332\n",
      "iteration 17640, loss: 0.0013795070117339492\n",
      "iteration 17641, loss: 0.0011038859374821186\n",
      "iteration 17642, loss: 0.0013617044314742088\n",
      "iteration 17643, loss: 0.0010538393398746848\n",
      "iteration 17644, loss: 0.0013959644129499793\n",
      "iteration 17645, loss: 0.0012751145986840129\n",
      "iteration 17646, loss: 0.0011827410198748112\n",
      "iteration 17647, loss: 0.0013175386702641845\n",
      "iteration 17648, loss: 0.0011103826109319925\n",
      "iteration 17649, loss: 0.0012909492943435907\n",
      "iteration 17650, loss: 0.0013616351643577218\n",
      "iteration 17651, loss: 0.001197643345221877\n",
      "iteration 17652, loss: 0.0012062890455126762\n",
      "iteration 17653, loss: 0.001110714627429843\n",
      "iteration 17654, loss: 0.0012658194173127413\n",
      "iteration 17655, loss: 0.0010776070412248373\n",
      "iteration 17656, loss: 0.0011949802283197641\n",
      "iteration 17657, loss: 0.001243939739651978\n",
      "iteration 17658, loss: 0.001162820030003786\n",
      "iteration 17659, loss: 0.0012996240984648466\n",
      "iteration 17660, loss: 0.001018165610730648\n",
      "iteration 17661, loss: 0.0013764874311164021\n",
      "iteration 17662, loss: 0.0012417198158800602\n",
      "iteration 17663, loss: 0.0011036419309675694\n",
      "iteration 17664, loss: 0.0012692949967458844\n",
      "iteration 17665, loss: 0.001248632208444178\n",
      "iteration 17666, loss: 0.0011752245482057333\n",
      "iteration 17667, loss: 0.0013075096067041159\n",
      "iteration 17668, loss: 0.0010172398760914803\n",
      "iteration 17669, loss: 0.0010149204172194004\n",
      "iteration 17670, loss: 0.0014884410193189979\n",
      "iteration 17671, loss: 0.0012000775896012783\n",
      "iteration 17672, loss: 0.0013249252224341035\n",
      "iteration 17673, loss: 0.0013093689922243357\n",
      "iteration 17674, loss: 0.0013311631046235561\n",
      "iteration 17675, loss: 0.0011407628189772367\n",
      "iteration 17676, loss: 0.0012423761654645205\n",
      "iteration 17677, loss: 0.0011743006762117147\n",
      "iteration 17678, loss: 0.0010867657838389277\n",
      "iteration 17679, loss: 0.0012713035102933645\n",
      "iteration 17680, loss: 0.001025776844471693\n",
      "iteration 17681, loss: 0.0011268558446317911\n",
      "iteration 17682, loss: 0.001178849721327424\n",
      "iteration 17683, loss: 0.0011647145729511976\n",
      "iteration 17684, loss: 0.0014062037225812674\n",
      "iteration 17685, loss: 0.0010489275446161628\n",
      "iteration 17686, loss: 0.0012057884596288204\n",
      "iteration 17687, loss: 0.0013761802110821009\n",
      "iteration 17688, loss: 0.0010118127102032304\n",
      "iteration 17689, loss: 0.0011493172496557236\n",
      "iteration 17690, loss: 0.0011807503178715706\n",
      "iteration 17691, loss: 0.0013005194487050176\n",
      "iteration 17692, loss: 0.0010775529081001878\n",
      "iteration 17693, loss: 0.0013565011322498322\n",
      "iteration 17694, loss: 0.0013375544222071767\n",
      "iteration 17695, loss: 0.0010876440210267901\n",
      "iteration 17696, loss: 0.0010760013246908784\n",
      "iteration 17697, loss: 0.0011992438230663538\n",
      "iteration 17698, loss: 0.0012824926525354385\n",
      "iteration 17699, loss: 0.0013827886432409286\n",
      "iteration 17700, loss: 0.001116922707296908\n",
      "iteration 17701, loss: 0.0012136413715779781\n",
      "iteration 17702, loss: 0.001327359932474792\n",
      "iteration 17703, loss: 0.0012815925292670727\n",
      "iteration 17704, loss: 0.0011877004289999604\n",
      "iteration 17705, loss: 0.0011749626137316227\n",
      "iteration 17706, loss: 0.0011503627756610513\n",
      "iteration 17707, loss: 0.001090915990062058\n",
      "iteration 17708, loss: 0.0011376964394003153\n",
      "iteration 17709, loss: 0.0017532126512378454\n",
      "iteration 17710, loss: 0.0010067317634820938\n",
      "iteration 17711, loss: 0.0011896223295480013\n",
      "iteration 17712, loss: 0.0015083239413797855\n",
      "iteration 17713, loss: 0.0012816444505006075\n",
      "iteration 17714, loss: 0.0014103311114013195\n",
      "iteration 17715, loss: 0.0010597605723887682\n",
      "iteration 17716, loss: 0.0010897517204284668\n",
      "iteration 17717, loss: 0.0010631229961290956\n",
      "iteration 17718, loss: 0.0015317879151552916\n",
      "iteration 17719, loss: 0.0011637960560619831\n",
      "iteration 17720, loss: 0.0011094289366155863\n",
      "iteration 17721, loss: 0.001207528868690133\n",
      "iteration 17722, loss: 0.0012397923273965716\n",
      "iteration 17723, loss: 0.001277441275306046\n",
      "iteration 17724, loss: 0.0013158830115571618\n",
      "iteration 17725, loss: 0.0014049502788111567\n",
      "iteration 17726, loss: 0.0016608370933681726\n",
      "iteration 17727, loss: 0.0011137567926198244\n",
      "iteration 17728, loss: 0.0011187903583049774\n",
      "iteration 17729, loss: 0.0012172244023531675\n",
      "iteration 17730, loss: 0.001358215231448412\n",
      "iteration 17731, loss: 0.0014345250092446804\n",
      "iteration 17732, loss: 0.001385994954034686\n",
      "iteration 17733, loss: 0.001210938673466444\n",
      "iteration 17734, loss: 0.0011965972371399403\n",
      "iteration 17735, loss: 0.0011820895597338676\n",
      "iteration 17736, loss: 0.0011098452378064394\n",
      "iteration 17737, loss: 0.0014942781999707222\n",
      "iteration 17738, loss: 0.0012376104714348912\n",
      "iteration 17739, loss: 0.00139088393189013\n",
      "iteration 17740, loss: 0.0012839949922636151\n",
      "iteration 17741, loss: 0.0015066959895193577\n",
      "iteration 17742, loss: 0.0010402299230918288\n",
      "iteration 17743, loss: 0.0012715340126305819\n",
      "iteration 17744, loss: 0.0011752777500078082\n",
      "iteration 17745, loss: 0.0014093250501900911\n",
      "iteration 17746, loss: 0.0015421032439917326\n",
      "iteration 17747, loss: 0.001443130662664771\n",
      "iteration 17748, loss: 0.0012692370219156146\n",
      "iteration 17749, loss: 0.001212884089909494\n",
      "iteration 17750, loss: 0.0012351972982287407\n",
      "iteration 17751, loss: 0.0010514925234019756\n",
      "iteration 17752, loss: 0.0011733375722542405\n",
      "iteration 17753, loss: 0.0013300282880663872\n",
      "iteration 17754, loss: 0.0012182401260361075\n",
      "iteration 17755, loss: 0.0013595035998150706\n",
      "iteration 17756, loss: 0.0013887323439121246\n",
      "iteration 17757, loss: 0.001041191047988832\n",
      "iteration 17758, loss: 0.001113393227569759\n",
      "iteration 17759, loss: 0.0013954689493402839\n",
      "iteration 17760, loss: 0.0013505828101187944\n",
      "iteration 17761, loss: 0.001383240451104939\n",
      "iteration 17762, loss: 0.0010617042426019907\n",
      "iteration 17763, loss: 0.001164786284789443\n",
      "iteration 17764, loss: 0.001257883501239121\n",
      "iteration 17765, loss: 0.0012301886454224586\n",
      "iteration 17766, loss: 0.0014866976998746395\n",
      "iteration 17767, loss: 0.0016570915468037128\n",
      "iteration 17768, loss: 0.0013602706603705883\n",
      "iteration 17769, loss: 0.0015919955912977457\n",
      "iteration 17770, loss: 0.0012245002435520291\n",
      "iteration 17771, loss: 0.0014905917923897505\n",
      "iteration 17772, loss: 0.0013772959355264902\n",
      "iteration 17773, loss: 0.0011633239919319749\n",
      "iteration 17774, loss: 0.001480617793276906\n",
      "iteration 17775, loss: 0.001258939621038735\n",
      "iteration 17776, loss: 0.00115734594874084\n",
      "iteration 17777, loss: 0.0018694223836064339\n",
      "iteration 17778, loss: 0.0013923794031143188\n",
      "iteration 17779, loss: 0.0014101609122008085\n",
      "iteration 17780, loss: 0.00131235271692276\n",
      "iteration 17781, loss: 0.001420753775164485\n",
      "iteration 17782, loss: 0.001137739047408104\n",
      "iteration 17783, loss: 0.0013418272137641907\n",
      "iteration 17784, loss: 0.0012538100127130747\n",
      "iteration 17785, loss: 0.0015881438739597797\n",
      "iteration 17786, loss: 0.001289280829951167\n",
      "iteration 17787, loss: 0.0013068730477243662\n",
      "iteration 17788, loss: 0.0012978981249034405\n",
      "iteration 17789, loss: 0.0010660949628800154\n",
      "iteration 17790, loss: 0.0015342221595346928\n",
      "iteration 17791, loss: 0.0012341816909611225\n",
      "iteration 17792, loss: 0.001254106406122446\n",
      "iteration 17793, loss: 0.0013721840223297477\n",
      "iteration 17794, loss: 0.0012121011968702078\n",
      "iteration 17795, loss: 0.001292368513531983\n",
      "iteration 17796, loss: 0.0012764529092237353\n",
      "iteration 17797, loss: 0.0013838742161169648\n",
      "iteration 17798, loss: 0.0011810733703896403\n",
      "iteration 17799, loss: 0.001132116885855794\n",
      "iteration 17800, loss: 0.0012063984759151936\n",
      "iteration 17801, loss: 0.001256422372534871\n",
      "iteration 17802, loss: 0.0012381845153868198\n",
      "iteration 17803, loss: 0.0017987152095884085\n",
      "iteration 17804, loss: 0.0012753370683640242\n",
      "iteration 17805, loss: 0.0012376010417938232\n",
      "iteration 17806, loss: 0.001355297863483429\n",
      "iteration 17807, loss: 0.0011846439447253942\n",
      "iteration 17808, loss: 0.0015559741295874119\n",
      "iteration 17809, loss: 0.0015941483434289694\n",
      "iteration 17810, loss: 0.001308130449615419\n",
      "iteration 17811, loss: 0.0014317266177386045\n",
      "iteration 17812, loss: 0.0009262869134545326\n",
      "iteration 17813, loss: 0.0012625709641724825\n",
      "iteration 17814, loss: 0.0012162660714238882\n",
      "iteration 17815, loss: 0.001425734837539494\n",
      "iteration 17816, loss: 0.0010499581694602966\n",
      "iteration 17817, loss: 0.0010377794969826937\n",
      "iteration 17818, loss: 0.0012666808906942606\n",
      "iteration 17819, loss: 0.001308739883825183\n",
      "iteration 17820, loss: 0.0011475587962195277\n",
      "iteration 17821, loss: 0.0012256349436938763\n",
      "iteration 17822, loss: 0.0010061694774776697\n",
      "iteration 17823, loss: 0.0013356511481106281\n",
      "iteration 17824, loss: 0.0013676279922947288\n",
      "iteration 17825, loss: 0.0011615671683102846\n",
      "iteration 17826, loss: 0.0014733315911144018\n",
      "iteration 17827, loss: 0.0013978704810142517\n",
      "iteration 17828, loss: 0.0013788107316941023\n",
      "iteration 17829, loss: 0.0014681641478091478\n",
      "iteration 17830, loss: 0.0011249133385717869\n",
      "iteration 17831, loss: 0.0015430626226589084\n",
      "iteration 17832, loss: 0.0010997785720974207\n",
      "iteration 17833, loss: 0.0012817762326449156\n",
      "iteration 17834, loss: 0.0014852851163595915\n",
      "iteration 17835, loss: 0.0008353044977411628\n",
      "iteration 17836, loss: 0.00158001109957695\n",
      "iteration 17837, loss: 0.00134257972240448\n",
      "iteration 17838, loss: 0.0011691366089507937\n",
      "iteration 17839, loss: 0.0013674532528966665\n",
      "iteration 17840, loss: 0.0014075548388063908\n",
      "iteration 17841, loss: 0.0015607541427016258\n",
      "iteration 17842, loss: 0.001111712888814509\n",
      "iteration 17843, loss: 0.0011444874107837677\n",
      "iteration 17844, loss: 0.0011071937624365091\n",
      "iteration 17845, loss: 0.0011416994966566563\n",
      "iteration 17846, loss: 0.0012821208219975233\n",
      "iteration 17847, loss: 0.001289393869228661\n",
      "iteration 17848, loss: 0.0013426546938717365\n",
      "iteration 17849, loss: 0.0012403447180986404\n",
      "iteration 17850, loss: 0.0012912966776639223\n",
      "iteration 17851, loss: 0.001275623799301684\n",
      "iteration 17852, loss: 0.0014161504805088043\n",
      "iteration 17853, loss: 0.001446459791623056\n",
      "iteration 17854, loss: 0.001102907583117485\n",
      "iteration 17855, loss: 0.001048251986503601\n",
      "iteration 17856, loss: 0.0011263447813689709\n",
      "iteration 17857, loss: 0.0013475620653480291\n",
      "iteration 17858, loss: 0.0012253959430381656\n",
      "iteration 17859, loss: 0.0012864023447036743\n",
      "iteration 17860, loss: 0.001421914086677134\n",
      "iteration 17861, loss: 0.00106878113001585\n",
      "iteration 17862, loss: 0.0011608536588028073\n",
      "iteration 17863, loss: 0.0013158867368474603\n",
      "iteration 17864, loss: 0.00119428476318717\n",
      "iteration 17865, loss: 0.0013111018342897296\n",
      "iteration 17866, loss: 0.0013786109630018473\n",
      "iteration 17867, loss: 0.0011357908369973302\n",
      "iteration 17868, loss: 0.0013159547233954072\n",
      "iteration 17869, loss: 0.0011324587976559997\n",
      "iteration 17870, loss: 0.0015852394280955195\n",
      "iteration 17871, loss: 0.0010373753029853106\n",
      "iteration 17872, loss: 0.001123509369790554\n",
      "iteration 17873, loss: 0.0013860127655789256\n",
      "iteration 17874, loss: 0.0013913960428908467\n",
      "iteration 17875, loss: 0.0012317918008193374\n",
      "iteration 17876, loss: 0.0010474747978150845\n",
      "iteration 17877, loss: 0.0012022324372082949\n",
      "iteration 17878, loss: 0.0013007133966311812\n",
      "iteration 17879, loss: 0.0012468888889998198\n",
      "iteration 17880, loss: 0.0017050083260983229\n",
      "iteration 17881, loss: 0.001410198863595724\n",
      "iteration 17882, loss: 0.001664180075749755\n",
      "iteration 17883, loss: 0.001238822704181075\n",
      "iteration 17884, loss: 0.0014174174284562469\n",
      "iteration 17885, loss: 0.0013080083299428225\n",
      "iteration 17886, loss: 0.0011942355195060372\n",
      "iteration 17887, loss: 0.0012969332747161388\n",
      "iteration 17888, loss: 0.0013725291937589645\n",
      "iteration 17889, loss: 0.0013349459040910006\n",
      "iteration 17890, loss: 0.0013916408643126488\n",
      "iteration 17891, loss: 0.0011916051153093576\n",
      "iteration 17892, loss: 0.001006960985250771\n",
      "iteration 17893, loss: 0.001647223369218409\n",
      "iteration 17894, loss: 0.0010820525931194425\n",
      "iteration 17895, loss: 0.0014229223597794771\n",
      "iteration 17896, loss: 0.0012074124533683062\n",
      "iteration 17897, loss: 0.0012413401855155826\n",
      "iteration 17898, loss: 0.0016366250347346067\n",
      "iteration 17899, loss: 0.0016322792507708073\n",
      "iteration 17900, loss: 0.0013054583687335253\n",
      "iteration 17901, loss: 0.0014222038444131613\n",
      "iteration 17902, loss: 0.0015185503289103508\n",
      "iteration 17903, loss: 0.0012003307929262519\n",
      "iteration 17904, loss: 0.0014312947168946266\n",
      "iteration 17905, loss: 0.0011600045254454017\n",
      "iteration 17906, loss: 0.0014294933062046766\n",
      "iteration 17907, loss: 0.0013010099064558744\n",
      "iteration 17908, loss: 0.0015386547893285751\n",
      "iteration 17909, loss: 0.0011830023722723126\n",
      "iteration 17910, loss: 0.001289124833419919\n",
      "iteration 17911, loss: 0.001518232747912407\n",
      "iteration 17912, loss: 0.0012716720812022686\n",
      "iteration 17913, loss: 0.0012815693626180291\n",
      "iteration 17914, loss: 0.001113401260226965\n",
      "iteration 17915, loss: 0.001293352572247386\n",
      "iteration 17916, loss: 0.0011935505317524076\n",
      "iteration 17917, loss: 0.0011817002668976784\n",
      "iteration 17918, loss: 0.0013825269415974617\n",
      "iteration 17919, loss: 0.0012634163722395897\n",
      "iteration 17920, loss: 0.0012158048339188099\n",
      "iteration 17921, loss: 0.001405162736773491\n",
      "iteration 17922, loss: 0.001435195212252438\n",
      "iteration 17923, loss: 0.0013568517751991749\n",
      "iteration 17924, loss: 0.0013563076499849558\n",
      "iteration 17925, loss: 0.0015074791153892875\n",
      "iteration 17926, loss: 0.0013539595529437065\n",
      "iteration 17927, loss: 0.0014786265091970563\n",
      "iteration 17928, loss: 0.0014002076350152493\n",
      "iteration 17929, loss: 0.001396654173731804\n",
      "iteration 17930, loss: 0.001647532801143825\n",
      "iteration 17931, loss: 0.0014104506699368358\n",
      "iteration 17932, loss: 0.0012734574265778065\n",
      "iteration 17933, loss: 0.0011985772289335728\n",
      "iteration 17934, loss: 0.001414233585819602\n",
      "iteration 17935, loss: 0.0014919689856469631\n",
      "iteration 17936, loss: 0.0012078494764864445\n",
      "iteration 17937, loss: 0.0012866160832345486\n",
      "iteration 17938, loss: 0.0013323689345270395\n",
      "iteration 17939, loss: 0.0013906112872064114\n",
      "iteration 17940, loss: 0.0013080433709546924\n",
      "iteration 17941, loss: 0.0014292222913354635\n",
      "iteration 17942, loss: 0.0014265035279095173\n",
      "iteration 17943, loss: 0.0016177623765543103\n",
      "iteration 17944, loss: 0.0013386219507083297\n",
      "iteration 17945, loss: 0.0012959728483110666\n",
      "iteration 17946, loss: 0.0012504997430369258\n",
      "iteration 17947, loss: 0.0014082778943702579\n",
      "iteration 17948, loss: 0.0014181423466652632\n",
      "iteration 17949, loss: 0.0013515878235921264\n",
      "iteration 17950, loss: 0.001440540887415409\n",
      "iteration 17951, loss: 0.0011109164915978909\n",
      "iteration 17952, loss: 0.001406610943377018\n",
      "iteration 17953, loss: 0.0010986935812979937\n",
      "iteration 17954, loss: 0.0010770823573693633\n",
      "iteration 17955, loss: 0.0011373277520760894\n",
      "iteration 17956, loss: 0.0010095612378790975\n",
      "iteration 17957, loss: 0.0010979979997500777\n",
      "iteration 17958, loss: 0.0014980640262365341\n",
      "iteration 17959, loss: 0.0012819538824260235\n",
      "iteration 17960, loss: 0.0010634211357682943\n",
      "iteration 17961, loss: 0.0012818132527172565\n",
      "iteration 17962, loss: 0.0010758021380752325\n",
      "iteration 17963, loss: 0.0013550969306379557\n",
      "iteration 17964, loss: 0.0011294173309579492\n",
      "iteration 17965, loss: 0.0013364944607019424\n",
      "iteration 17966, loss: 0.00131006701849401\n",
      "iteration 17967, loss: 0.0010525190737098455\n",
      "iteration 17968, loss: 0.0012215805472806096\n",
      "iteration 17969, loss: 0.0015856553800404072\n",
      "iteration 17970, loss: 0.0012823713477700949\n",
      "iteration 17971, loss: 0.001053603831678629\n",
      "iteration 17972, loss: 0.001118250424042344\n",
      "iteration 17973, loss: 0.0009777902159839869\n",
      "iteration 17974, loss: 0.0011073015630245209\n",
      "iteration 17975, loss: 0.001232409500516951\n",
      "iteration 17976, loss: 0.0012692769523710012\n",
      "iteration 17977, loss: 0.0014328634133562446\n",
      "iteration 17978, loss: 0.0013121122028678656\n",
      "iteration 17979, loss: 0.0013653391506522894\n",
      "iteration 17980, loss: 0.0012987481895834208\n",
      "iteration 17981, loss: 0.0016406525392085314\n",
      "iteration 17982, loss: 0.0012218938209116459\n",
      "iteration 17983, loss: 0.0012612089049071074\n",
      "iteration 17984, loss: 0.0011025385465472937\n",
      "iteration 17985, loss: 0.0010186504805460572\n",
      "iteration 17986, loss: 0.0012014880776405334\n",
      "iteration 17987, loss: 0.0011502880370244384\n",
      "iteration 17988, loss: 0.0009227595874108374\n",
      "iteration 17989, loss: 0.0011434759944677353\n",
      "iteration 17990, loss: 0.0014103278517723083\n",
      "iteration 17991, loss: 0.0010998023208230734\n",
      "iteration 17992, loss: 0.0012177141616120934\n",
      "iteration 17993, loss: 0.0011109125334769487\n",
      "iteration 17994, loss: 0.001011502929031849\n",
      "iteration 17995, loss: 0.0012090156087651849\n",
      "iteration 17996, loss: 0.001186954090371728\n",
      "iteration 17997, loss: 0.001108410069718957\n",
      "iteration 17998, loss: 0.0012533713597804308\n",
      "iteration 17999, loss: 0.0011486033909022808\n",
      "iteration 18000, loss: 0.0011507405433803797\n",
      "iteration 18001, loss: 0.0010798295261338353\n",
      "iteration 18002, loss: 0.0012893910752609372\n",
      "iteration 18003, loss: 0.0010880987392738461\n",
      "iteration 18004, loss: 0.0010329405777156353\n",
      "iteration 18005, loss: 0.0012668046401813626\n",
      "iteration 18006, loss: 0.0011023133993148804\n",
      "iteration 18007, loss: 0.0013306516921147704\n",
      "iteration 18008, loss: 0.0012418830301612616\n",
      "iteration 18009, loss: 0.0011672347318381071\n",
      "iteration 18010, loss: 0.0013175768544897437\n",
      "iteration 18011, loss: 0.0010406951187178493\n",
      "iteration 18012, loss: 0.0010334537364542484\n",
      "iteration 18013, loss: 0.0014552748762071133\n",
      "iteration 18014, loss: 0.0012359274551272392\n",
      "iteration 18015, loss: 0.0011434005573391914\n",
      "iteration 18016, loss: 0.0013088083360344172\n",
      "iteration 18017, loss: 0.0011904931161552668\n",
      "iteration 18018, loss: 0.0012295498745515943\n",
      "iteration 18019, loss: 0.00124089524615556\n",
      "iteration 18020, loss: 0.001424540881998837\n",
      "iteration 18021, loss: 0.001043858239427209\n",
      "iteration 18022, loss: 0.001345331664197147\n",
      "iteration 18023, loss: 0.0011770639102905989\n",
      "iteration 18024, loss: 0.0010470938868820667\n",
      "iteration 18025, loss: 0.0013502485817298293\n",
      "iteration 18026, loss: 0.0014333571307361126\n",
      "iteration 18027, loss: 0.0012560077011585236\n",
      "iteration 18028, loss: 0.0010573031613603234\n",
      "iteration 18029, loss: 0.0010077564511448145\n",
      "iteration 18030, loss: 0.001216165372170508\n",
      "iteration 18031, loss: 0.0012818316463381052\n",
      "iteration 18032, loss: 0.00106977263931185\n",
      "iteration 18033, loss: 0.001279314048588276\n",
      "iteration 18034, loss: 0.0011398174101486802\n",
      "iteration 18035, loss: 0.001094600884243846\n",
      "iteration 18036, loss: 0.000942105776630342\n",
      "iteration 18037, loss: 0.0011539226397871971\n",
      "iteration 18038, loss: 0.001197363599203527\n",
      "iteration 18039, loss: 0.0013637857045978308\n",
      "iteration 18040, loss: 0.0010848266538232565\n",
      "iteration 18041, loss: 0.0009895933326333761\n",
      "iteration 18042, loss: 0.0012595447478815913\n",
      "iteration 18043, loss: 0.000862174085341394\n",
      "iteration 18044, loss: 0.0011472151381894946\n",
      "iteration 18045, loss: 0.0010424298234283924\n",
      "iteration 18046, loss: 0.0010655595688149333\n",
      "iteration 18047, loss: 0.0009803320281207561\n",
      "iteration 18048, loss: 0.0010880748741328716\n",
      "iteration 18049, loss: 0.0010716012911871076\n",
      "iteration 18050, loss: 0.001026634476147592\n",
      "iteration 18051, loss: 0.0009926713537424803\n",
      "iteration 18052, loss: 0.0010147206485271454\n",
      "iteration 18053, loss: 0.0012804812286049128\n",
      "iteration 18054, loss: 0.0009716261993162334\n",
      "iteration 18055, loss: 0.0012719728983938694\n",
      "iteration 18056, loss: 0.0012650048593059182\n",
      "iteration 18057, loss: 0.0010377957951277494\n",
      "iteration 18058, loss: 0.0011127798352390528\n",
      "iteration 18059, loss: 0.0010467468528077006\n",
      "iteration 18060, loss: 0.0011651047971099615\n",
      "iteration 18061, loss: 0.0012920426670461893\n",
      "iteration 18062, loss: 0.0012067037168890238\n",
      "iteration 18063, loss: 0.001112288562580943\n",
      "iteration 18064, loss: 0.0014870318118482828\n",
      "iteration 18065, loss: 0.0013238198589533567\n",
      "iteration 18066, loss: 0.0009985072538256645\n",
      "iteration 18067, loss: 0.0012497963616624475\n",
      "iteration 18068, loss: 0.0011541033163666725\n",
      "iteration 18069, loss: 0.0011876141652464867\n",
      "iteration 18070, loss: 0.0010973289608955383\n",
      "iteration 18071, loss: 0.0009836481185629964\n",
      "iteration 18072, loss: 0.0009141442133113742\n",
      "iteration 18073, loss: 0.0010622323025017977\n",
      "iteration 18074, loss: 0.0012506055645644665\n",
      "iteration 18075, loss: 0.0009113677078858018\n",
      "iteration 18076, loss: 0.0011174870887771249\n",
      "iteration 18077, loss: 0.0010456331074237823\n",
      "iteration 18078, loss: 0.001224064501002431\n",
      "iteration 18079, loss: 0.001072871033102274\n",
      "iteration 18080, loss: 0.0012959025334566832\n",
      "iteration 18081, loss: 0.0010785575723275542\n",
      "iteration 18082, loss: 0.001177256228402257\n",
      "iteration 18083, loss: 0.001261307392269373\n",
      "iteration 18084, loss: 0.0012494511902332306\n",
      "iteration 18085, loss: 0.0014232395915314555\n",
      "iteration 18086, loss: 0.0010570655576884747\n",
      "iteration 18087, loss: 0.0014491014881059527\n",
      "iteration 18088, loss: 0.001274016103707254\n",
      "iteration 18089, loss: 0.0011242479085922241\n",
      "iteration 18090, loss: 0.001529844943434\n",
      "iteration 18091, loss: 0.0013808475341647863\n",
      "iteration 18092, loss: 0.0011640347074717283\n",
      "iteration 18093, loss: 0.0012748667504638433\n",
      "iteration 18094, loss: 0.0011745748342946172\n",
      "iteration 18095, loss: 0.0011171994265168905\n",
      "iteration 18096, loss: 0.0011812555603682995\n",
      "iteration 18097, loss: 0.001257437514141202\n",
      "iteration 18098, loss: 0.0011380864307284355\n",
      "iteration 18099, loss: 0.0010929538402706385\n",
      "iteration 18100, loss: 0.0011895050993189216\n",
      "iteration 18101, loss: 0.0012548159575089812\n",
      "iteration 18102, loss: 0.001215001568198204\n",
      "iteration 18103, loss: 0.001102894893847406\n",
      "iteration 18104, loss: 0.0013408007798716426\n",
      "iteration 18105, loss: 0.001253772177733481\n",
      "iteration 18106, loss: 0.001121509587392211\n",
      "iteration 18107, loss: 0.0011802585795521736\n",
      "iteration 18108, loss: 0.0010201706318184733\n",
      "iteration 18109, loss: 0.0011132600484415889\n",
      "iteration 18110, loss: 0.00107298674993217\n",
      "iteration 18111, loss: 0.0011253025149926543\n",
      "iteration 18112, loss: 0.0011599420104175806\n",
      "iteration 18113, loss: 0.0015459046699106693\n",
      "iteration 18114, loss: 0.0011765335220843554\n",
      "iteration 18115, loss: 0.0014237009454518557\n",
      "iteration 18116, loss: 0.0012035247636958957\n",
      "iteration 18117, loss: 0.001050741644576192\n",
      "iteration 18118, loss: 0.0011583217419683933\n",
      "iteration 18119, loss: 0.0008926161099225283\n",
      "iteration 18120, loss: 0.0012619646731764078\n",
      "iteration 18121, loss: 0.001073699677363038\n",
      "iteration 18122, loss: 0.0011683076154440641\n",
      "iteration 18123, loss: 0.0010450102854520082\n",
      "iteration 18124, loss: 0.0012802849523723125\n",
      "iteration 18125, loss: 0.0013027372770011425\n",
      "iteration 18126, loss: 0.0012970494572073221\n",
      "iteration 18127, loss: 0.001291866647079587\n",
      "iteration 18128, loss: 0.0011054715141654015\n",
      "iteration 18129, loss: 0.0010698643745854497\n",
      "iteration 18130, loss: 0.0015984894707798958\n",
      "iteration 18131, loss: 0.0012739121448248625\n",
      "iteration 18132, loss: 0.0008887194562703371\n",
      "iteration 18133, loss: 0.0011619694996625185\n",
      "iteration 18134, loss: 0.0011396553600206971\n",
      "iteration 18135, loss: 0.0012370753102004528\n",
      "iteration 18136, loss: 0.0011943140998482704\n",
      "iteration 18137, loss: 0.00118641194421798\n",
      "iteration 18138, loss: 0.0009932054672390223\n",
      "iteration 18139, loss: 0.0010432840790599585\n",
      "iteration 18140, loss: 0.0015010649804025888\n",
      "iteration 18141, loss: 0.0010826028883457184\n",
      "iteration 18142, loss: 0.0011466320138424635\n",
      "iteration 18143, loss: 0.0013480866327881813\n",
      "iteration 18144, loss: 0.00123761803843081\n",
      "iteration 18145, loss: 0.0011174781247973442\n",
      "iteration 18146, loss: 0.0015290306182578206\n",
      "iteration 18147, loss: 0.0010766887571662664\n",
      "iteration 18148, loss: 0.001048796926625073\n",
      "iteration 18149, loss: 0.0011223217006772757\n",
      "iteration 18150, loss: 0.001191606163047254\n",
      "iteration 18151, loss: 0.0012256462359800935\n",
      "iteration 18152, loss: 0.0012969739036634564\n",
      "iteration 18153, loss: 0.0012074434198439121\n",
      "iteration 18154, loss: 0.0013438212918117642\n",
      "iteration 18155, loss: 0.0009638663614168763\n",
      "iteration 18156, loss: 0.0010023522190749645\n",
      "iteration 18157, loss: 0.0010024129878729582\n",
      "iteration 18158, loss: 0.0011086130980402231\n",
      "iteration 18159, loss: 0.0013438314199447632\n",
      "iteration 18160, loss: 0.000983547419309616\n",
      "iteration 18161, loss: 0.0013402350014075637\n",
      "iteration 18162, loss: 0.0010973961325362325\n",
      "iteration 18163, loss: 0.0014147176407277584\n",
      "iteration 18164, loss: 0.0011301867198199034\n",
      "iteration 18165, loss: 0.001120123197324574\n",
      "iteration 18166, loss: 0.001315662288106978\n",
      "iteration 18167, loss: 0.0012400143314152956\n",
      "iteration 18168, loss: 0.001354253850877285\n",
      "iteration 18169, loss: 0.001362818293273449\n",
      "iteration 18170, loss: 0.0014498932287096977\n",
      "iteration 18171, loss: 0.0011050222674384713\n",
      "iteration 18172, loss: 0.0015973191475495696\n",
      "iteration 18173, loss: 0.001153414137661457\n",
      "iteration 18174, loss: 0.0010314155369997025\n",
      "iteration 18175, loss: 0.0010679606348276138\n",
      "iteration 18176, loss: 0.0011351264547556639\n",
      "iteration 18177, loss: 0.0013707996113225818\n",
      "iteration 18178, loss: 0.0015344086568802595\n",
      "iteration 18179, loss: 0.0010012643178924918\n",
      "iteration 18180, loss: 0.0011455586645752192\n",
      "iteration 18181, loss: 0.0009027651394717395\n",
      "iteration 18182, loss: 0.0014700641622766852\n",
      "iteration 18183, loss: 0.0010003302013501525\n",
      "iteration 18184, loss: 0.0012334088096395135\n",
      "iteration 18185, loss: 0.0011088419705629349\n",
      "iteration 18186, loss: 0.0011707717785611749\n",
      "iteration 18187, loss: 0.0009439572459086776\n",
      "iteration 18188, loss: 0.0010119786020368338\n",
      "iteration 18189, loss: 0.0010824317578226328\n",
      "iteration 18190, loss: 0.001322634518146515\n",
      "iteration 18191, loss: 0.0010482894722372293\n",
      "iteration 18192, loss: 0.0012424588203430176\n",
      "iteration 18193, loss: 0.0009796344675123692\n",
      "iteration 18194, loss: 0.0010308464989066124\n",
      "iteration 18195, loss: 0.0011333473958075047\n",
      "iteration 18196, loss: 0.0013090516440570354\n",
      "iteration 18197, loss: 0.0016340105794370174\n",
      "iteration 18198, loss: 0.0010578696383163333\n",
      "iteration 18199, loss: 0.0012415956007316709\n",
      "iteration 18200, loss: 0.0013786621857434511\n",
      "iteration 18201, loss: 0.0012073025573045015\n",
      "iteration 18202, loss: 0.0012391360942274332\n",
      "iteration 18203, loss: 0.0012016701512038708\n",
      "iteration 18204, loss: 0.0011352157453075051\n",
      "iteration 18205, loss: 0.0012652361765503883\n",
      "iteration 18206, loss: 0.0010556252673268318\n",
      "iteration 18207, loss: 0.0013657079543918371\n",
      "iteration 18208, loss: 0.0011300034821033478\n",
      "iteration 18209, loss: 0.0011730296537280083\n",
      "iteration 18210, loss: 0.001270545064471662\n",
      "iteration 18211, loss: 0.0011870275484398007\n",
      "iteration 18212, loss: 0.0013339726720005274\n",
      "iteration 18213, loss: 0.0012347116135060787\n",
      "iteration 18214, loss: 0.0014937190571799874\n",
      "iteration 18215, loss: 0.0011903992854058743\n",
      "iteration 18216, loss: 0.0011776845203712583\n",
      "iteration 18217, loss: 0.0011083250865340233\n",
      "iteration 18218, loss: 0.0012019326677545905\n",
      "iteration 18219, loss: 0.0016747713088989258\n",
      "iteration 18220, loss: 0.0010368740186095238\n",
      "iteration 18221, loss: 0.0011116184759885073\n",
      "iteration 18222, loss: 0.0013743634335696697\n",
      "iteration 18223, loss: 0.0013647704618051648\n",
      "iteration 18224, loss: 0.001133437966927886\n",
      "iteration 18225, loss: 0.0014839950017631054\n",
      "iteration 18226, loss: 0.0014939710963517427\n",
      "iteration 18227, loss: 0.001135766040533781\n",
      "iteration 18228, loss: 0.001368681900203228\n",
      "iteration 18229, loss: 0.0012193399015814066\n",
      "iteration 18230, loss: 0.001106982585042715\n",
      "iteration 18231, loss: 0.001094917068257928\n",
      "iteration 18232, loss: 0.0015268935821950436\n",
      "iteration 18233, loss: 0.0013215059880167246\n",
      "iteration 18234, loss: 0.0012267990969121456\n",
      "iteration 18235, loss: 0.0017574694938957691\n",
      "iteration 18236, loss: 0.000993645517155528\n",
      "iteration 18237, loss: 0.0013324558967724442\n",
      "iteration 18238, loss: 0.0016652913764119148\n",
      "iteration 18239, loss: 0.0011377655901014805\n",
      "iteration 18240, loss: 0.001516802003607154\n",
      "iteration 18241, loss: 0.0010864143259823322\n",
      "iteration 18242, loss: 0.0012507267529144883\n",
      "iteration 18243, loss: 0.0012219883501529694\n",
      "iteration 18244, loss: 0.0014203950995579362\n",
      "iteration 18245, loss: 0.001303687458857894\n",
      "iteration 18246, loss: 0.0012643353547900915\n",
      "iteration 18247, loss: 0.0012000852730125189\n",
      "iteration 18248, loss: 0.0013404879719018936\n",
      "iteration 18249, loss: 0.0010876209707930684\n",
      "iteration 18250, loss: 0.0009187683463096619\n",
      "iteration 18251, loss: 0.0015843831934034824\n",
      "iteration 18252, loss: 0.0010852657724171877\n",
      "iteration 18253, loss: 0.0012835445813834667\n",
      "iteration 18254, loss: 0.001476213103160262\n",
      "iteration 18255, loss: 0.001117313513532281\n",
      "iteration 18256, loss: 0.0011938007082790136\n",
      "iteration 18257, loss: 0.001136062084697187\n",
      "iteration 18258, loss: 0.0012173829600214958\n",
      "iteration 18259, loss: 0.0011243987828493118\n",
      "iteration 18260, loss: 0.0013230168260633945\n",
      "iteration 18261, loss: 0.0009518015431240201\n",
      "iteration 18262, loss: 0.0011583555024117231\n",
      "iteration 18263, loss: 0.0012062507448717952\n",
      "iteration 18264, loss: 0.0015934112016111612\n",
      "iteration 18265, loss: 0.001037105917930603\n",
      "iteration 18266, loss: 0.0012561569456011057\n",
      "iteration 18267, loss: 0.001118387095630169\n",
      "iteration 18268, loss: 0.0009623105288483202\n",
      "iteration 18269, loss: 0.0011806217953562737\n",
      "iteration 18270, loss: 0.0009578240569680929\n",
      "iteration 18271, loss: 0.00124759366735816\n",
      "iteration 18272, loss: 0.0011691427789628506\n",
      "iteration 18273, loss: 0.001188815338537097\n",
      "iteration 18274, loss: 0.0011808373965322971\n",
      "iteration 18275, loss: 0.0011986838653683662\n",
      "iteration 18276, loss: 0.0013953959569334984\n",
      "iteration 18277, loss: 0.0011028933804482222\n",
      "iteration 18278, loss: 0.0012053344398736954\n",
      "iteration 18279, loss: 0.00155737460590899\n",
      "iteration 18280, loss: 0.0013624744024127722\n",
      "iteration 18281, loss: 0.0009376887464895844\n",
      "iteration 18282, loss: 0.0011583658633753657\n",
      "iteration 18283, loss: 0.0009896759875118732\n",
      "iteration 18284, loss: 0.001108494121581316\n",
      "iteration 18285, loss: 0.0013248594477772713\n",
      "iteration 18286, loss: 0.0012621029745787382\n",
      "iteration 18287, loss: 0.0009956912836059928\n",
      "iteration 18288, loss: 0.0011330473935231566\n",
      "iteration 18289, loss: 0.0010341890156269073\n",
      "iteration 18290, loss: 0.0013792780227959156\n",
      "iteration 18291, loss: 0.0010978919453918934\n",
      "iteration 18292, loss: 0.0011266623623669147\n",
      "iteration 18293, loss: 0.0010736789554357529\n",
      "iteration 18294, loss: 0.0010682456195354462\n",
      "iteration 18295, loss: 0.0010929836425930262\n",
      "iteration 18296, loss: 0.0011217296123504639\n",
      "iteration 18297, loss: 0.0008798312046565115\n",
      "iteration 18298, loss: 0.0009089523227885365\n",
      "iteration 18299, loss: 0.0011453786864876747\n",
      "iteration 18300, loss: 0.000985676539130509\n",
      "iteration 18301, loss: 0.0010213457280769944\n",
      "iteration 18302, loss: 0.00104524043854326\n",
      "iteration 18303, loss: 0.0012229385320097208\n",
      "iteration 18304, loss: 0.0014560073614120483\n",
      "iteration 18305, loss: 0.0011121362913399935\n",
      "iteration 18306, loss: 0.000937937293201685\n",
      "iteration 18307, loss: 0.0011957185342907906\n",
      "iteration 18308, loss: 0.001093710190616548\n",
      "iteration 18309, loss: 0.000965583196375519\n",
      "iteration 18310, loss: 0.0011340146884322166\n",
      "iteration 18311, loss: 0.001398465596139431\n",
      "iteration 18312, loss: 0.0009356746450066566\n",
      "iteration 18313, loss: 0.0012047226773574948\n",
      "iteration 18314, loss: 0.0013584709959104657\n",
      "iteration 18315, loss: 0.0011667278595268726\n",
      "iteration 18316, loss: 0.0012318214867264032\n",
      "iteration 18317, loss: 0.001286413287743926\n",
      "iteration 18318, loss: 0.0013268280308693647\n",
      "iteration 18319, loss: 0.0013512937584891915\n",
      "iteration 18320, loss: 0.0008754294831305742\n",
      "iteration 18321, loss: 0.0012418520636856556\n",
      "iteration 18322, loss: 0.0013808791991323233\n",
      "iteration 18323, loss: 0.001144507434219122\n",
      "iteration 18324, loss: 0.001280900789424777\n",
      "iteration 18325, loss: 0.0012582254130393267\n",
      "iteration 18326, loss: 0.0013911286368966103\n",
      "iteration 18327, loss: 0.0013170749880373478\n",
      "iteration 18328, loss: 0.0010064288508147001\n",
      "iteration 18329, loss: 0.0009814308723434806\n",
      "iteration 18330, loss: 0.0013224794529378414\n",
      "iteration 18331, loss: 0.0012049509678035975\n",
      "iteration 18332, loss: 0.0015294351615011692\n",
      "iteration 18333, loss: 0.001103223767131567\n",
      "iteration 18334, loss: 0.00113680271897465\n",
      "iteration 18335, loss: 0.001279689371585846\n",
      "iteration 18336, loss: 0.0012753920163959265\n",
      "iteration 18337, loss: 0.0010889775585383177\n",
      "iteration 18338, loss: 0.0012883429881185293\n",
      "iteration 18339, loss: 0.0012284033000469208\n",
      "iteration 18340, loss: 0.0011153414379805326\n",
      "iteration 18341, loss: 0.0011135403765365481\n",
      "iteration 18342, loss: 0.0010194199858233333\n",
      "iteration 18343, loss: 0.0011463973205536604\n",
      "iteration 18344, loss: 0.001235559000633657\n",
      "iteration 18345, loss: 0.001237597782164812\n",
      "iteration 18346, loss: 0.0012297407956793904\n",
      "iteration 18347, loss: 0.000913158874027431\n",
      "iteration 18348, loss: 0.00092063017655164\n",
      "iteration 18349, loss: 0.0010773756075650454\n",
      "iteration 18350, loss: 0.00131521956063807\n",
      "iteration 18351, loss: 0.0009709439473226666\n",
      "iteration 18352, loss: 0.0011901319958269596\n",
      "iteration 18353, loss: 0.0010486110113561153\n",
      "iteration 18354, loss: 0.001312802778556943\n",
      "iteration 18355, loss: 0.0010948504786938429\n",
      "iteration 18356, loss: 0.0011553445365279913\n",
      "iteration 18357, loss: 0.0012341859983280301\n",
      "iteration 18358, loss: 0.0011600832222029567\n",
      "iteration 18359, loss: 0.001282678684219718\n",
      "iteration 18360, loss: 0.0012076467974111438\n",
      "iteration 18361, loss: 0.0010501875076442957\n",
      "iteration 18362, loss: 0.0011357725597918034\n",
      "iteration 18363, loss: 0.0009869582718238235\n",
      "iteration 18364, loss: 0.0012275169137865305\n",
      "iteration 18365, loss: 0.0010267688194289804\n",
      "iteration 18366, loss: 0.0012001320719718933\n",
      "iteration 18367, loss: 0.0014243340119719505\n",
      "iteration 18368, loss: 0.000995586859062314\n",
      "iteration 18369, loss: 0.0011869679437950253\n",
      "iteration 18370, loss: 0.001279750606045127\n",
      "iteration 18371, loss: 0.0010478461626917124\n",
      "iteration 18372, loss: 0.0011869235895574093\n",
      "iteration 18373, loss: 0.0011245162459090352\n",
      "iteration 18374, loss: 0.0010036681778728962\n",
      "iteration 18375, loss: 0.0013225924922153354\n",
      "iteration 18376, loss: 0.0010294803651049733\n",
      "iteration 18377, loss: 0.0011922925477847457\n",
      "iteration 18378, loss: 0.0012643181253224611\n",
      "iteration 18379, loss: 0.0009675626643002033\n",
      "iteration 18380, loss: 0.0013734984677284956\n",
      "iteration 18381, loss: 0.001086959382519126\n",
      "iteration 18382, loss: 0.0008024240960367024\n",
      "iteration 18383, loss: 0.0011532881762832403\n",
      "iteration 18384, loss: 0.0012614113511517644\n",
      "iteration 18385, loss: 0.0012398529797792435\n",
      "iteration 18386, loss: 0.0011023059487342834\n",
      "iteration 18387, loss: 0.0011586430482566357\n",
      "iteration 18388, loss: 0.0011602116283029318\n",
      "iteration 18389, loss: 0.0011206793133169413\n",
      "iteration 18390, loss: 0.0015112587716430426\n",
      "iteration 18391, loss: 0.0014213717076927423\n",
      "iteration 18392, loss: 0.001082977163605392\n",
      "iteration 18393, loss: 0.0009464268805459142\n",
      "iteration 18394, loss: 0.001016297610476613\n",
      "iteration 18395, loss: 0.0010129453148692846\n",
      "iteration 18396, loss: 0.0012936738785356283\n",
      "iteration 18397, loss: 0.0011526012094691396\n",
      "iteration 18398, loss: 0.0014182525919750333\n",
      "iteration 18399, loss: 0.0010103033855557442\n",
      "iteration 18400, loss: 0.0012980676256120205\n",
      "iteration 18401, loss: 0.001086793839931488\n",
      "iteration 18402, loss: 0.0013398471055552363\n",
      "iteration 18403, loss: 0.0014085718430578709\n",
      "iteration 18404, loss: 0.0009803612483665347\n",
      "iteration 18405, loss: 0.0010389152448624372\n",
      "iteration 18406, loss: 0.0012551085092127323\n",
      "iteration 18407, loss: 0.0011135116219520569\n",
      "iteration 18408, loss: 0.0010974089382216334\n",
      "iteration 18409, loss: 0.001629912992939353\n",
      "iteration 18410, loss: 0.0010773963294923306\n",
      "iteration 18411, loss: 0.0010661507258191705\n",
      "iteration 18412, loss: 0.0011820390354841948\n",
      "iteration 18413, loss: 0.0009512632968835533\n",
      "iteration 18414, loss: 0.0011226332280784845\n",
      "iteration 18415, loss: 0.0010142766404896975\n",
      "iteration 18416, loss: 0.0009840535931289196\n",
      "iteration 18417, loss: 0.0014335659798234701\n",
      "iteration 18418, loss: 0.0011550167109817266\n",
      "iteration 18419, loss: 0.001036756788380444\n",
      "iteration 18420, loss: 0.0014902534894645214\n",
      "iteration 18421, loss: 0.0012138057500123978\n",
      "iteration 18422, loss: 0.0011037543881684542\n",
      "iteration 18423, loss: 0.0011968937469646335\n",
      "iteration 18424, loss: 0.0011157349217683077\n",
      "iteration 18425, loss: 0.0009634902235120535\n",
      "iteration 18426, loss: 0.001103023998439312\n",
      "iteration 18427, loss: 0.0012163878418505192\n",
      "iteration 18428, loss: 0.001450053183361888\n",
      "iteration 18429, loss: 0.0011189216747879982\n",
      "iteration 18430, loss: 0.0014301470946520567\n",
      "iteration 18431, loss: 0.0010696982499212027\n",
      "iteration 18432, loss: 0.0013030503178015351\n",
      "iteration 18433, loss: 0.0012968413066118956\n",
      "iteration 18434, loss: 0.0011812134180217981\n",
      "iteration 18435, loss: 0.0013972026063129306\n",
      "iteration 18436, loss: 0.0009709433070383966\n",
      "iteration 18437, loss: 0.0011613548267632723\n",
      "iteration 18438, loss: 0.0011429974110797048\n",
      "iteration 18439, loss: 0.0014829596038907766\n",
      "iteration 18440, loss: 0.0014574837405234575\n",
      "iteration 18441, loss: 0.0014429204165935516\n",
      "iteration 18442, loss: 0.0010535011533647776\n",
      "iteration 18443, loss: 0.0013804194750264287\n",
      "iteration 18444, loss: 0.0010741789592429996\n",
      "iteration 18445, loss: 0.0012462922604754567\n",
      "iteration 18446, loss: 0.0013441697228699923\n",
      "iteration 18447, loss: 0.0010297837434336543\n",
      "iteration 18448, loss: 0.0009609386324882507\n",
      "iteration 18449, loss: 0.0011933540226891637\n",
      "iteration 18450, loss: 0.0009702633833512664\n",
      "iteration 18451, loss: 0.0010725334286689758\n",
      "iteration 18452, loss: 0.0011295902077108622\n",
      "iteration 18453, loss: 0.0011399587383493781\n",
      "iteration 18454, loss: 0.0014979809056967497\n",
      "iteration 18455, loss: 0.0012435360113158822\n",
      "iteration 18456, loss: 0.0012658823980018497\n",
      "iteration 18457, loss: 0.0013022779021412134\n",
      "iteration 18458, loss: 0.0014046486467123032\n",
      "iteration 18459, loss: 0.0009998430032283068\n",
      "iteration 18460, loss: 0.0012906671036034822\n",
      "iteration 18461, loss: 0.001110609038732946\n",
      "iteration 18462, loss: 0.0012351080076768994\n",
      "iteration 18463, loss: 0.0014662542380392551\n",
      "iteration 18464, loss: 0.0014842238742858171\n",
      "iteration 18465, loss: 0.0012327437289059162\n",
      "iteration 18466, loss: 0.0010227614548057318\n",
      "iteration 18467, loss: 0.0014883008552715182\n",
      "iteration 18468, loss: 0.0013741918373852968\n",
      "iteration 18469, loss: 0.00105958036147058\n",
      "iteration 18470, loss: 0.0012772830668836832\n",
      "iteration 18471, loss: 0.0011114113731309772\n",
      "iteration 18472, loss: 0.0012701672967523336\n",
      "iteration 18473, loss: 0.0010346604976803064\n",
      "iteration 18474, loss: 0.0010058274492621422\n",
      "iteration 18475, loss: 0.001324312062934041\n",
      "iteration 18476, loss: 0.00117706716991961\n",
      "iteration 18477, loss: 0.0010377424769103527\n",
      "iteration 18478, loss: 0.0010670787887647748\n",
      "iteration 18479, loss: 0.0011261915788054466\n",
      "iteration 18480, loss: 0.0010799197480082512\n",
      "iteration 18481, loss: 0.0013931510038673878\n",
      "iteration 18482, loss: 0.0011823661625385284\n",
      "iteration 18483, loss: 0.00120900000911206\n",
      "iteration 18484, loss: 0.001151043688878417\n",
      "iteration 18485, loss: 0.0013347675558179617\n",
      "iteration 18486, loss: 0.0009277858771383762\n",
      "iteration 18487, loss: 0.0012838602997362614\n",
      "iteration 18488, loss: 0.0012706344714388251\n",
      "iteration 18489, loss: 0.0012795039219781756\n",
      "iteration 18490, loss: 0.001267150742933154\n",
      "iteration 18491, loss: 0.0011406487319618464\n",
      "iteration 18492, loss: 0.0011832425370812416\n",
      "iteration 18493, loss: 0.0012056238483637571\n",
      "iteration 18494, loss: 0.0013046050444245338\n",
      "iteration 18495, loss: 0.0012230079155415297\n",
      "iteration 18496, loss: 0.0012270845472812653\n",
      "iteration 18497, loss: 0.0008835612679831684\n",
      "iteration 18498, loss: 0.0011573455994948745\n",
      "iteration 18499, loss: 0.0010214203502982855\n",
      "iteration 18500, loss: 0.00108831818215549\n",
      "iteration 18501, loss: 0.0010505398968234658\n",
      "iteration 18502, loss: 0.0011663478799164295\n",
      "iteration 18503, loss: 0.0014545738231390715\n",
      "iteration 18504, loss: 0.001248617423698306\n",
      "iteration 18505, loss: 0.001177060417830944\n",
      "iteration 18506, loss: 0.0008896118961274624\n",
      "iteration 18507, loss: 0.0016278254333883524\n",
      "iteration 18508, loss: 0.0012251073494553566\n",
      "iteration 18509, loss: 0.0009553179843351245\n",
      "iteration 18510, loss: 0.0012994685675948858\n",
      "iteration 18511, loss: 0.0011926691513508558\n",
      "iteration 18512, loss: 0.0011172352824360132\n",
      "iteration 18513, loss: 0.0013551886659115553\n",
      "iteration 18514, loss: 0.0013144437689334154\n",
      "iteration 18515, loss: 0.001362059498205781\n",
      "iteration 18516, loss: 0.0012356867082417011\n",
      "iteration 18517, loss: 0.0013802859466522932\n",
      "iteration 18518, loss: 0.0012050899676978588\n",
      "iteration 18519, loss: 0.0012049725046381354\n",
      "iteration 18520, loss: 0.0011457642540335655\n",
      "iteration 18521, loss: 0.0017895371420308948\n",
      "iteration 18522, loss: 0.0009087015641853213\n",
      "iteration 18523, loss: 0.0011334603186696768\n",
      "iteration 18524, loss: 0.0012042687740176916\n",
      "iteration 18525, loss: 0.0013092209119349718\n",
      "iteration 18526, loss: 0.0010130319278687239\n",
      "iteration 18527, loss: 0.001360699418000877\n",
      "iteration 18528, loss: 0.0010989062720909715\n",
      "iteration 18529, loss: 0.001251631649211049\n",
      "iteration 18530, loss: 0.00135961570776999\n",
      "iteration 18531, loss: 0.0012148681562393904\n",
      "iteration 18532, loss: 0.0015287342248484492\n",
      "iteration 18533, loss: 0.0013171808095648885\n",
      "iteration 18534, loss: 0.0011061432305723429\n",
      "iteration 18535, loss: 0.001329688704572618\n",
      "iteration 18536, loss: 0.0012830945197492838\n",
      "iteration 18537, loss: 0.00104146811645478\n",
      "iteration 18538, loss: 0.0012501839082688093\n",
      "iteration 18539, loss: 0.0011849136790260673\n",
      "iteration 18540, loss: 0.001000372227281332\n",
      "iteration 18541, loss: 0.0007805260247550905\n",
      "iteration 18542, loss: 0.001314366701990366\n",
      "iteration 18543, loss: 0.0013288003392517567\n",
      "iteration 18544, loss: 0.001036487752571702\n",
      "iteration 18545, loss: 0.0013581367675215006\n",
      "iteration 18546, loss: 0.0012173531576991081\n",
      "iteration 18547, loss: 0.0012464193860068917\n",
      "iteration 18548, loss: 0.0010359259322285652\n",
      "iteration 18549, loss: 0.0011557906400412321\n",
      "iteration 18550, loss: 0.0011323131620883942\n",
      "iteration 18551, loss: 0.00103818962816149\n",
      "iteration 18552, loss: 0.0012360814725980163\n",
      "iteration 18553, loss: 0.0010485444217920303\n",
      "iteration 18554, loss: 0.0011304813669994473\n",
      "iteration 18555, loss: 0.0012453899253159761\n",
      "iteration 18556, loss: 0.000981830875389278\n",
      "iteration 18557, loss: 0.0010694058146327734\n",
      "iteration 18558, loss: 0.0013197838561609387\n",
      "iteration 18559, loss: 0.0011964160948991776\n",
      "iteration 18560, loss: 0.0010575084015727043\n",
      "iteration 18561, loss: 0.001379446592181921\n",
      "iteration 18562, loss: 0.0011386872502043843\n",
      "iteration 18563, loss: 0.0010014567524194717\n",
      "iteration 18564, loss: 0.0013080149656161666\n",
      "iteration 18565, loss: 0.0014183889143168926\n",
      "iteration 18566, loss: 0.00111834064591676\n",
      "iteration 18567, loss: 0.0012452895753085613\n",
      "iteration 18568, loss: 0.0013623032718896866\n",
      "iteration 18569, loss: 0.0011527466122061014\n",
      "iteration 18570, loss: 0.0010544117540121078\n",
      "iteration 18571, loss: 0.0010178567608818412\n",
      "iteration 18572, loss: 0.0011547893518581986\n",
      "iteration 18573, loss: 0.0010991520248353481\n",
      "iteration 18574, loss: 0.001592931803315878\n",
      "iteration 18575, loss: 0.0012137434678152204\n",
      "iteration 18576, loss: 0.0012727469438686967\n",
      "iteration 18577, loss: 0.001070450060069561\n",
      "iteration 18578, loss: 0.0012081516906619072\n",
      "iteration 18579, loss: 0.001165783149190247\n",
      "iteration 18580, loss: 0.0014190527144819498\n",
      "iteration 18581, loss: 0.001220286125317216\n",
      "iteration 18582, loss: 0.0010431818664073944\n",
      "iteration 18583, loss: 0.0009910271037369967\n",
      "iteration 18584, loss: 0.0010682280408218503\n",
      "iteration 18585, loss: 0.0013464358635246754\n",
      "iteration 18586, loss: 0.0011911597102880478\n",
      "iteration 18587, loss: 0.0012512251269072294\n",
      "iteration 18588, loss: 0.0011688009835779667\n",
      "iteration 18589, loss: 0.001098767388612032\n",
      "iteration 18590, loss: 0.0011198676656931639\n",
      "iteration 18591, loss: 0.0017441989621147513\n",
      "iteration 18592, loss: 0.000994436559267342\n",
      "iteration 18593, loss: 0.001267325016669929\n",
      "iteration 18594, loss: 0.001171401934698224\n",
      "iteration 18595, loss: 0.0010771651286631823\n",
      "iteration 18596, loss: 0.0011223065666854382\n",
      "iteration 18597, loss: 0.001100085093639791\n",
      "iteration 18598, loss: 0.0009571087430231273\n",
      "iteration 18599, loss: 0.0010140652302652597\n",
      "iteration 18600, loss: 0.0008693438139744103\n",
      "iteration 18601, loss: 0.0009470568038523197\n",
      "iteration 18602, loss: 0.0010719213169068098\n",
      "iteration 18603, loss: 0.001300953095778823\n",
      "iteration 18604, loss: 0.0011423928663134575\n",
      "iteration 18605, loss: 0.0009416347602382302\n",
      "iteration 18606, loss: 0.0013844827190041542\n",
      "iteration 18607, loss: 0.001072410959750414\n",
      "iteration 18608, loss: 0.001087239827029407\n",
      "iteration 18609, loss: 0.0011055977083742619\n",
      "iteration 18610, loss: 0.0011314821895211935\n",
      "iteration 18611, loss: 0.001278112642467022\n",
      "iteration 18612, loss: 0.0013763949973508716\n",
      "iteration 18613, loss: 0.0008931615157052875\n",
      "iteration 18614, loss: 0.0010807181242853403\n",
      "iteration 18615, loss: 0.0010660754051059484\n",
      "iteration 18616, loss: 0.0009594410075806081\n",
      "iteration 18617, loss: 0.0012511129025369883\n",
      "iteration 18618, loss: 0.001045768614858389\n",
      "iteration 18619, loss: 0.0010654802899807692\n",
      "iteration 18620, loss: 0.0013976141344755888\n",
      "iteration 18621, loss: 0.0010292928200215101\n",
      "iteration 18622, loss: 0.001220031175762415\n",
      "iteration 18623, loss: 0.001311963191255927\n",
      "iteration 18624, loss: 0.0011132278013974428\n",
      "iteration 18625, loss: 0.001092870719730854\n",
      "iteration 18626, loss: 0.0011406836565583944\n",
      "iteration 18627, loss: 0.0011646212078630924\n",
      "iteration 18628, loss: 0.0009305949206463993\n",
      "iteration 18629, loss: 0.0012144092470407486\n",
      "iteration 18630, loss: 0.0012676503974944353\n",
      "iteration 18631, loss: 0.0012635343009606004\n",
      "iteration 18632, loss: 0.0011934763751924038\n",
      "iteration 18633, loss: 0.0012972353724762797\n",
      "iteration 18634, loss: 0.0012317319633439183\n",
      "iteration 18635, loss: 0.001652128528803587\n",
      "iteration 18636, loss: 0.0011926518054679036\n",
      "iteration 18637, loss: 0.0010113974567502737\n",
      "iteration 18638, loss: 0.0012822388671338558\n",
      "iteration 18639, loss: 0.0010479259071871638\n",
      "iteration 18640, loss: 0.0012050720397382975\n",
      "iteration 18641, loss: 0.0011011587921530008\n",
      "iteration 18642, loss: 0.0009900998556986451\n",
      "iteration 18643, loss: 0.0011045413557440042\n",
      "iteration 18644, loss: 0.0013047063257545233\n",
      "iteration 18645, loss: 0.0011335762683302164\n",
      "iteration 18646, loss: 0.001113344682380557\n",
      "iteration 18647, loss: 0.0014576888643205166\n",
      "iteration 18648, loss: 0.0009929456282407045\n",
      "iteration 18649, loss: 0.0011115989182144403\n",
      "iteration 18650, loss: 0.0011303829960525036\n",
      "iteration 18651, loss: 0.001166612608358264\n",
      "iteration 18652, loss: 0.001410019351169467\n",
      "iteration 18653, loss: 0.001215506810694933\n",
      "iteration 18654, loss: 0.0012881220318377018\n",
      "iteration 18655, loss: 0.0010731497313827276\n",
      "iteration 18656, loss: 0.0011802992084994912\n",
      "iteration 18657, loss: 0.0011558174155652523\n",
      "iteration 18658, loss: 0.0010912378784269094\n",
      "iteration 18659, loss: 0.0010147897992283106\n",
      "iteration 18660, loss: 0.0010055258171632886\n",
      "iteration 18661, loss: 0.0011824502144008875\n",
      "iteration 18662, loss: 0.00098952348344028\n",
      "iteration 18663, loss: 0.0007878711330704391\n",
      "iteration 18664, loss: 0.0011472026817500591\n",
      "iteration 18665, loss: 0.0012671544682234526\n",
      "iteration 18666, loss: 0.0014506833394989371\n",
      "iteration 18667, loss: 0.0011204271577298641\n",
      "iteration 18668, loss: 0.0008997088298201561\n",
      "iteration 18669, loss: 0.0010649875039234757\n",
      "iteration 18670, loss: 0.0010644994908943772\n",
      "iteration 18671, loss: 0.0012179580517113209\n",
      "iteration 18672, loss: 0.0012523995246738195\n",
      "iteration 18673, loss: 0.0009682717500254512\n",
      "iteration 18674, loss: 0.0009047951316460967\n",
      "iteration 18675, loss: 0.0012327998410910368\n",
      "iteration 18676, loss: 0.001157080288976431\n",
      "iteration 18677, loss: 0.0011508578900247812\n",
      "iteration 18678, loss: 0.0008663195185363293\n",
      "iteration 18679, loss: 0.00104187848046422\n",
      "iteration 18680, loss: 0.0016237832605838776\n",
      "iteration 18681, loss: 0.000984810059890151\n",
      "iteration 18682, loss: 0.0012786558363586664\n",
      "iteration 18683, loss: 0.0011581586441025138\n",
      "iteration 18684, loss: 0.0011351716239005327\n",
      "iteration 18685, loss: 0.0009167154785245657\n",
      "iteration 18686, loss: 0.001140373875387013\n",
      "iteration 18687, loss: 0.0012941155582666397\n",
      "iteration 18688, loss: 0.0013260376872494817\n",
      "iteration 18689, loss: 0.0010393373668193817\n",
      "iteration 18690, loss: 0.001209357986226678\n",
      "iteration 18691, loss: 0.001110557932406664\n",
      "iteration 18692, loss: 0.0012885333271697164\n",
      "iteration 18693, loss: 0.0009916271083056927\n",
      "iteration 18694, loss: 0.0014047151198610663\n",
      "iteration 18695, loss: 0.0010939044877886772\n",
      "iteration 18696, loss: 0.0012454886455088854\n",
      "iteration 18697, loss: 0.001036916975863278\n",
      "iteration 18698, loss: 0.0009897755226120353\n",
      "iteration 18699, loss: 0.001325372140854597\n",
      "iteration 18700, loss: 0.0011213639518246055\n",
      "iteration 18701, loss: 0.0012089950032532215\n",
      "iteration 18702, loss: 0.0015289622824639082\n",
      "iteration 18703, loss: 0.0011121946154162288\n",
      "iteration 18704, loss: 0.0010738761629909277\n",
      "iteration 18705, loss: 0.0011972862994298339\n",
      "iteration 18706, loss: 0.001110508106648922\n",
      "iteration 18707, loss: 0.0012404086301103234\n",
      "iteration 18708, loss: 0.0010521886870265007\n",
      "iteration 18709, loss: 0.0011224087793380022\n",
      "iteration 18710, loss: 0.0011881692335009575\n",
      "iteration 18711, loss: 0.0011330598499625921\n",
      "iteration 18712, loss: 0.0010558725334703922\n",
      "iteration 18713, loss: 0.0010776733979582787\n",
      "iteration 18714, loss: 0.0009557476732879877\n",
      "iteration 18715, loss: 0.0011516029480844736\n",
      "iteration 18716, loss: 0.0010532501619309187\n",
      "iteration 18717, loss: 0.001171237789094448\n",
      "iteration 18718, loss: 0.0010139576625078917\n",
      "iteration 18719, loss: 0.0013073098380118608\n",
      "iteration 18720, loss: 0.0010103461099788547\n",
      "iteration 18721, loss: 0.0012726477580145001\n",
      "iteration 18722, loss: 0.001050874823704362\n",
      "iteration 18723, loss: 0.0009751596953719854\n",
      "iteration 18724, loss: 0.001156094716861844\n",
      "iteration 18725, loss: 0.0011196425184607506\n",
      "iteration 18726, loss: 0.0010287074837833643\n",
      "iteration 18727, loss: 0.0012709128204733133\n",
      "iteration 18728, loss: 0.0011937374947592616\n",
      "iteration 18729, loss: 0.0011209994554519653\n",
      "iteration 18730, loss: 0.0009636227623559535\n",
      "iteration 18731, loss: 0.0010696467943489552\n",
      "iteration 18732, loss: 0.0009496523998677731\n",
      "iteration 18733, loss: 0.0011730040423572063\n",
      "iteration 18734, loss: 0.0011715996079146862\n",
      "iteration 18735, loss: 0.0011567827314138412\n",
      "iteration 18736, loss: 0.001092451740987599\n",
      "iteration 18737, loss: 0.000992150278761983\n",
      "iteration 18738, loss: 0.0009884561877697706\n",
      "iteration 18739, loss: 0.0011298446916043758\n",
      "iteration 18740, loss: 0.0011714720167219639\n",
      "iteration 18741, loss: 0.0010450804838910699\n",
      "iteration 18742, loss: 0.001155592268332839\n",
      "iteration 18743, loss: 0.001085924101062119\n",
      "iteration 18744, loss: 0.0012203415390104055\n",
      "iteration 18745, loss: 0.0011507946765050292\n",
      "iteration 18746, loss: 0.001012070570141077\n",
      "iteration 18747, loss: 0.0011532183270901442\n",
      "iteration 18748, loss: 0.0010548431891947985\n",
      "iteration 18749, loss: 0.0010125299450010061\n",
      "iteration 18750, loss: 0.0011391646694391966\n",
      "iteration 18751, loss: 0.0008559745037928224\n",
      "iteration 18752, loss: 0.0010040339548140764\n",
      "iteration 18753, loss: 0.0010245924349874258\n",
      "iteration 18754, loss: 0.0011107079917564988\n",
      "iteration 18755, loss: 0.0009628745028749108\n",
      "iteration 18756, loss: 0.000989084946922958\n",
      "iteration 18757, loss: 0.0008540999842807651\n",
      "iteration 18758, loss: 0.0009098615846596658\n",
      "iteration 18759, loss: 0.0010524545796215534\n",
      "iteration 18760, loss: 0.001032817643135786\n",
      "iteration 18761, loss: 0.0010750804794952273\n",
      "iteration 18762, loss: 0.0009547032532282174\n",
      "iteration 18763, loss: 0.0009045371552929282\n",
      "iteration 18764, loss: 0.0012008317280560732\n",
      "iteration 18765, loss: 0.0011111559579148889\n",
      "iteration 18766, loss: 0.0011402445379644632\n",
      "iteration 18767, loss: 0.0009651449508965015\n",
      "iteration 18768, loss: 0.0012595757143571973\n",
      "iteration 18769, loss: 0.000841073109768331\n",
      "iteration 18770, loss: 0.0009982234332710505\n",
      "iteration 18771, loss: 0.0011125672608613968\n",
      "iteration 18772, loss: 0.001048542675562203\n",
      "iteration 18773, loss: 0.0009343870915472507\n",
      "iteration 18774, loss: 0.0009171718265861273\n",
      "iteration 18775, loss: 0.001027021324262023\n",
      "iteration 18776, loss: 0.001097209518775344\n",
      "iteration 18777, loss: 0.0011976953828707337\n",
      "iteration 18778, loss: 0.0009688129066489637\n",
      "iteration 18779, loss: 0.0009199188789352775\n",
      "iteration 18780, loss: 0.001126270042732358\n",
      "iteration 18781, loss: 0.001021798001602292\n",
      "iteration 18782, loss: 0.0015864723827689886\n",
      "iteration 18783, loss: 0.001091495156288147\n",
      "iteration 18784, loss: 0.0011261122999712825\n",
      "iteration 18785, loss: 0.0011426834389567375\n",
      "iteration 18786, loss: 0.0013701145071536303\n",
      "iteration 18787, loss: 0.001419341191649437\n",
      "iteration 18788, loss: 0.0011078065726906061\n",
      "iteration 18789, loss: 0.000880106701515615\n",
      "iteration 18790, loss: 0.0010968908900395036\n",
      "iteration 18791, loss: 0.001146127237007022\n",
      "iteration 18792, loss: 0.0014185839099809527\n",
      "iteration 18793, loss: 0.001190191600471735\n",
      "iteration 18794, loss: 0.0011476348154246807\n",
      "iteration 18795, loss: 0.001236181822605431\n",
      "iteration 18796, loss: 0.0010146702406927943\n",
      "iteration 18797, loss: 0.0010780184529721737\n",
      "iteration 18798, loss: 0.0010945536196231842\n",
      "iteration 18799, loss: 0.0012510123196989298\n",
      "iteration 18800, loss: 0.0009195730090141296\n",
      "iteration 18801, loss: 0.0011487475130707026\n",
      "iteration 18802, loss: 0.0008880506502464414\n",
      "iteration 18803, loss: 0.0011502435663715005\n",
      "iteration 18804, loss: 0.0011886367574334145\n",
      "iteration 18805, loss: 0.0009157820604741573\n",
      "iteration 18806, loss: 0.0009924834594130516\n",
      "iteration 18807, loss: 0.00110881426371634\n",
      "iteration 18808, loss: 0.0010120985098183155\n",
      "iteration 18809, loss: 0.0012129495153203607\n",
      "iteration 18810, loss: 0.0010647824965417385\n",
      "iteration 18811, loss: 0.000994207221083343\n",
      "iteration 18812, loss: 0.001269027590751648\n",
      "iteration 18813, loss: 0.0012924594338983297\n",
      "iteration 18814, loss: 0.0011072359047830105\n",
      "iteration 18815, loss: 0.001244858605787158\n",
      "iteration 18816, loss: 0.0013434170978143811\n",
      "iteration 18817, loss: 0.0010294781532138586\n",
      "iteration 18818, loss: 0.0010359580628573895\n",
      "iteration 18819, loss: 0.0012362950947135687\n",
      "iteration 18820, loss: 0.0009987052762880921\n",
      "iteration 18821, loss: 0.00107792008202523\n",
      "iteration 18822, loss: 0.0011354098096489906\n",
      "iteration 18823, loss: 0.0009483045432716608\n",
      "iteration 18824, loss: 0.0010517246555536985\n",
      "iteration 18825, loss: 0.0011695220600813627\n",
      "iteration 18826, loss: 0.00109368865378201\n",
      "iteration 18827, loss: 0.0011866060085594654\n",
      "iteration 18828, loss: 0.0011198834981769323\n",
      "iteration 18829, loss: 0.001156769460067153\n",
      "iteration 18830, loss: 0.000966676278039813\n",
      "iteration 18831, loss: 0.0012055878760293126\n",
      "iteration 18832, loss: 0.0010594086488708854\n",
      "iteration 18833, loss: 0.0013024193467572331\n",
      "iteration 18834, loss: 0.001199902966618538\n",
      "iteration 18835, loss: 0.0010134030599147081\n",
      "iteration 18836, loss: 0.0014139572158455849\n",
      "iteration 18837, loss: 0.0010949827264994383\n",
      "iteration 18838, loss: 0.0009514800040051341\n",
      "iteration 18839, loss: 0.0013244003057479858\n",
      "iteration 18840, loss: 0.001254927832633257\n",
      "iteration 18841, loss: 0.0010980477090924978\n",
      "iteration 18842, loss: 0.0012288253055885434\n",
      "iteration 18843, loss: 0.001115347957238555\n",
      "iteration 18844, loss: 0.0009169323602691293\n",
      "iteration 18845, loss: 0.0008949910406954587\n",
      "iteration 18846, loss: 0.0009457875275984406\n",
      "iteration 18847, loss: 0.0011771076824516058\n",
      "iteration 18848, loss: 0.0010316293919458985\n",
      "iteration 18849, loss: 0.0012154849246144295\n",
      "iteration 18850, loss: 0.0012160006444901228\n",
      "iteration 18851, loss: 0.0008755386807024479\n",
      "iteration 18852, loss: 0.0012743818806484342\n",
      "iteration 18853, loss: 0.0009913030080497265\n",
      "iteration 18854, loss: 0.0011824400862678885\n",
      "iteration 18855, loss: 0.0014140191487967968\n",
      "iteration 18856, loss: 0.0010121711529791355\n",
      "iteration 18857, loss: 0.0012011617654934525\n",
      "iteration 18858, loss: 0.0010156022617593408\n",
      "iteration 18859, loss: 0.0011957582319155335\n",
      "iteration 18860, loss: 0.0012872853549197316\n",
      "iteration 18861, loss: 0.0010186145082116127\n",
      "iteration 18862, loss: 0.0009473685640841722\n",
      "iteration 18863, loss: 0.0011126978788524866\n",
      "iteration 18864, loss: 0.0014553796499967575\n",
      "iteration 18865, loss: 0.0011453557526692748\n",
      "iteration 18866, loss: 0.0011469258461147547\n",
      "iteration 18867, loss: 0.0012380036059767008\n",
      "iteration 18868, loss: 0.001104788389056921\n",
      "iteration 18869, loss: 0.0009673811728134751\n",
      "iteration 18870, loss: 0.0009853378869593143\n",
      "iteration 18871, loss: 0.0012002866715192795\n",
      "iteration 18872, loss: 0.001214043004438281\n",
      "iteration 18873, loss: 0.0012501057935878634\n",
      "iteration 18874, loss: 0.0010104267857968807\n",
      "iteration 18875, loss: 0.0011868124129250646\n",
      "iteration 18876, loss: 0.0012543208431452513\n",
      "iteration 18877, loss: 0.0013701459392905235\n",
      "iteration 18878, loss: 0.0013687541941180825\n",
      "iteration 18879, loss: 0.0012060125591233373\n",
      "iteration 18880, loss: 0.0011735614389181137\n",
      "iteration 18881, loss: 0.0010596595238894224\n",
      "iteration 18882, loss: 0.001052199862897396\n",
      "iteration 18883, loss: 0.0012365272268652916\n",
      "iteration 18884, loss: 0.0010951622389256954\n",
      "iteration 18885, loss: 0.0015271678566932678\n",
      "iteration 18886, loss: 0.0011821428779512644\n",
      "iteration 18887, loss: 0.0012375349178910255\n",
      "iteration 18888, loss: 0.0013989738654345274\n",
      "iteration 18889, loss: 0.0010490138083696365\n",
      "iteration 18890, loss: 0.001509451074525714\n",
      "iteration 18891, loss: 0.0011539087863638997\n",
      "iteration 18892, loss: 0.0014101597480475903\n",
      "iteration 18893, loss: 0.0010254989610984921\n",
      "iteration 18894, loss: 0.0012068990617990494\n",
      "iteration 18895, loss: 0.0012954184785485268\n",
      "iteration 18896, loss: 0.0013524016831070185\n",
      "iteration 18897, loss: 0.0015174682484939694\n",
      "iteration 18898, loss: 0.0015339917736127973\n",
      "iteration 18899, loss: 0.0011138543486595154\n",
      "iteration 18900, loss: 0.0013740474823862314\n",
      "iteration 18901, loss: 0.001075205160304904\n",
      "iteration 18902, loss: 0.0011834920151159167\n",
      "iteration 18903, loss: 0.0010449871188029647\n",
      "iteration 18904, loss: 0.00104035553522408\n",
      "iteration 18905, loss: 0.0012598710600286722\n",
      "iteration 18906, loss: 0.001553668873384595\n",
      "iteration 18907, loss: 0.0012614617589861155\n",
      "iteration 18908, loss: 0.001217555021867156\n",
      "iteration 18909, loss: 0.0012247089762240648\n",
      "iteration 18910, loss: 0.0011886248830705881\n",
      "iteration 18911, loss: 0.0011445775162428617\n",
      "iteration 18912, loss: 0.0010081487707793713\n",
      "iteration 18913, loss: 0.0013331633526831865\n",
      "iteration 18914, loss: 0.000983349746093154\n",
      "iteration 18915, loss: 0.0012243521632626653\n",
      "iteration 18916, loss: 0.0013296945253387094\n",
      "iteration 18917, loss: 0.0011375020258128643\n",
      "iteration 18918, loss: 0.001240718294866383\n",
      "iteration 18919, loss: 0.0008280471665784717\n",
      "iteration 18920, loss: 0.001119877677410841\n",
      "iteration 18921, loss: 0.0009746051509864628\n",
      "iteration 18922, loss: 0.0010663894936442375\n",
      "iteration 18923, loss: 0.0012457463890314102\n",
      "iteration 18924, loss: 0.0009956042049452662\n",
      "iteration 18925, loss: 0.001385429291985929\n",
      "iteration 18926, loss: 0.0011664069024845958\n",
      "iteration 18927, loss: 0.0010337525745853782\n",
      "iteration 18928, loss: 0.001312466338276863\n",
      "iteration 18929, loss: 0.0014486955478787422\n",
      "iteration 18930, loss: 0.001315992558375001\n",
      "iteration 18931, loss: 0.0012829451588913798\n",
      "iteration 18932, loss: 0.0012167796958237886\n",
      "iteration 18933, loss: 0.0010446342639625072\n",
      "iteration 18934, loss: 0.0018091313540935516\n",
      "iteration 18935, loss: 0.001155231730081141\n",
      "iteration 18936, loss: 0.0009004674502648413\n",
      "iteration 18937, loss: 0.0012784582795575261\n",
      "iteration 18938, loss: 0.0014862848911434412\n",
      "iteration 18939, loss: 0.0012684892863035202\n",
      "iteration 18940, loss: 0.0013016578741371632\n",
      "iteration 18941, loss: 0.0011212026001885533\n",
      "iteration 18942, loss: 0.0012230542488396168\n",
      "iteration 18943, loss: 0.0013070980785414577\n",
      "iteration 18944, loss: 0.0010454083094373345\n",
      "iteration 18945, loss: 0.0011665304191410542\n",
      "iteration 18946, loss: 0.0011058039963245392\n",
      "iteration 18947, loss: 0.0011499910615384579\n",
      "iteration 18948, loss: 0.001093683997169137\n",
      "iteration 18949, loss: 0.00122162071056664\n",
      "iteration 18950, loss: 0.001125200535170734\n",
      "iteration 18951, loss: 0.0011428919387981296\n",
      "iteration 18952, loss: 0.0009715532069094479\n",
      "iteration 18953, loss: 0.0009047810453921556\n",
      "iteration 18954, loss: 0.0012883878080174327\n",
      "iteration 18955, loss: 0.00117296795360744\n",
      "iteration 18956, loss: 0.000902337022125721\n",
      "iteration 18957, loss: 0.0011000889353454113\n",
      "iteration 18958, loss: 0.001030326820909977\n",
      "iteration 18959, loss: 0.0009623930673114955\n",
      "iteration 18960, loss: 0.0008971645147539675\n",
      "iteration 18961, loss: 0.0011065348517149687\n",
      "iteration 18962, loss: 0.001144391018897295\n",
      "iteration 18963, loss: 0.001026360085234046\n",
      "iteration 18964, loss: 0.0011162551818415523\n",
      "iteration 18965, loss: 0.0011099119437858462\n",
      "iteration 18966, loss: 0.0010041971690952778\n",
      "iteration 18967, loss: 0.0010081257205456495\n",
      "iteration 18968, loss: 0.0012838328257203102\n",
      "iteration 18969, loss: 0.0010061298962682486\n",
      "iteration 18970, loss: 0.0010378547012805939\n",
      "iteration 18971, loss: 0.001065201940946281\n",
      "iteration 18972, loss: 0.0009654018213041127\n",
      "iteration 18973, loss: 0.0010602881666272879\n",
      "iteration 18974, loss: 0.0011093543143942952\n",
      "iteration 18975, loss: 0.0011285680811852217\n",
      "iteration 18976, loss: 0.001040874281898141\n",
      "iteration 18977, loss: 0.0009492522804066539\n",
      "iteration 18978, loss: 0.0009740104433149099\n",
      "iteration 18979, loss: 0.0013304047752171755\n",
      "iteration 18980, loss: 0.0011832958552986383\n",
      "iteration 18981, loss: 0.0012967955553904176\n",
      "iteration 18982, loss: 0.0012553730048239231\n",
      "iteration 18983, loss: 0.001025504432618618\n",
      "iteration 18984, loss: 0.0011374669848009944\n",
      "iteration 18985, loss: 0.0011152976658195257\n",
      "iteration 18986, loss: 0.0014078689273446798\n",
      "iteration 18987, loss: 0.0011455175699666142\n",
      "iteration 18988, loss: 0.0011257212609052658\n",
      "iteration 18989, loss: 0.0009910069638863206\n",
      "iteration 18990, loss: 0.0009701924282126129\n",
      "iteration 18991, loss: 0.0010686940513551235\n",
      "iteration 18992, loss: 0.001078458153642714\n",
      "iteration 18993, loss: 0.0012355824001133442\n",
      "iteration 18994, loss: 0.0010710882488638163\n",
      "iteration 18995, loss: 0.001114815124310553\n",
      "iteration 18996, loss: 0.0011246874928474426\n",
      "iteration 18997, loss: 0.0011984382290393114\n",
      "iteration 18998, loss: 0.0011619741562753916\n",
      "iteration 18999, loss: 0.001071098493412137\n",
      "iteration 19000, loss: 0.000967849453445524\n",
      "iteration 19001, loss: 0.0009850366041064262\n",
      "iteration 19002, loss: 0.0009418351692147553\n",
      "iteration 19003, loss: 0.0011577550321817398\n",
      "iteration 19004, loss: 0.0010425068903714418\n",
      "iteration 19005, loss: 0.0010374777484685183\n",
      "iteration 19006, loss: 0.0009278483921661973\n",
      "iteration 19007, loss: 0.0009860838763415813\n",
      "iteration 19008, loss: 0.0012559746392071247\n",
      "iteration 19009, loss: 0.0011835119221359491\n",
      "iteration 19010, loss: 0.0011575991520658135\n",
      "iteration 19011, loss: 0.0010786450002342463\n",
      "iteration 19012, loss: 0.0009756872314028442\n",
      "iteration 19013, loss: 0.001159392879344523\n",
      "iteration 19014, loss: 0.001191234914585948\n",
      "iteration 19015, loss: 0.0008941444684751332\n",
      "iteration 19016, loss: 0.001126165734604001\n",
      "iteration 19017, loss: 0.0011550127528607845\n",
      "iteration 19018, loss: 0.0015199215849861503\n",
      "iteration 19019, loss: 0.0012983721680939198\n",
      "iteration 19020, loss: 0.0012314176419749856\n",
      "iteration 19021, loss: 0.0012902171583846211\n",
      "iteration 19022, loss: 0.0012477985583245754\n",
      "iteration 19023, loss: 0.0008966012392193079\n",
      "iteration 19024, loss: 0.0011168508790433407\n",
      "iteration 19025, loss: 0.0009244895773008466\n",
      "iteration 19026, loss: 0.001171992626041174\n",
      "iteration 19027, loss: 0.000972353620454669\n",
      "iteration 19028, loss: 0.0011932034976780415\n",
      "iteration 19029, loss: 0.0012842166470363736\n",
      "iteration 19030, loss: 0.0011046627769246697\n",
      "iteration 19031, loss: 0.0011037109652534127\n",
      "iteration 19032, loss: 0.0011612672824412584\n",
      "iteration 19033, loss: 0.0010341479210183024\n",
      "iteration 19034, loss: 0.0012744971318170428\n",
      "iteration 19035, loss: 0.0011522951535880566\n",
      "iteration 19036, loss: 0.0012472121743485332\n",
      "iteration 19037, loss: 0.0013273516669869423\n",
      "iteration 19038, loss: 0.001033857697620988\n",
      "iteration 19039, loss: 0.000981782446615398\n",
      "iteration 19040, loss: 0.0009867165936157107\n",
      "iteration 19041, loss: 0.0011168427299708128\n",
      "iteration 19042, loss: 0.0013450783444568515\n",
      "iteration 19043, loss: 0.001174568897113204\n",
      "iteration 19044, loss: 0.0010026167146861553\n",
      "iteration 19045, loss: 0.001026443438604474\n",
      "iteration 19046, loss: 0.0011275820434093475\n",
      "iteration 19047, loss: 0.0011027902364730835\n",
      "iteration 19048, loss: 0.0012132773408666253\n",
      "iteration 19049, loss: 0.0010829897364601493\n",
      "iteration 19050, loss: 0.0012767468579113483\n",
      "iteration 19051, loss: 0.0012549164239317179\n",
      "iteration 19052, loss: 0.001333387102931738\n",
      "iteration 19053, loss: 0.0008900865213945508\n",
      "iteration 19054, loss: 0.0015044203028082848\n",
      "iteration 19055, loss: 0.001214695512317121\n",
      "iteration 19056, loss: 0.0012139810714870691\n",
      "iteration 19057, loss: 0.0012816318776458502\n",
      "iteration 19058, loss: 0.0011112617794424295\n",
      "iteration 19059, loss: 0.0014487623702734709\n",
      "iteration 19060, loss: 0.0015013772062957287\n",
      "iteration 19061, loss: 0.0013146857963874936\n",
      "iteration 19062, loss: 0.0014384894166141748\n",
      "iteration 19063, loss: 0.001231220318004489\n",
      "iteration 19064, loss: 0.0012447000481188297\n",
      "iteration 19065, loss: 0.0010670908959582448\n",
      "iteration 19066, loss: 0.0012457226403057575\n",
      "iteration 19067, loss: 0.0010392885887995362\n",
      "iteration 19068, loss: 0.0012527911458164454\n",
      "iteration 19069, loss: 0.001147344009950757\n",
      "iteration 19070, loss: 0.0011738284956663847\n",
      "iteration 19071, loss: 0.001040842616930604\n",
      "iteration 19072, loss: 0.0011637179413810372\n",
      "iteration 19073, loss: 0.001083976123481989\n",
      "iteration 19074, loss: 0.0011462384136393666\n",
      "iteration 19075, loss: 0.001153748482465744\n",
      "iteration 19076, loss: 0.0009400839917361736\n",
      "iteration 19077, loss: 0.001078509958460927\n",
      "iteration 19078, loss: 0.0010643011191859841\n",
      "iteration 19079, loss: 0.0011935889488086104\n",
      "iteration 19080, loss: 0.001295057823881507\n",
      "iteration 19081, loss: 0.00140287890098989\n",
      "iteration 19082, loss: 0.0011379225179553032\n",
      "iteration 19083, loss: 0.0008463377598673105\n",
      "iteration 19084, loss: 0.0009866214822977781\n",
      "iteration 19085, loss: 0.0010212245397269726\n",
      "iteration 19086, loss: 0.0012327813310548663\n",
      "iteration 19087, loss: 0.001127569004893303\n",
      "iteration 19088, loss: 0.001187824411317706\n",
      "iteration 19089, loss: 0.001137429615482688\n",
      "iteration 19090, loss: 0.00125372433103621\n",
      "iteration 19091, loss: 0.001051554223522544\n",
      "iteration 19092, loss: 0.0013363047037273645\n",
      "iteration 19093, loss: 0.001308221137151122\n",
      "iteration 19094, loss: 0.0013722480507567525\n",
      "iteration 19095, loss: 0.000973148737102747\n",
      "iteration 19096, loss: 0.0011370673310011625\n",
      "iteration 19097, loss: 0.0012317177606746554\n",
      "iteration 19098, loss: 0.0010719038546085358\n",
      "iteration 19099, loss: 0.001249547116458416\n",
      "iteration 19100, loss: 0.0011064981808885932\n",
      "iteration 19101, loss: 0.0009831974748522043\n",
      "iteration 19102, loss: 0.0012297122739255428\n",
      "iteration 19103, loss: 0.001204310217872262\n",
      "iteration 19104, loss: 0.0012804132420569658\n",
      "iteration 19105, loss: 0.0011181326117366552\n",
      "iteration 19106, loss: 0.0013993012253195047\n",
      "iteration 19107, loss: 0.0013457534369081259\n",
      "iteration 19108, loss: 0.0011569022899493575\n",
      "iteration 19109, loss: 0.0011533780489116907\n",
      "iteration 19110, loss: 0.0011599694844335318\n",
      "iteration 19111, loss: 0.0012831398053094745\n",
      "iteration 19112, loss: 0.0012248686980456114\n",
      "iteration 19113, loss: 0.0010917135514318943\n",
      "iteration 19114, loss: 0.0013139923103153706\n",
      "iteration 19115, loss: 0.0010897559113800526\n",
      "iteration 19116, loss: 0.0010531300213187933\n",
      "iteration 19117, loss: 0.0010591632453724742\n",
      "iteration 19118, loss: 0.0008634464466013014\n",
      "iteration 19119, loss: 0.0013989412691444159\n",
      "iteration 19120, loss: 0.001363975927233696\n",
      "iteration 19121, loss: 0.0009732082835398614\n",
      "iteration 19122, loss: 0.0010291235521435738\n",
      "iteration 19123, loss: 0.0011508121388033032\n",
      "iteration 19124, loss: 0.0012863981537520885\n",
      "iteration 19125, loss: 0.0012206609826534986\n",
      "iteration 19126, loss: 0.0010966842528432608\n",
      "iteration 19127, loss: 0.0013027680106461048\n",
      "iteration 19128, loss: 0.001007665996439755\n",
      "iteration 19129, loss: 0.0010255048982799053\n",
      "iteration 19130, loss: 0.0010111404117196798\n",
      "iteration 19131, loss: 0.0012743049301207066\n",
      "iteration 19132, loss: 0.0011797480983659625\n",
      "iteration 19133, loss: 0.0013712611980736256\n",
      "iteration 19134, loss: 0.0011313844006508589\n",
      "iteration 19135, loss: 0.0011471706675365567\n",
      "iteration 19136, loss: 0.0013080567587167025\n",
      "iteration 19137, loss: 0.0011739055626094341\n",
      "iteration 19138, loss: 0.0011143587762489915\n",
      "iteration 19139, loss: 0.001137048238888383\n",
      "iteration 19140, loss: 0.0009554842836223543\n",
      "iteration 19141, loss: 0.0011873207986354828\n",
      "iteration 19142, loss: 0.001172542804852128\n",
      "iteration 19143, loss: 0.0011426967103034258\n",
      "iteration 19144, loss: 0.0011720256879925728\n",
      "iteration 19145, loss: 0.0011475279461592436\n",
      "iteration 19146, loss: 0.001316584413871169\n",
      "iteration 19147, loss: 0.000929306959733367\n",
      "iteration 19148, loss: 0.0010381231550127268\n",
      "iteration 19149, loss: 0.0011672975961118937\n",
      "iteration 19150, loss: 0.0008681322215124965\n",
      "iteration 19151, loss: 0.0011793249286711216\n",
      "iteration 19152, loss: 0.000877687125466764\n",
      "iteration 19153, loss: 0.0012287992285564542\n",
      "iteration 19154, loss: 0.0010420763865113258\n",
      "iteration 19155, loss: 0.0012013192754238844\n",
      "iteration 19156, loss: 0.0011474629864096642\n",
      "iteration 19157, loss: 0.0010941762011498213\n",
      "iteration 19158, loss: 0.0013095568865537643\n",
      "iteration 19159, loss: 0.0010578499641269445\n",
      "iteration 19160, loss: 0.0009406793396919966\n",
      "iteration 19161, loss: 0.0009557456360198557\n",
      "iteration 19162, loss: 0.0010087236296385527\n",
      "iteration 19163, loss: 0.001083830138668418\n",
      "iteration 19164, loss: 0.0011403928510844707\n",
      "iteration 19165, loss: 0.000884136650711298\n",
      "iteration 19166, loss: 0.0010610586032271385\n",
      "iteration 19167, loss: 0.00104435789398849\n",
      "iteration 19168, loss: 0.0008982890285551548\n",
      "iteration 19169, loss: 0.0010459980694577098\n",
      "iteration 19170, loss: 0.0009339069365523756\n",
      "iteration 19171, loss: 0.0014331231359392405\n",
      "iteration 19172, loss: 0.0011082249693572521\n",
      "iteration 19173, loss: 0.0012761193793267012\n",
      "iteration 19174, loss: 0.0008888788288459182\n",
      "iteration 19175, loss: 0.001171815674751997\n",
      "iteration 19176, loss: 0.000989869935438037\n",
      "iteration 19177, loss: 0.001182863488793373\n",
      "iteration 19178, loss: 0.0010223786812275648\n",
      "iteration 19179, loss: 0.0010550389997661114\n",
      "iteration 19180, loss: 0.0010825940407812595\n",
      "iteration 19181, loss: 0.000952474249061197\n",
      "iteration 19182, loss: 0.0012717354111373425\n",
      "iteration 19183, loss: 0.0014117289101704955\n",
      "iteration 19184, loss: 0.0010396031429991126\n",
      "iteration 19185, loss: 0.0010568120051175356\n",
      "iteration 19186, loss: 0.0012017115950584412\n",
      "iteration 19187, loss: 0.0012424474116414785\n",
      "iteration 19188, loss: 0.0009659661445766687\n",
      "iteration 19189, loss: 0.0011474229395389557\n",
      "iteration 19190, loss: 0.0010546003468334675\n",
      "iteration 19191, loss: 0.0011095714289695024\n",
      "iteration 19192, loss: 0.0011260738829150796\n",
      "iteration 19193, loss: 0.0009581124177202582\n",
      "iteration 19194, loss: 0.0008612670353613794\n",
      "iteration 19195, loss: 0.001630405429750681\n",
      "iteration 19196, loss: 0.0012964833294972777\n",
      "iteration 19197, loss: 0.001414167694747448\n",
      "iteration 19198, loss: 0.0010236662346869707\n",
      "iteration 19199, loss: 0.0009393179789185524\n",
      "iteration 19200, loss: 0.0011697852751240134\n",
      "iteration 19201, loss: 0.001035561435855925\n",
      "iteration 19202, loss: 0.0011728272074833512\n",
      "iteration 19203, loss: 0.0013029613764956594\n",
      "iteration 19204, loss: 0.0010015361476689577\n",
      "iteration 19205, loss: 0.0011453641345724463\n",
      "iteration 19206, loss: 0.000884011504240334\n",
      "iteration 19207, loss: 0.001143174828030169\n",
      "iteration 19208, loss: 0.001163899665698409\n",
      "iteration 19209, loss: 0.001247145002707839\n",
      "iteration 19210, loss: 0.001302358927205205\n",
      "iteration 19211, loss: 0.0009604105725884438\n",
      "iteration 19212, loss: 0.0010786360362544656\n",
      "iteration 19213, loss: 0.0010370868258178234\n",
      "iteration 19214, loss: 0.0010458474280312657\n",
      "iteration 19215, loss: 0.0013050960842519999\n",
      "iteration 19216, loss: 0.0012901999289169908\n",
      "iteration 19217, loss: 0.0010093423770740628\n",
      "iteration 19218, loss: 0.0012346492148935795\n",
      "iteration 19219, loss: 0.0011267915833741426\n",
      "iteration 19220, loss: 0.0009553179261274636\n",
      "iteration 19221, loss: 0.0010522848460823298\n",
      "iteration 19222, loss: 0.0011034253984689713\n",
      "iteration 19223, loss: 0.0010410984978079796\n",
      "iteration 19224, loss: 0.0010553456377238035\n",
      "iteration 19225, loss: 0.0009680336806923151\n",
      "iteration 19226, loss: 0.001019056187942624\n",
      "iteration 19227, loss: 0.0010738221462816\n",
      "iteration 19228, loss: 0.0011409649159759283\n",
      "iteration 19229, loss: 0.0008977780234999955\n",
      "iteration 19230, loss: 0.0011637976858764887\n",
      "iteration 19231, loss: 0.0013365154154598713\n",
      "iteration 19232, loss: 0.0009116836590692401\n",
      "iteration 19233, loss: 0.0013875836739316583\n",
      "iteration 19234, loss: 0.000994318863376975\n",
      "iteration 19235, loss: 0.001160922460258007\n",
      "iteration 19236, loss: 0.0011351844295859337\n",
      "iteration 19237, loss: 0.0011061173863708973\n",
      "iteration 19238, loss: 0.001124449772760272\n",
      "iteration 19239, loss: 0.0009945903439074755\n",
      "iteration 19240, loss: 0.0012423070147633553\n",
      "iteration 19241, loss: 0.0013034751173108816\n",
      "iteration 19242, loss: 0.0011843841057270765\n",
      "iteration 19243, loss: 0.0010583181865513325\n",
      "iteration 19244, loss: 0.0008372536976821721\n",
      "iteration 19245, loss: 0.0010484326630830765\n",
      "iteration 19246, loss: 0.00131984893232584\n",
      "iteration 19247, loss: 0.0008918814128264785\n",
      "iteration 19248, loss: 0.0014230792876332998\n",
      "iteration 19249, loss: 0.001341561321169138\n",
      "iteration 19250, loss: 0.0009428222547285259\n",
      "iteration 19251, loss: 0.001032451749779284\n",
      "iteration 19252, loss: 0.0011367228580638766\n",
      "iteration 19253, loss: 0.0011363686062395573\n",
      "iteration 19254, loss: 0.0010104088578373194\n",
      "iteration 19255, loss: 0.0010238450486212969\n",
      "iteration 19256, loss: 0.0012834647204726934\n",
      "iteration 19257, loss: 0.0011070254258811474\n",
      "iteration 19258, loss: 0.0011097847018390894\n",
      "iteration 19259, loss: 0.0010567907011136413\n",
      "iteration 19260, loss: 0.0010561483213678002\n",
      "iteration 19261, loss: 0.0011880251113325357\n",
      "iteration 19262, loss: 0.0012684661196544766\n",
      "iteration 19263, loss: 0.001237867516465485\n",
      "iteration 19264, loss: 0.001224338891915977\n",
      "iteration 19265, loss: 0.0009644737001508474\n",
      "iteration 19266, loss: 0.0011741912458091974\n",
      "iteration 19267, loss: 0.0010240913834422827\n",
      "iteration 19268, loss: 0.0010085111716762185\n",
      "iteration 19269, loss: 0.0010379730956628919\n",
      "iteration 19270, loss: 0.0012437129626050591\n",
      "iteration 19271, loss: 0.0009992981795221567\n",
      "iteration 19272, loss: 0.001064635580405593\n",
      "iteration 19273, loss: 0.0011316852178424597\n",
      "iteration 19274, loss: 0.0010739152785390615\n",
      "iteration 19275, loss: 0.0011761204805225134\n",
      "iteration 19276, loss: 0.0011945073492825031\n",
      "iteration 19277, loss: 0.0011711118277162313\n",
      "iteration 19278, loss: 0.0011839126236736774\n",
      "iteration 19279, loss: 0.0010615315986797214\n",
      "iteration 19280, loss: 0.0011960454285144806\n",
      "iteration 19281, loss: 0.0011034670751541853\n",
      "iteration 19282, loss: 0.0011629676446318626\n",
      "iteration 19283, loss: 0.0012361216358840466\n",
      "iteration 19284, loss: 0.0011706054210662842\n",
      "iteration 19285, loss: 0.0009581155609339476\n",
      "iteration 19286, loss: 0.001029482576996088\n",
      "iteration 19287, loss: 0.0013163647381588817\n",
      "iteration 19288, loss: 0.0011712964624166489\n",
      "iteration 19289, loss: 0.001131286146119237\n",
      "iteration 19290, loss: 0.001056163921020925\n",
      "iteration 19291, loss: 0.0012265401892364025\n",
      "iteration 19292, loss: 0.0010983155807480216\n",
      "iteration 19293, loss: 0.0010115900076925755\n",
      "iteration 19294, loss: 0.0009830843191593885\n",
      "iteration 19295, loss: 0.0012222565710544586\n",
      "iteration 19296, loss: 0.0008743172511458397\n",
      "iteration 19297, loss: 0.0013582026585936546\n",
      "iteration 19298, loss: 0.0010195595677942038\n",
      "iteration 19299, loss: 0.001120086177252233\n",
      "iteration 19300, loss: 0.0009129250538535416\n",
      "iteration 19301, loss: 0.0011306896340101957\n",
      "iteration 19302, loss: 0.0009164420771412551\n",
      "iteration 19303, loss: 0.0010104588000103831\n",
      "iteration 19304, loss: 0.001411764184013009\n",
      "iteration 19305, loss: 0.0009674307657405734\n",
      "iteration 19306, loss: 0.0014069348108023405\n",
      "iteration 19307, loss: 0.0010931541910395026\n",
      "iteration 19308, loss: 0.0009211500291712582\n",
      "iteration 19309, loss: 0.0011435423512011766\n",
      "iteration 19310, loss: 0.0009122734190896153\n",
      "iteration 19311, loss: 0.0012056829873472452\n",
      "iteration 19312, loss: 0.0010693320073187351\n",
      "iteration 19313, loss: 0.0011183840688318014\n",
      "iteration 19314, loss: 0.001094306237064302\n",
      "iteration 19315, loss: 0.00103041622787714\n",
      "iteration 19316, loss: 0.0009531851392239332\n",
      "iteration 19317, loss: 0.0009770363103598356\n",
      "iteration 19318, loss: 0.0011021919781342149\n",
      "iteration 19319, loss: 0.0008815997280180454\n",
      "iteration 19320, loss: 0.0010698063997551799\n",
      "iteration 19321, loss: 0.0010425574146211147\n",
      "iteration 19322, loss: 0.000900734681636095\n",
      "iteration 19323, loss: 0.0011863622348755598\n",
      "iteration 19324, loss: 0.001089391065761447\n",
      "iteration 19325, loss: 0.0008266125805675983\n",
      "iteration 19326, loss: 0.0012119081802666187\n",
      "iteration 19327, loss: 0.0010814759880304337\n",
      "iteration 19328, loss: 0.0009136005537584424\n",
      "iteration 19329, loss: 0.0011794714955613017\n",
      "iteration 19330, loss: 0.0011295427102595568\n",
      "iteration 19331, loss: 0.0011088093742728233\n",
      "iteration 19332, loss: 0.0011903232662007213\n",
      "iteration 19333, loss: 0.0008925242582336068\n",
      "iteration 19334, loss: 0.0011381050571799278\n",
      "iteration 19335, loss: 0.0009677876951172948\n",
      "iteration 19336, loss: 0.0012144860811531544\n",
      "iteration 19337, loss: 0.0010411787079647183\n",
      "iteration 19338, loss: 0.00103462440893054\n",
      "iteration 19339, loss: 0.0014017446665093303\n",
      "iteration 19340, loss: 0.0010699988342821598\n",
      "iteration 19341, loss: 0.0016795069677755237\n",
      "iteration 19342, loss: 0.0009831417119130492\n",
      "iteration 19343, loss: 0.0012192635331302881\n",
      "iteration 19344, loss: 0.001181960105895996\n",
      "iteration 19345, loss: 0.001156904036179185\n",
      "iteration 19346, loss: 0.0009624306112527847\n",
      "iteration 19347, loss: 0.001092883525416255\n",
      "iteration 19348, loss: 0.0012584773357957602\n",
      "iteration 19349, loss: 0.001064535928890109\n",
      "iteration 19350, loss: 0.0009722545510157943\n",
      "iteration 19351, loss: 0.0013068888802081347\n",
      "iteration 19352, loss: 0.0011295423610135913\n",
      "iteration 19353, loss: 0.0010148268193006516\n",
      "iteration 19354, loss: 0.0012775023933500051\n",
      "iteration 19355, loss: 0.0012069500517100096\n",
      "iteration 19356, loss: 0.0012674791505560279\n",
      "iteration 19357, loss: 0.001101369271054864\n",
      "iteration 19358, loss: 0.0008874748018570244\n",
      "iteration 19359, loss: 0.0012616215972229838\n",
      "iteration 19360, loss: 0.0012723506661131978\n",
      "iteration 19361, loss: 0.0010911459103226662\n",
      "iteration 19362, loss: 0.001290256273932755\n",
      "iteration 19363, loss: 0.0009235097095370293\n",
      "iteration 19364, loss: 0.0010967638809233904\n",
      "iteration 19365, loss: 0.0012298058718442917\n",
      "iteration 19366, loss: 0.0011583487503230572\n",
      "iteration 19367, loss: 0.0010656106751412153\n",
      "iteration 19368, loss: 0.0013614953495562077\n",
      "iteration 19369, loss: 0.001113055506721139\n",
      "iteration 19370, loss: 0.0010497551411390305\n",
      "iteration 19371, loss: 0.0013005903456360102\n",
      "iteration 19372, loss: 0.0010871447157114744\n",
      "iteration 19373, loss: 0.0011517328675836325\n",
      "iteration 19374, loss: 0.001107282703742385\n",
      "iteration 19375, loss: 0.001141123939305544\n",
      "iteration 19376, loss: 0.0011582273291423917\n",
      "iteration 19377, loss: 0.0012640529312193394\n",
      "iteration 19378, loss: 0.0012332373298704624\n",
      "iteration 19379, loss: 0.0009125116630457342\n",
      "iteration 19380, loss: 0.0011576958931982517\n",
      "iteration 19381, loss: 0.0009848149493336678\n",
      "iteration 19382, loss: 0.0009498841245658696\n",
      "iteration 19383, loss: 0.001473432988859713\n",
      "iteration 19384, loss: 0.0013087608385831118\n",
      "iteration 19385, loss: 0.001201324979774654\n",
      "iteration 19386, loss: 0.0009512128308415413\n",
      "iteration 19387, loss: 0.001602275064215064\n",
      "iteration 19388, loss: 0.0011080914409831166\n",
      "iteration 19389, loss: 0.0012750253081321716\n",
      "iteration 19390, loss: 0.0009638813207857311\n",
      "iteration 19391, loss: 0.0009461138397455215\n",
      "iteration 19392, loss: 0.001161094638518989\n",
      "iteration 19393, loss: 0.0010316941188648343\n",
      "iteration 19394, loss: 0.0011564968153834343\n",
      "iteration 19395, loss: 0.001104103634133935\n",
      "iteration 19396, loss: 0.0012353756465017796\n",
      "iteration 19397, loss: 0.0009872033260762691\n",
      "iteration 19398, loss: 0.0011587736662477255\n",
      "iteration 19399, loss: 0.0015682998346164823\n",
      "iteration 19400, loss: 0.0010457831667736173\n",
      "iteration 19401, loss: 0.0015963847981765866\n",
      "iteration 19402, loss: 0.0013317563571035862\n",
      "iteration 19403, loss: 0.0011716309236362576\n",
      "iteration 19404, loss: 0.0011680378811433911\n",
      "iteration 19405, loss: 0.0014628097414970398\n",
      "iteration 19406, loss: 0.0009509020019322634\n",
      "iteration 19407, loss: 0.001459472463466227\n",
      "iteration 19408, loss: 0.001076748943887651\n",
      "iteration 19409, loss: 0.001194875338114798\n",
      "iteration 19410, loss: 0.0016022452618926764\n",
      "iteration 19411, loss: 0.001089242985472083\n",
      "iteration 19412, loss: 0.0009379740222357213\n",
      "iteration 19413, loss: 0.0011271380353718996\n",
      "iteration 19414, loss: 0.0012658641207963228\n",
      "iteration 19415, loss: 0.001625613309442997\n",
      "iteration 19416, loss: 0.0013501819921657443\n",
      "iteration 19417, loss: 0.0013320938451215625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(output[:, \u001b[38;5;241m0\u001b[39m], targets[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     27\u001b[0m adam\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m adam\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/rail1/src/cuda/.venv/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rail1/src/cuda/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "num_samples_per_batch = 1024\n",
    "\n",
    "def reflective_sin_emb(x, dim):\n",
    "    result = []\n",
    "    for i in range(1, dim + 1, 2):\n",
    "        result.append(torch.sin(i / 2 * torch.pi * x))\n",
    "    return torch.stack(result, dim=-1)\n",
    "\n",
    "i = 0 \n",
    "while True:\n",
    "\n",
    "    # random coordinates in [-1, 1]\n",
    "    random_coordinates = torch.rand(batch_size, num_samples_per_batch, 2, device='cuda') * 2 - 1\n",
    "    # F.grid_sample(image_tensor.unsqueeze(0).unsqueeze(0), torch.tensor([[[[1., 1.]]]], dtype=torch.float32), align_corners=True, mode='nearest')\n",
    "    targets = F.grid_sample(image_tensor.unsqueeze(0).unsqueeze(0), random_coordinates.unsqueeze(2), align_corners=True, mode='nearest')\n",
    "    targets = targets.squeeze(1).squeeze(-1)\n",
    "\n",
    "    random_coordinates = random_coordinates.squeeze(0)\n",
    "\n",
    "    embedded_coordinates = torch.cat([reflective_sin_emb(random_coordinates[:, 0], 1024), reflective_sin_emb(random_coordinates[:, 1], 1024)], dim=-1)\n",
    "\n",
    "    output = mlp(embedded_coordinates)\n",
    "    \n",
    "    loss = F.mse_loss(output[:, 0], targets[0])\n",
    "\n",
    "    adam.zero_grad()\n",
    "    loss.backward()\n",
    "    adam.step()\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        print(f'iteration {i}, loss: {loss.item()}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# meshgrid between -1 and 1\n",
    "size = 128\n",
    "repeats = 16\n",
    "x = torch.linspace(-1, -1 + repeats * 2, size * repeats)\n",
    "y = torch.linspace(-1, -1 + repeats * 2, size * repeats)\n",
    "x, y = torch.meshgrid(x, y, indexing='xy')\n",
    "\n",
    "mlp = mlp.cpu()\n",
    "\n",
    "# evaluate the model on the meshgrid\n",
    "with torch.no_grad():\n",
    "    embedded_coordinates = torch.cat([reflective_sin_emb(x.reshape(-1), 1024), reflective_sin_emb(y.reshape(-1), 1024)], dim=-1).cpu()\n",
    "    output = mlp(embedded_coordinates)\n",
    "    output = output.view(size * repeats, size * repeats)\n",
    "\n",
    "plt.imshow(output.cpu(), cmap='gray')\n",
    "plt.savefig(\"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
