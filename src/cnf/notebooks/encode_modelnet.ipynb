{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/druhe/rail1/src/cnf\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/druhe/rail1/src/cnf/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /home/druhe/rail1/src/cnf\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['DATAROOT'] = '/home/druhe/datasets'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import rail1.run\n",
    "import rail1.checkpoint\n",
    "import rail1.utils\n",
    "import tempfile\n",
    "import models\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "rail1.utils.set_seed(0, deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "path = 'druhe/cnf/hiq9zm8b'\n",
    "run = api.run(path)\n",
    "run_dir = os.path.join('runs', 'notebooks', run.id)\n",
    "os.makedirs(run_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "artifact_dict = {}\n",
    "\n",
    "for a in run.logged_artifacts():\n",
    "\n",
    "    if a.type != \"checkpoint\":\n",
    "        continue\n",
    "\n",
    "    file = a.metadata['filename']\n",
    "\n",
    "    metrics = file.rsplit('/')[-1]\n",
    "\n",
    "    match = re.search(r\"-val_neg_iou=(-\\d+\\.\\d+)\", metrics)\n",
    "\n",
    "    neg_iou = float(match.group(1)) if match else None\n",
    "\n",
    "    dictionary = {'neg_iou': neg_iou}\n",
    "\n",
    "    dictionary['artifact'] = a\n",
    "\n",
    "    artifact_dict[a] = dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_dict = {k: v for k, v in artifact_dict.items() if v['neg_iou'] is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_artifact = min(artifact_dict, key=lambda x: artifact_dict[x]['neg_iou'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'runs/notebooks/hiq9zm8b/checkpoints'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_artifact.download(root=os.path.join(run_dir, 'checkpoints'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = run.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getattr(models, config[\"model\"].pop(\"name\"))(**config[\"model\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully restored complete state from: runs/notebooks/hiq9zm8b/checkpoints/step=77824-epoch=0-val_loss=15.7940-val_neg_iou=-0.9304-val_s_it=0.0249.pt\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'global_step': 77824,\n",
       " 'last_global_step': 77824,\n",
       " 'should_raise': None,\n",
       " 'current_epoch': 0,\n",
       " 'device': device(type='cuda', index=0),\n",
       " 'train_metrics': defaultdict(list,\n",
       "             {'loss': [tensor([16.1938, 13.6535, 17.8294, 18.0112, 15.9553, 14.8408, 10.8084, 21.1689,\n",
       "                       11.5269, 10.3226, 15.3130, 12.8669, 10.6736, 12.6412,  9.1542, 11.2532],\n",
       "                      device='cuda:0'),\n",
       "               tensor([ 8.7740,  8.1999, 22.9469, 23.3657,  8.6043, 26.4167, 10.0762, 12.9795,\n",
       "                       13.7604, 20.2086,  9.9927, 27.4175, 25.3610, 15.3054, 12.0219, 13.4289],\n",
       "                      device='cuda:0'),\n",
       "               tensor([11.0404, 30.1185, 15.7833,  8.8691,  7.5659,  9.6644,  8.2191,  9.6064,\n",
       "                       11.0079,  8.1241, 20.0432,  9.3383, 10.7815, 18.7779, 11.4723, 27.7128],\n",
       "                      device='cuda:0'),\n",
       "               tensor([15.1763, 21.0724, 11.4410, 20.2192, 13.4554, 11.9887, 10.9133, 25.9882,\n",
       "                       12.8582, 13.1301, 16.1605, 26.5512, 31.6584,  8.9636, 21.9447, 21.8138],\n",
       "                      device='cuda:0'),\n",
       "               tensor([13.2134, 30.9466,  9.6201,  8.7711,  7.2174, 17.9203, 22.9545,  9.2439,\n",
       "                       17.7609, 16.2751, 14.9993, 14.3790,  6.9363,  9.3893, 13.2391, 11.9453],\n",
       "                      device='cuda:0'),\n",
       "               tensor([33.1388,  7.9776, 12.5607,  6.0293, 10.2317, 16.6672, 18.9064, 14.2502,\n",
       "                       16.1564, 10.5370, 10.3654, 17.6460, 13.0819, 10.4650, 12.4791, 16.9077],\n",
       "                      device='cuda:0'),\n",
       "               tensor([13.2141, 11.7702, 13.3825, 13.0521, 11.7441, 15.0074, 10.0546, 10.2754,\n",
       "                       18.0360, 15.4938, 11.8910, 11.6900, 19.2515, 15.2550, 12.1850, 13.4147],\n",
       "                      device='cuda:0'),\n",
       "               tensor([14.3231, 16.5407,  8.8738, 11.1107,  7.0553, 24.2683,  7.4041, 13.3987,\n",
       "                       15.8374,  9.3869, 10.7426, 12.3917, 21.4227, 24.1644, 11.0184, 21.6171],\n",
       "                      device='cuda:0'),\n",
       "               tensor([20.7782, 11.6070, 44.6948, 34.0288, 15.3155,  8.9114, 20.9370, 16.4048,\n",
       "                       11.6567, 23.7603, 12.5971, 24.7395, 12.1216, 18.6422, 10.3391,  6.3541],\n",
       "                      device='cuda:0'),\n",
       "               tensor([12.3709,  7.1680, 18.6438, 11.7945,  9.7191, 13.4542, 14.4083, 24.5705,\n",
       "                       19.3358, 22.4556, 33.4745, 13.9916, 11.3795, 18.2080, 72.4237, 13.4443],\n",
       "                      device='cuda:0'),\n",
       "               tensor([18.3307,  9.5193, 14.6333, 12.8098, 16.7891, 10.8061, 11.3283, 11.6719,\n",
       "                       33.4194,  8.5132, 12.2864, 26.0205, 13.1960, 20.8582, 17.1170,  7.8256],\n",
       "                      device='cuda:0'),\n",
       "               tensor([12.6218, 13.4156, 30.0796, 10.1186, 18.2114, 32.1762,  6.7185, 19.3712,\n",
       "                       14.8888,  6.4438,  8.4221, 17.8657, 21.2736, 10.9245, 21.2113, 18.4231],\n",
       "                      device='cuda:0'),\n",
       "               tensor([28.0051, 17.7429, 15.0658, 10.5795,  8.7010, 33.6552,  5.0266,  7.6079,\n",
       "                       11.4235, 10.8806,  9.9996, 13.4450, 19.8376, 10.5096, 11.4294, 28.5174],\n",
       "                      device='cuda:0'),\n",
       "               tensor([16.1558, 12.5592, 20.4404, 24.8975, 13.0331, 10.5091, 31.4838, 16.1614,\n",
       "                       29.1994, 21.4986, 30.5652, 17.4766,  8.5716, 21.4645, 13.1730, 12.2816],\n",
       "                      device='cuda:0'),\n",
       "               tensor([ 9.4808, 24.5169, 12.2530, 17.4895, 14.8540, 12.3988, 28.0091, 14.3604,\n",
       "                        9.9465, 10.7783, 29.9684, 20.4348, 14.7266, 12.4843,  9.5723,  8.2995],\n",
       "                      device='cuda:0'),\n",
       "               tensor([30.4288, 18.5888, 20.8358, 12.9300, 14.9251, 12.4944, 37.1100, 14.3981,\n",
       "                       21.7040, 21.2591, 13.2682, 22.6822, 14.8615, 18.3472,  7.2206, 14.3634],\n",
       "                      device='cuda:0'),\n",
       "               tensor([18.3721, 10.0768, 18.7359, 11.7155, 13.8964, 17.6902, 25.4696, 10.5964,\n",
       "                        9.7975, 18.8799, 17.6659, 23.4082, 24.0670, 11.5685, 12.4046, 18.9710],\n",
       "                      device='cuda:0'),\n",
       "               tensor([19.4631, 13.0491, 14.6236, 13.0809, 18.1587, 12.5204, 14.2873, 20.2692,\n",
       "                        9.9789, 14.4598,  8.7922,  9.8993, 14.6439, 10.5103, 12.9061, 10.1387],\n",
       "                      device='cuda:0'),\n",
       "               tensor([ 9.8656, 11.9806, 24.2936, 25.4676,  8.9601, 12.5516,  9.3707, 24.9395,\n",
       "                       19.0568, 17.7164, 19.4189,  9.4419, 15.9822, 17.6751, 19.3657, 15.6568],\n",
       "                      device='cuda:0'),\n",
       "               tensor([11.4028,  9.1507, 20.1653, 24.1168,  8.3142, 20.2219, 10.4388, 14.0920,\n",
       "                       13.7150, 15.1492, 19.9867, 10.3005, 14.2766, 33.6874, 12.9938, 24.5350],\n",
       "                      device='cuda:0'),\n",
       "               tensor([18.2558, 24.9782, 15.4914, 13.3252, 10.3093, 53.4702,  8.6031, 12.6683,\n",
       "                       13.5328, 27.2718, 16.8339, 16.7446,  9.0334, 11.9703, 19.5047, 11.6401],\n",
       "                      device='cuda:0'),\n",
       "               tensor([ 7.6592, 10.2538,  5.9768, 32.8125,  7.5449,  7.0882, 17.7505, 27.4655,\n",
       "                       21.5708, 16.8114, 10.2592, 18.4538, 12.6291, 11.2568, 14.4035, 37.3519],\n",
       "                      device='cuda:0'),\n",
       "               tensor([ 7.0155, 15.1559, 11.6808, 14.4157, 10.3882, 23.9150, 10.0897, 17.5317,\n",
       "                       10.0948,  9.8552,  9.3130, 18.8348, 28.9601, 24.9267,  6.4683, 10.9792],\n",
       "                      device='cuda:0'),\n",
       "               tensor([11.0810, 13.2344, 13.2256, 23.6447, 23.6018, 16.4653, 12.2913, 42.8036,\n",
       "                       17.3108, 14.7738, 19.3258, 16.8185, 11.5289, 11.3073, 12.6054, 10.1138],\n",
       "                      device='cuda:0')],\n",
       "              'parameter_norm': [tensor(84.0714, device='cuda:0'),\n",
       "               tensor(84.0715, device='cuda:0'),\n",
       "               tensor(84.0715, device='cuda:0'),\n",
       "               tensor(84.0717, device='cuda:0'),\n",
       "               tensor(84.0719, device='cuda:0'),\n",
       "               tensor(84.0721, device='cuda:0'),\n",
       "               tensor(84.0722, device='cuda:0'),\n",
       "               tensor(84.0724, device='cuda:0'),\n",
       "               tensor(84.0727, device='cuda:0'),\n",
       "               tensor(84.0730, device='cuda:0'),\n",
       "               tensor(84.0733, device='cuda:0'),\n",
       "               tensor(84.0734, device='cuda:0'),\n",
       "               tensor(84.0736, device='cuda:0'),\n",
       "               tensor(84.0739, device='cuda:0'),\n",
       "               tensor(84.0741, device='cuda:0'),\n",
       "               tensor(84.0743, device='cuda:0'),\n",
       "               tensor(84.0744, device='cuda:0'),\n",
       "               tensor(84.0746, device='cuda:0'),\n",
       "               tensor(84.0749, device='cuda:0'),\n",
       "               tensor(84.0751, device='cuda:0'),\n",
       "               tensor(84.0752, device='cuda:0'),\n",
       "               tensor(84.0754, device='cuda:0'),\n",
       "               tensor(84.0756, device='cuda:0'),\n",
       "               tensor(84.0759, device='cuda:0')],\n",
       "              'gradient_norm': [tensor(258.7522, device='cuda:0'),\n",
       "               tensor(1370.3270, device='cuda:0'),\n",
       "               tensor(300.3107, device='cuda:0'),\n",
       "               tensor(746.1696, device='cuda:0'),\n",
       "               tensor(768.8132, device='cuda:0'),\n",
       "               tensor(269.2733, device='cuda:0'),\n",
       "               tensor(1576.0756, device='cuda:0'),\n",
       "               tensor(518.1619, device='cuda:0'),\n",
       "               tensor(1436.6935, device='cuda:0'),\n",
       "               tensor(828.0152, device='cuda:0'),\n",
       "               tensor(1407.9290, device='cuda:0'),\n",
       "               tensor(1173.6434, device='cuda:0'),\n",
       "               tensor(1080.6602, device='cuda:0'),\n",
       "               tensor(1508.4397, device='cuda:0'),\n",
       "               tensor(311.7126, device='cuda:0'),\n",
       "               tensor(1268.9740, device='cuda:0'),\n",
       "               tensor(1014.1456, device='cuda:0'),\n",
       "               tensor(873.1339, device='cuda:0'),\n",
       "               tensor(1539.2944, device='cuda:0'),\n",
       "               tensor(1523.1456, device='cuda:0'),\n",
       "               tensor(468.7985, device='cuda:0'),\n",
       "               tensor(706.2479, device='cuda:0'),\n",
       "               tensor(546.2040, device='cuda:0'),\n",
       "               tensor(446.2175, device='cuda:0')]}),\n",
       " 'total_parameters': 3372737,\n",
       " 'starting_time': 1709545613.1998827,\n",
       " 'batch_index': 40}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_state = {}\n",
    "optimizer = None\n",
    "rail1.checkpoint.load_checkpoint(os.path.join(run_dir, 'checkpoints'), model, train_state=train_state, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = {'name': 'modelnet40'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelnet_files(path):\n",
    "\n",
    "    num_classes = 40\n",
    "\n",
    "    def fetch_files(filelist):\n",
    "        return [item.strip() for item in open(filelist).readlines()]\n",
    "\n",
    "    train_files = fetch_files(os.path.join(path, 'train_files.txt'))\n",
    "    test_files = fetch_files(os.path.join(path, 'test_files.txt'))\n",
    "\n",
    "    return num_classes, train_files, test_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, torch, h5py, warnings, numpy as np\n",
    "import open3d as o3d\n",
    "import torch.utils.data\n",
    "\n",
    "class H5PyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_list, num_point=2048, data_aug=0):\n",
    "        self.num_point = num_point\n",
    "        self.file_list = file_list\n",
    "        self.points_list = np.zeros((1, num_point, 3))\n",
    "        self.labels_list = np.zeros((1,))\n",
    "        self.data_aug = data_aug\n",
    "\n",
    "        for file in self.file_list:\n",
    "            data, label = self.loadh5DataFile(file)\n",
    "            self.points_list = np.concatenate(\n",
    "                [self.points_list, data[:, :self.num_point, :]], axis=0)\n",
    "            self.labels_list = np.concatenate([self.labels_list, label.ravel()], axis=0)\n",
    "        \n",
    "        self.points_list = self.points_list[1:]\n",
    "        self.labels_list = self.labels_list[1:]\n",
    "        assert len(self.points_list) == len(self.labels_list)\n",
    "        print('Number of Objects: ', len(self.labels_list))\n",
    "\n",
    "    @staticmethod\n",
    "    def loadh5DataFile(PathtoFile):\n",
    "        f = h5py.File(PathtoFile, 'r')\n",
    "        return f['data'][:], f['label'][:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.points_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        point_xyz = self.points_list[index][:, 0:3]\n",
    "\n",
    "        point_label = self.labels_list[index].astype(np.int32)\n",
    "\n",
    "        return point_xyz, point_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Objects:  9840\n",
      "Number of Objects:  2468\n"
     ]
    }
   ],
   "source": [
    "_, train_files, test_files = modelnet_files(os.path.join(os.environ['DATAROOT'], 'modelnet40_ply_hdf5_2048'))\n",
    "\n",
    "train_files = [os.path.join(os.environ['DATAROOT'], 'modelnet40_ply_hdf5_2048', file.split('/')[-1]) for file in train_files]\n",
    "test_files = [os.path.join(os.environ['DATAROOT'], 'modelnet40_ply_hdf5_2048', file.split('/')[-1]) for file in test_files]\n",
    "\n",
    "\n",
    "train_dataset = H5PyDataset(train_files, num_point=2048)\n",
    "test_dataset = H5PyDataset(test_files, num_point=2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail1.data import batchloader\n",
    "\n",
    "train_loader = batchloader.BatchLoader(train_dataset, batch_size=24, shuffle=False, num_workers=4)\n",
    "test_loader = batchloader.BatchLoader(test_dataset, batch_size=24, shuffle=False, num_workers=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.modules import dgcnn_cls\n",
    "\n",
    "\n",
    "def encode_fn(model, x):\n",
    "\n",
    "    x = x.transpose(2, 1).contiguous()\n",
    "\n",
    "    batch_size, _, num_points = x.size()\n",
    "    x = dgcnn_cls.get_graph_feature(\n",
    "        x, k=model.k\n",
    "    )  # (batch_size, 3, num_points) -> (batch_size, 3*2, num_points, k)\n",
    "    x = model.conv1(\n",
    "        x\n",
    "    )  # (batch_size, 3*2, num_points, k) -> (batch_size, 64, num_points, k)\n",
    "    x1 = x.max(dim=-1, keepdim=False)[\n",
    "        0\n",
    "    ]  # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n",
    "\n",
    "    x = dgcnn_cls.get_graph_feature(\n",
    "        x1, k=model.k\n",
    "    )  # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n",
    "    x = model.conv2(\n",
    "        x\n",
    "    )  # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n",
    "    x2 = x.max(dim=-1, keepdim=False)[\n",
    "        0\n",
    "    ]  # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n",
    "\n",
    "    x = dgcnn_cls.get_graph_feature(\n",
    "        x2, k=model.k\n",
    "    )  # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n",
    "    x = model.conv3(\n",
    "        x\n",
    "    )  # (batch_size, 64*2, num_points, k) -> (batch_size, 128, num_points, k)\n",
    "    x3 = x.max(dim=-1, keepdim=False)[\n",
    "        0\n",
    "    ]  # (batch_size, 128, num_points, k) -> (batch_size, 128, num_points)\n",
    "\n",
    "    x = dgcnn_cls.get_graph_feature(\n",
    "        x3, k=model.k\n",
    "    )  # (batch_size, 128, num_points) -> (batch_size, 128*2, num_points, k)\n",
    "    x = model.conv4(\n",
    "        x\n",
    "    )  # (batch_size, 128*2, num_points, k) -> (batch_size, 256, num_points, k)\n",
    "    x4 = x.max(dim=-1, keepdim=False)[\n",
    "        0\n",
    "    ]  # (batch_size, 256, num_points, k) -> (batch_size, 256, num_points)\n",
    "\n",
    "    x = torch.cat((x1, x2, x3, x4), dim=1)  # (batch_size, 512, num_points)\n",
    "\n",
    "    x = model.conv5(\n",
    "        x\n",
    "    )  # (batch_size, 512, num_points) -> (batch_size, feat_dims, num_points)\n",
    "    x = x.max(dim=-1, keepdim=True)[\n",
    "        0\n",
    "    ]  # (batch_size, feat_dims, num_points) -> (batch_size, feat_dims)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from rail1.utils import math as math_utils\n",
    "X_train, y_train = [], []\n",
    "\n",
    "\n",
    "for i in range(len(train_loader)):\n",
    "\n",
    "    points, target = train_loader[i]\n",
    "\n",
    "    if points.shape[2] == 6:\n",
    "        points = points[:, :, :3]\n",
    "\n",
    "    points, target = points.float().cuda(), target.long().cuda()\n",
    "\n",
    "    # z = model.encoder.dgcnn_encoder(points)\n",
    "    z = encode_fn(model.encoder.dgcnn_encoder, points)\n",
    "    X_train.append(z.cpu().detach().numpy())\n",
    "    y_train.append(target.cpu().detach().numpy())\n",
    "    \n",
    "\n",
    "X_train = np.concatenate(X_train, axis=0)\n",
    "y_train = np.concatenate(y_train, axis=0)\n",
    "\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = [], []\n",
    "\n",
    "num_test_batches = math_utils.ceildiv(\n",
    "    len(test_loader.dataset), test_loader.batch_size\n",
    ")\n",
    "\n",
    "for i in range(len(test_loader)):\n",
    "\n",
    "        points, target = test_loader[i]\n",
    "    \n",
    "        if points.shape[2] == 6:\n",
    "            points = points[:, :, :3]\n",
    "    \n",
    "        points, target = points.float().cuda(), target.long().cuda()\n",
    "    \n",
    "        z = encode_fn(model.encoder.dgcnn_encoder, points)\n",
    "        X_test.append(z.cpu().detach().numpy())\n",
    "        y_test.append(target.cpu().detach().numpy())\n",
    "\n",
    "X_test = np.concatenate(X_test, axis=0)\n",
    "y_test = np.concatenate(y_test, axis=0)\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.squeeze(-1)\n",
    "X_test = X_test.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.693303"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.61787605"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfer linear SVM accuracy: 91.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, metrics\n",
    "# Import standard scaler\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "clf = svm.LinearSVC(random_state=0, max_iter=1000, dual='auto', C=0.01)\n",
    "clf.fit(X_train, y_train)\n",
    "result = clf.predict(X_test)  \n",
    "accuracy = np.sum(result==y_test).astype(float) / np.size(y_test)\n",
    "print(\"Transfer linear SVM accuracy: {:.2f}%\".format(accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, 1.0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.min(), X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9840, 1024)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
