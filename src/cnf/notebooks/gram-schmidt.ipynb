{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj(v, u):\n",
    "    return torch.dot(v, u) / torch.dot(u, u) * u\n",
    "\n",
    "def gram_schmidt(v):\n",
    "    u = v.clone()\n",
    "    u[:, 1] -= proj(u[:, 1], u[:, 0])\n",
    "    u[:, 2] -= proj(u[:, 2], u[:, 0]) + proj(u[:, 2], u[:, 1])\n",
    "    return u\n",
    "\n",
    "v = torch.randn(3, 3)\n",
    "u = gram_schmidt(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-1.3878e-17), tensor(0.), tensor(0.))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(u[:, 0], u[:, 1]), torch.dot(u[:, 0], u[:, 2]), torch.dot(u[:, 1], u[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.3878e-16), tensor(0.), tensor(5.5511e-17))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def proj(v, u):\n",
    "    return torch.sum(v * u, 1, keepdim=True) / torch.sum(u * u, 1, keepdim=True) * u\n",
    "\n",
    "def gram_schmidt(v):\n",
    "    u = v.clone()\n",
    "    u[:, :, 1] -= proj(u[:, :, 1], u[:, :, 0])\n",
    "    u[:, :, 2] -= proj(u[:, :, 2], u[:, :, 0]) + proj(u[:, :, 2], u[:, :, 1])\n",
    "    return u\n",
    "\n",
    "v = torch.randn(3, 3, 3)\n",
    "u = gram_schmidt(v)\n",
    "u = u[0]\n",
    "\n",
    "torch.dot(u[:, 0], u[:, 1]), torch.dot(u[:, 0], u[:, 2]), torch.dot(u[:, 1], u[:, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_schmidt(v):\n",
    "    u = v.clone()\n",
    "    for i in range(1, u.shape[2]):  # Assuming the last dimension is the one along which vectors are defined\n",
    "        for j in range(i):\n",
    "            u[:, :, i] -= proj(u[:, :, i], u[:, :, j])\n",
    "    u /= torch.norm(u, dim=2, keepdim=True)  # Normalize each vector\n",
    "    print(torch.einsum('bnd,bmd->bnm', u, u)[0])  # Debugging output\n",
    "    return u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000,  0.3553, -0.6161],\n",
      "        [ 0.3553,  1.0000,  0.4818],\n",
      "        [-0.6161,  0.4818,  1.0000]])\n"
     ]
    }
   ],
   "source": [
    "result = gram_schmidt(torch.randn(1024, 3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.3553, -0.6161],\n",
       "        [ 0.3553,  1.0000,  0.4818],\n",
       "        [-0.6161,  0.4818,  1.0000]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('bnd,bmd->bnm', result, result)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
