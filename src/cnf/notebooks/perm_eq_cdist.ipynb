{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/druhe/rail1/src/cnf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/druhe/rail1/src/cnf/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /home/druhe/rail1/src/cnf\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['DATAROOT'] = '/home/druhe/datasets'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PermutationBilinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # self.weight = nn.Parameter(torch.randn(num_heads * output_dim, input_dim) / input_dim ** 0.5)\n",
    "        # self.bias = nn.Parameter(torch.zeros(num_heads * output_dim))\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, output_dim * num_heads)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, C = x.size()\n",
    "        x = x.transpose(2, 1)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        x = x.reshape(B, H, self.num_heads, self.output_dim)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "data = torch.randn(32, 1024, 3)\n",
    "perm = torch.randperm(1024)\n",
    "data_perm = data[:, perm, :]\n",
    "\n",
    "ip = torch.einsum('bik, bjk -> bij', data, data)\n",
    "ip_perm = torch.einsum('bik, bjk -> bij', data_perm, data_perm)\n",
    "\n",
    "\n",
    "bil = PermutationBilinear(1024, 512, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 1024])\n",
      "torch.Size([32, 3, 1024])\n"
     ]
    }
   ],
   "source": [
    "z = bil(data)\n",
    "z_perm = bil(data_perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [-2.1458e-06,  4.5300e-06, -8.2254e-06,  ...,  2.5034e-06,\n",
       "          4.7684e-07,  6.4820e-06],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        ...,\n",
       "        [ 0.0000e+00, -7.2957e-07, -7.1526e-07,  ...,  4.7684e-07,\n",
       "          3.5763e-07, -4.7684e-07],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 2.3842e-07, -2.9802e-08, -4.7684e-07,  ..., -9.5367e-07,\n",
       "         -1.1921e-07,  5.9605e-08]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "z[:, perm] - z_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.3644e-07, -4.7684e-07, -7.1526e-07],\n",
       "         [-5.3644e-07, -4.7684e-07, -7.1526e-07]],\n",
       "\n",
       "        [[ 2.1458e-06, -6.1989e-06, -8.3447e-07],\n",
       "         [ 2.1458e-06, -6.1989e-06, -8.3447e-07]],\n",
       "\n",
       "        [[ 1.3113e-06,  3.8147e-06,  2.3842e-06],\n",
       "         [ 1.3113e-06,  3.8147e-06,  2.3842e-06]],\n",
       "\n",
       "        [[-8.3447e-07, -4.7684e-07, -2.3842e-07],\n",
       "         [-8.3447e-07, -4.7684e-07, -2.3842e-07]],\n",
       "\n",
       "        [[ 5.9605e-08, -5.3644e-07, -1.7881e-06],\n",
       "         [ 5.9605e-08, -5.3644e-07, -1.7881e-06]],\n",
       "\n",
       "        [[-1.4305e-06, -2.3842e-06, -4.7684e-06],\n",
       "         [-1.4305e-06, -2.3842e-06, -4.7684e-06]],\n",
       "\n",
       "        [[ 3.8147e-06, -1.0729e-06,  2.3842e-07],\n",
       "         [ 3.8147e-06, -1.0729e-06,  2.3842e-07]],\n",
       "\n",
       "        [[ 1.1921e-06, -1.9073e-06,  2.9802e-08],\n",
       "         [ 1.1921e-06, -1.9073e-06,  2.9802e-08]],\n",
       "\n",
       "        [[ 2.3842e-06, -3.5763e-06, -2.3842e-07],\n",
       "         [ 2.3842e-06, -3.5763e-06, -2.3842e-07]],\n",
       "\n",
       "        [[ 1.1921e-07,  4.4703e-07,  4.7684e-07],\n",
       "         [ 1.1921e-07,  4.4703e-07,  4.7684e-07]],\n",
       "\n",
       "        [[ 8.9407e-07,  4.7684e-07, -1.9073e-06],\n",
       "         [ 8.9407e-07,  4.7684e-07, -1.9073e-06]],\n",
       "\n",
       "        [[-6.6757e-06,  4.7684e-07,  5.9605e-08],\n",
       "         [-6.6757e-06,  4.7684e-07,  5.9605e-08]],\n",
       "\n",
       "        [[ 1.1921e-07, -3.0994e-06,  3.8743e-07],\n",
       "         [ 1.1921e-07, -3.0994e-06,  3.8743e-07]],\n",
       "\n",
       "        [[ 4.7684e-07, -1.1921e-06, -1.7881e-06],\n",
       "         [ 4.7684e-07, -1.1921e-06, -1.7881e-06]],\n",
       "\n",
       "        [[-3.2783e-07,  0.0000e+00, -3.0994e-06],\n",
       "         [-3.2783e-07,  0.0000e+00, -3.0994e-06]],\n",
       "\n",
       "        [[-7.1526e-07, -2.3842e-07, -2.6226e-06],\n",
       "         [-7.1526e-07, -2.3842e-07, -2.6226e-06]],\n",
       "\n",
       "        [[ 3.5763e-07, -2.6226e-06, -3.5763e-07],\n",
       "         [ 3.5763e-07, -2.6226e-06, -3.5763e-07]],\n",
       "\n",
       "        [[-2.8610e-06,  4.0531e-06,  2.0862e-07],\n",
       "         [-2.8610e-06,  4.0531e-06,  2.0862e-07]],\n",
       "\n",
       "        [[ 5.3644e-07, -1.5497e-06, -3.8743e-07],\n",
       "         [ 5.3644e-07, -1.5497e-06, -3.8743e-07]],\n",
       "\n",
       "        [[-7.4506e-08, -4.0531e-06,  3.5763e-07],\n",
       "         [-7.4506e-08, -4.0531e-06,  3.5763e-07]],\n",
       "\n",
       "        [[ 9.5367e-07, -7.1526e-07, -1.1921e-07],\n",
       "         [ 9.5367e-07, -7.1526e-07, -1.1921e-07]],\n",
       "\n",
       "        [[-2.9802e-07,  2.3842e-07,  3.5763e-07],\n",
       "         [-2.9802e-07,  2.3842e-07,  3.5763e-07]],\n",
       "\n",
       "        [[ 1.1921e-06, -1.1921e-07,  1.6689e-06],\n",
       "         [ 1.1921e-06, -1.1921e-07,  1.6689e-06]],\n",
       "\n",
       "        [[ 1.3113e-06,  9.7752e-06, -1.1325e-06],\n",
       "         [ 1.3113e-06,  9.7752e-06, -1.1325e-06]],\n",
       "\n",
       "        [[ 1.2666e-07,  1.4782e-05, -5.9605e-07],\n",
       "         [ 1.2666e-07,  1.4782e-05, -5.9605e-07]],\n",
       "\n",
       "        [[-7.1526e-07, -9.5367e-07,  7.1526e-07],\n",
       "         [-7.1526e-07, -9.5367e-07,  7.1526e-07]],\n",
       "\n",
       "        [[-2.8610e-06, -7.1526e-06, -9.5367e-07],\n",
       "         [-2.8610e-06, -7.1526e-06, -9.5367e-07]],\n",
       "\n",
       "        [[ 5.2452e-06,  4.7684e-07,  0.0000e+00],\n",
       "         [ 5.2452e-06,  4.7684e-07,  0.0000e+00]],\n",
       "\n",
       "        [[-4.7684e-07,  0.0000e+00, -3.5763e-07],\n",
       "         [-4.7684e-07,  0.0000e+00, -3.5763e-07]],\n",
       "\n",
       "        [[-9.5367e-07, -3.5763e-06,  1.1921e-07],\n",
       "         [-9.5367e-07, -3.5763e-06,  1.1921e-07]],\n",
       "\n",
       "        [[ 1.1921e-06, -2.6822e-07,  4.7684e-07],\n",
       "         [ 1.1921e-06, -2.6822e-07,  4.7684e-07]],\n",
       "\n",
       "        [[ 2.3842e-07, -7.1526e-06, -7.1526e-07],\n",
       "         [ 2.3842e-07, -7.1526e-06, -7.1526e-07]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [B, L, 3] -> [B, L / 2, 3]\n",
    "\n",
    "\n",
    "# [B, L, 1, 3] -> # [B, L, L/2, 3]\n",
    "\n",
    "# [B, L, L / 2, 3] x [B, L, 3] -> [B, L / 2, C]\n",
    "\n",
    "\n",
    "class PermutationBilinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_heads\n",
    "        # self.bilinear = nn.Bilinear(input_dim, input_dim, output_dim)\n",
    "        self.weight = nn.Parameter(torch.randn(output_dim, input_dim) / input_dim ** 0.5)\n",
    "\n",
    "        # W [L / 2, L] steerable\n",
    "        # Can do attention [L, C] -> [L, C / 2]]\n",
    "\n",
    "    def forward(self, input):\n",
    "        B, L, C = input.size()\n",
    "        h = input.unsqueeze(2).expand(B, L, self.num_heads, C) # Can be done with linear [C -> C * num_heads]\n",
    "        energy = torch.einsum('blhc, oc, blc -> bloh', h, self.weight, input)\n",
    "\n",
    "        attn = F.softmax(energy, dim=1)\n",
    "\n",
    "        output = torch.einsum('bloh, blhc -> bho', attn, h)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "pmb = PermutationBilinear(3, 3, 2)\n",
    "\n",
    "output = pmb(data)\n",
    "output_perm = pmb(data_perm)\n",
    "\n",
    "output - output_perm\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = MultiHeadAttention(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 1024, 1024]) torch.Size([32, 1024, 1, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_pyg230/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[61], line 38\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, values, keys, query, mask)\u001b[0m\n\u001b[1;32m     35\u001b[0m attention \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(energy \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_size \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(attention\u001b[38;5;241m.\u001b[39mshape, values\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     40\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnhql,nlhd->nqhd\u001b[39m\u001b[38;5;124m\"\u001b[39m, [attention, values])\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m     41\u001b[0m     N, query_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     44\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(out)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "attn(data, data, data, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32768x3 and 4x4096)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m K_lin \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(in_channels, out_channels \u001b[38;5;241m*\u001b[39m num_heads)\n\u001b[1;32m      9\u001b[0m V_lin \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(in_channels, out_channels \u001b[38;5;241m*\u001b[39m num_heads)\n\u001b[0;32m---> 11\u001b[0m Q \u001b[38;5;241m=\u001b[39m \u001b[43mQ_lin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m K \u001b[38;5;241m=\u001b[39m K_lin(x)\n\u001b[1;32m     13\u001b[0m V \u001b[38;5;241m=\u001b[39m V_lin(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_pyg230/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_pyg230/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32768x3 and 4x4096)"
     ]
    }
   ],
   "source": [
    "x = torch.randn(32, 1024, 3)\n",
    "\n",
    "in_channels = 4\n",
    "out_channels = 8\n",
    "num_heads = 512 \n",
    "\n",
    "Q_lin = nn.Linear(in_channels, out_channels * num_heads)\n",
    "K_lin = nn.Linear(in_channels, out_channels * num_heads)\n",
    "V_lin = nn.Linear(in_channels, out_channels * num_heads)\n",
    "\n",
    "Q = Q_lin(x)\n",
    "K = K_lin(x)\n",
    "V = V_lin(x)\n",
    "\n",
    "attn_layer = nn.MultiheadAttention(out_channels * num_heads, 64)\n",
    "\n",
    "output, weights = attn_layer(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, 512, 6]' is invalid for input of size 192",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[32, 512, 6]' is invalid for input of size 192"
     ]
    }
   ],
   "source": [
    "output = output.sum(1).reshape(32, num_heads, out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 512, 6])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.Q_lin = nn.Linear(in_channels, out_channels * num_heads)\n",
    "        self.K_lin = nn.Linear(in_channels, out_channels * num_heads)\n",
    "        self.V_lin = nn.Linear(in_channels, out_channels * num_heads)\n",
    "\n",
    "        self.attn_layer = nn.MultiheadAttention(out_channels * num_heads, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.Q_lin(x)\n",
    "        K = self.K_lin(x)\n",
    "        V = self.V_lin(x)\n",
    "\n",
    "        output, weights = self.attn_layer(Q, K, V)\n",
    "\n",
    "        output = output.sum(1).reshape(32, self.num_heads, self.out_channels)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = MultiHeadAttention(4, 8, 512)\n",
    "        self.layer2 = MultiHeadAttention(8, 16, 256)\n",
    "        self.layer3 = MultiHeadAttention(16, 32, 128)\n",
    "        self.layer4 = MultiHeadAttention(32, 64, 64)\n",
    "        self.layer5 = MultiHeadAttention(64, 128, 32)\n",
    "        self.layer6 = MultiHeadAttention(128, 256, 16)\n",
    "        self.layer7 = MultiHeadAttention(256, 512, 8)\n",
    "        self.layer8 = MultiHeadAttention(512, 1024, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        x = self.layer8(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network().cuda()\n",
    "input = torch.randn(32, 1024, 4).cuda()\n",
    "output = network(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4, 1024])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PermutationBilinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.weight = nn.Parameter(torch.randn(output_dim, input_dim) / input_dim ** 0.5)\n",
    "\n",
    "        # W [L / 2, L] steerable\n",
    "        # Can do attention [L, C] -> [L, C / 2]]\n",
    "\n",
    "    def forward(self, input):\n",
    "        B, L, C = input.size()\n",
    "        h = input.unsqueeze(2).expand(B, L, self.num_heads, C) # Can be done with linear [C -> C * num_heads]\n",
    "        energy = torch.einsum('blhc, oc, blc -> bloh', h, self.weight, input)\n",
    "\n",
    "        attn = F.softmax(energy, dim=1)\n",
    "\n",
    "        output = torch.einsum('bloh, blhc -> bho', attn, h)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = PermutationBilinear(4, 8, 512)\n",
    "        self.layer2 = PermutationBilinear(8, 16, 256)\n",
    "        self.layer3 = PermutationBilinear(16, 32, 128)\n",
    "        self.layer4 = PermutationBilinear(32, 64, 64)\n",
    "        self.layer5 = PermutationBilinear(64, 128, 32)\n",
    "        self.layer6 = PermutationBilinear(128, 256, 16)\n",
    "        self.layer7 = PermutationBilinear(256, 512, 8)\n",
    "        self.layer8 = PermutationBilinear(512, 1024, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        x = self.layer8(x)\n",
    "\n",
    "        return x \n",
    "    \n",
    "network = Network()\n",
    "input = torch.randn(32, 1024, 4)\n",
    "output = network(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4, 1024])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(32, 1024, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [B, L, K, C] # knn\n",
    "\n",
    "# [B, L, C'] # message passing\n",
    "\n",
    "# [B, L, L'] # Get permutation- steerable kernel\n",
    "\n",
    "# [B, L / 2, C'] # Apply permutation- steerable kernel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [B, L, L, 1] cdist\n",
    "# [B, L, L, C] apply kernel\n",
    "# [B, L, C] sum\n",
    "# [B, L, L'] get permutation-steerable kernel\n",
    "# [B, L, C] apply kernel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "expecting key weights shape of (3, 3), but got torch.Size([3, 6])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     14\u001b[0m layer \u001b[38;5;241m=\u001b[39m Layer(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_pyg230/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[40], line 11\u001b[0m, in \u001b[0;36mLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_pyg230/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_pyg230/lib/python3.10/site-packages/torch/nn/modules/activation.py:1142\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         query, key, value \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (query, key, value)]\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qkv_same_embed_dim:\n\u001b[0;32m-> 1142\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_separate_proj_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq_proj_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_proj_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv_proj_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1153\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1154\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1155\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask, need_weights\u001b[38;5;241m=\u001b[39mneed_weights,\n\u001b[1;32m   1160\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask, average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_pyg230/lib/python3.10/site-packages/torch/nn/functional.py:5075\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   5073\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5074\u001b[0m         b_q, b_k, b_v \u001b[38;5;241m=\u001b[39m in_proj_bias\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m-> 5075\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43m_in_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_v\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5077\u001b[0m \u001b[38;5;66;03m# prep attention mask\u001b[39;00m\n\u001b[1;32m   5078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_pyg230/lib/python3.10/site-packages/torch/nn/functional.py:4808\u001b[0m, in \u001b[0;36m_in_projection\u001b[0;34m(q, k, v, w_q, w_k, w_v, b_q, b_k, b_v)\u001b[0m\n\u001b[1;32m   4806\u001b[0m Eq, Ek, Ev \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), k\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), v\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   4807\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m w_q\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (Eq, Eq), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting query weights shape of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(Eq,\u001b[38;5;250m \u001b[39mEq)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw_q\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 4808\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m w_k\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (Eq, Ek), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting key weights shape of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(Eq,\u001b[38;5;250m \u001b[39mEk)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw_k\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4809\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m w_v\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (Eq, Ev), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting value weights shape of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(Eq,\u001b[38;5;250m \u001b[39mEv)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw_v\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4810\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m b_q \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m b_q\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (Eq,), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting query bias shape of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(Eq,)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb_q\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: expecting key weights shape of (3, 3), but got torch.Size([3, 6])"
     ]
    }
   ],
   "source": [
    "class Layer(nn.Module):\n",
    "    def __init__(self, channels, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attn = nn.MultiheadAttention(channels, num_heads, batch_first=True, vdim=2*channels, kdim=2 * channels,)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.attn(x, x, x)[0]\n",
    "\n",
    "x = torch.randn(32, 1024, 3)\n",
    "layer = Layer(3, 1)\n",
    "output = layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(32, 1024, 3)\n",
    "\n",
    "\n",
    "w = torch.randn(512, 3)\n",
    "\n",
    "x_w = F.linear(x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s = torch.einsum('bic, bik -> bkc', x, x_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = nn.MultiheadAttention(3, 1, batch_first=True)\n",
    "\n",
    "res, _ = attn(x_s, x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 4\n",
    "a = torch.randn(d, 3)\n",
    "perm = torch.randperm(d)\n",
    "\n",
    "a_perm = a[perm]\n",
    "# cdist = torch.cdist(a, a)\n",
    "# cdist_perm = torch.cdist(a_perm, a_perm)\n",
    "\n",
    "a_norm = torch.norm(a, dim=1)\n",
    "a_perm_norm = torch.norm(a_perm, dim=1)\n",
    "\n",
    "w = torch.randn(1, d)\n",
    "# w = torch.ones(1, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.linspace(-1, 1, 4)\n",
    "\n",
    "\n",
    "d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_norm = a_norm[:, None] ** 2 @ wd\n",
    "a_perm_norm = a_perm_norm[:, None] ** 2 @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.2649, -0.4269,  0.2583,  0.3775],\n",
       "         [ 2.2522, -0.7602,  0.4598,  0.6721],\n",
       "         [ 2.8475, -0.9611,  0.5814,  0.8498],\n",
       "         [ 3.0734, -1.0374,  0.6275,  0.9172]]),\n",
       " tensor([[ 1.2649, -0.4269,  0.2583,  0.3775],\n",
       "         [ 2.2522, -0.7602,  0.4598,  0.6721],\n",
       "         [ 2.8475, -0.9611,  0.5814,  0.8498],\n",
       "         [ 3.0734, -1.0374,  0.6275,  0.9172]]))"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_norm, a_perm_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = a_norm * a_norm\n",
    "proc_perm = a_perm_norm * a_perm_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.5999, 0.1823, 0.0667, 0.1425],\n",
       "         [5.0724, 0.5779, 0.2115, 0.4518],\n",
       "         [8.1083, 0.9238, 0.3380, 0.7221],\n",
       "         [9.4458, 1.0761, 0.3938, 0.8413]]),\n",
       " tensor([[1.5999, 0.1823, 0.0667, 0.1425],\n",
       "         [5.0724, 0.5779, 0.2115, 0.4518],\n",
       "         [8.1083, 0.9238, 0.3380, 0.7221],\n",
       "         [9.4458, 1.0761, 0.3938, 0.8413]]))"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc, proc_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_max = proc == proc.max(1).values[:, None]\n",
    "is_max_perm = proc_perm == proc_perm.max(1).values[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False],\n",
       "        [ True, False, False, False],\n",
       "        [ True, False, False, False],\n",
       "        [ True, False, False, False]])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False],\n",
       "        [ True, False, False, False],\n",
       "        [ True, False, False, False],\n",
       "        [ True, False, False, False]])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_max_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0210, 2.0210, 2.0210, 0.0000])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.2000, 2.0000, 2.0100],\n",
       "        [0.2000, 0.0000, 2.0100, 2.0000],\n",
       "        [2.0000, 2.0100, 0.0000, 0.2000],\n",
       "        [2.0100, 2.0000, 0.2000, 0.0000]])"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "[-0.1, 1],\n",
    "[0.1, 1],\n",
    "[-0.1, -1],\n",
    "[0.1, -1],\n",
    "]\n",
    ")\n",
    "\n",
    "torch.cdist(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0050,  0.9900, -0.9900, -1.0100],\n",
       "        [ 0.9900,  0.0050, -1.0100, -0.9900],\n",
       "        [-0.9900, -1.0100,  0.0050,  0.9900],\n",
       "        [-1.0100, -0.9900,  0.9900,  0.0050]])"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1\n",
    "inner = torch.einsum('ni, mi->nm', x, x)\n",
    "\n",
    "inner - torch.diag(x.norm(dim=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0100,  0.9900, -0.9900, -1.0100],\n",
       "        [ 0.9900,  1.0100, -1.0100, -0.9900],\n",
       "        [-0.9900, -1.0100,  1.0100,  0.9900],\n",
       "        [-1.0100, -0.9900,  0.9900,  1.0100]])"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4448, 0.4360, 0.0602, 0.0590],\n",
       "        [0.4360, 0.4448, 0.0590, 0.0602],\n",
       "        [0.0602, 0.0590, 0.4448, 0.4360],\n",
       "        [0.0590, 0.0602, 0.4360, 0.4448]])"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inner.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 1.0100, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 1.0100, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 1.0100]])"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(x.norm(dim=-1) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = torch.randperm(8)\n",
    "\n",
    "x = torch.randn(8, 3)\n",
    "x_perm = x[perm]\n",
    "w = torch.randn(3, 4)\n",
    "\n",
    "xw = x @ w\n",
    "xw_perm = x_perm @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = xw.T @ x\n",
    "y_perm = xw_perm.T @ x_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = torch.einsum('ni, mi->nm', x, y)\n",
    "ip_perm = torch.einsum('ni, mi->nm', x_perm, y_perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([16.2656, 14.4808, 20.3985,  8.9729]),\n",
       "indices=tensor([7, 0, 6, 6]))"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip.max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([16.2656, 14.4808, 20.3985,  8.9729]),\n",
       "indices=tensor([1, 5, 6, 6]))"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip_perm.max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.3709,  14.4808,   9.5594,   2.5053],\n",
       "        [  4.2277,   0.4717,  -1.7566,  -0.9818],\n",
       "        [ 14.0354,  10.2816,  -4.7372,  -3.5390],\n",
       "        [ -9.3594,  -7.3979,   2.2389,   2.0479],\n",
       "        [ -4.2079,  -3.3659,   3.9725,   2.0707],\n",
       "        [-10.8712,  -0.3860,   9.8752,   4.5297],\n",
       "        [-19.6837,   1.8114,  20.3985,   8.9729],\n",
       "        [ 16.2656,  -4.2352, -15.2898,  -6.5882]])"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.8712,  -0.3860,   9.8752,   4.5297],\n",
       "        [ 16.2656,  -4.2353, -15.2898,  -6.5882],\n",
       "        [ 14.0354,  10.2816,  -4.7372,  -3.5390],\n",
       "        [  4.2277,   0.4717,  -1.7566,  -0.9818],\n",
       "        [ -4.2079,  -3.3659,   3.9725,   2.0707],\n",
       "        [  0.3709,  14.4808,   9.5594,   2.5053],\n",
       "        [-19.6837,   1.8114,  20.3985,   8.9729],\n",
       "        [ -9.3594,  -7.3979,   2.2389,   2.0479]])"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([10.4142, 11.1507, 11.1549]),\n",
       "indices=tensor([1, 5, 4]))"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(x, x).sum(-1).topk(3, largest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 3, 4, 2, 4, 1, 3, 3, 1, 4, 4, 0, 1, 4, 0, 4, 1, 3, 4, 4, 4, 0, 3, 3,\n",
      "        2, 3, 2, 4, 1, 4, 0, 1, 3, 3, 3, 1, 0, 1, 0, 0, 3, 4, 0, 1, 3, 3, 0, 3,\n",
      "        1, 4, 4, 0, 1, 1, 1, 3, 4, 2, 4, 4, 0, 1, 2, 3, 4, 1, 1, 4, 0, 1, 1, 2,\n",
      "        0, 2, 4, 0, 1, 2, 1, 1, 0, 4, 3, 4, 0, 0, 1, 4, 1, 4, 2, 1, 0, 1, 4, 1,\n",
      "        0, 0, 0, 0, 1, 2, 4, 3, 3, 0, 1, 1, 0, 0, 4, 4, 4, 1, 4, 3, 0, 1, 4, 3,\n",
      "        1, 1, 1, 4, 1, 2, 4, 1, 3, 0, 4, 1, 4, 1, 0, 4, 4, 1, 2, 4, 4, 4, 4, 0,\n",
      "        2, 1, 3, 1, 0, 0, 1, 4, 3, 4, 0, 1, 4, 4, 0, 0, 2, 1, 4, 0, 4, 2, 4, 0,\n",
      "        1, 1, 4, 0, 0, 3, 4, 1, 1, 3, 2, 4, 1, 1, 1, 1, 1, 1, 0, 3, 1, 0, 2, 2,\n",
      "        4, 0, 4, 3, 4, 0, 0, 2, 4, 2, 1, 0, 0, 4, 4, 4, 2, 1, 3, 4, 1, 1, 1, 3,\n",
      "        0, 1, 1, 0, 1, 4, 0, 0, 3, 4, 2, 4, 4, 4, 4, 1, 4, 0, 1, 0, 3, 1, 1, 2,\n",
      "        1, 0, 4, 4, 4, 2, 1, 4, 4, 1, 0, 0, 4, 0, 3, 4, 1, 2, 4, 1, 0, 0, 0, 4,\n",
      "        0, 1, 1, 3, 4, 4, 4, 4, 1, 3, 4, 4, 2, 2, 0, 0, 0, 1, 4, 4, 4, 0, 1, 1,\n",
      "        4, 0, 4, 3, 2, 1, 1, 1, 4, 0, 4, 4, 3, 0, 1, 1, 4, 0, 4, 1, 1, 3, 4, 0,\n",
      "        0, 2, 3, 1, 1, 1, 3, 0, 1, 4, 4, 3, 4, 3, 4, 2, 1, 1, 4, 0, 2, 4, 4, 3,\n",
      "        2, 0, 2, 0, 4, 4, 3, 2, 4, 4, 4, 1, 4, 1, 3, 1, 1, 2, 0, 1, 3, 4, 4, 4,\n",
      "        3, 1, 0, 2, 1, 4, 4, 0, 1, 1, 0, 0, 3, 0, 3, 0, 4, 1, 4, 3, 4, 1, 2, 4,\n",
      "        4, 0, 4, 4, 0, 3, 3, 3, 1, 1, 4, 2, 1, 2, 4, 0, 0, 4, 1, 1, 1, 4, 1, 3,\n",
      "        1, 1, 3, 4, 0, 4, 4, 0, 4, 1, 4, 4, 3, 1, 0, 1, 1, 1, 0, 3, 1, 2, 0, 4,\n",
      "        4, 4, 2, 1, 0, 1, 1, 0, 0, 1, 2, 4, 0, 1, 3, 0, 4, 4, 3, 3, 0, 0, 3, 0,\n",
      "        1, 4, 1, 1, 0, 0, 4, 2, 4, 0, 2, 4, 3, 4, 3, 4, 4, 4, 1, 4, 1, 0, 0, 0,\n",
      "        1, 1, 2, 2, 1, 4, 0, 1, 4, 0, 0, 1, 0, 4, 1, 4, 1, 4, 3, 0, 1, 4, 0, 0,\n",
      "        0, 2, 0, 3, 2, 0, 4, 4, 0, 3, 0, 4, 4, 1, 0, 0, 3, 4, 1, 1, 4, 2, 0, 3,\n",
      "        0, 1, 1, 4, 4, 0, 0, 1, 1, 2, 4, 2, 0, 0, 2, 4, 4, 4, 3, 2, 0, 2, 2, 1,\n",
      "        1, 4, 4, 4, 1, 4, 3, 1, 4, 2, 3, 1, 1, 0, 1, 4, 4, 4, 1, 0, 4, 2, 1, 2,\n",
      "        4, 3, 1, 0, 4, 4, 1, 0, 3, 1, 3, 1, 4, 4, 1, 2, 4, 1, 1, 1, 0, 3, 4, 0,\n",
      "        1, 4, 0, 1, 4, 1, 0, 3, 4, 4, 1, 1, 4, 4, 0, 4, 0, 0, 1, 1, 1, 4, 1, 1,\n",
      "        4, 4, 0, 0, 2, 4, 1, 4, 4, 4, 0, 1, 1, 1, 4, 0, 1, 1, 2, 3, 1, 4, 4, 4,\n",
      "        4, 3, 1, 1, 0, 1, 2, 0, 1, 0, 1, 1, 3, 1, 1, 4, 1, 4, 3, 4, 0, 1, 4, 4,\n",
      "        4, 3, 1, 0, 4, 1, 1, 1, 3, 1, 2, 2, 1, 1, 0, 2, 0, 3, 0, 1, 1, 0, 2, 3,\n",
      "        4, 1, 0, 0, 3, 2, 0, 4, 1, 0, 3, 0, 4, 3, 1, 4, 0, 4, 3, 3, 0, 0, 1, 4,\n",
      "        1, 0, 3, 4, 3, 0, 4, 2, 4, 4, 0, 4, 1, 0, 0, 1, 0, 0, 3, 0, 0, 4, 3, 1,\n",
      "        4, 2, 1, 4, 0, 0, 1, 1, 1, 4, 2, 1, 1, 4, 4, 0, 4, 1, 3, 1, 2, 4, 1, 1,\n",
      "        1, 0, 0, 1, 1, 1, 2, 4, 4, 4, 1, 3, 0, 0, 4, 3, 4, 2, 4, 0, 2, 4, 0, 4,\n",
      "        4, 4, 1, 1, 1, 4, 4, 1, 1, 4, 0, 0, 0, 0, 4, 0, 0, 4, 4, 1, 2, 1, 3, 0,\n",
      "        0, 4, 4, 3, 4, 4, 4, 0, 0, 4, 0, 1, 1, 0, 3, 4, 1, 4, 4, 1, 3, 0, 4, 0,\n",
      "        1, 1, 3, 2, 3, 4, 1, 3, 3, 4, 1, 1, 0, 3, 3, 4, 1, 3, 1, 4, 1, 0, 1, 0,\n",
      "        4, 1, 3, 0, 4, 1, 1, 1, 0, 1, 4, 4, 4, 0, 0, 0, 0, 4, 4, 2, 0, 1, 3, 3,\n",
      "        1, 0, 0, 1, 1, 4, 0, 2, 0, 4, 1, 2, 2, 3, 4, 0, 1, 1, 4, 3, 1, 1, 0, 4,\n",
      "        4, 2, 1, 1, 3, 3, 4, 4, 1, 3, 3, 0, 0, 1, 3, 0, 1, 0, 1, 1, 1, 4, 0, 1,\n",
      "        3, 3, 1, 4, 0, 1, 3, 3, 1, 0, 1, 3, 3, 0, 1, 1, 1, 4, 4, 0, 4, 1, 1, 1,\n",
      "        4, 4, 0, 2, 3, 4, 4, 4, 0, 3, 1, 1, 2, 1, 4, 3, 1, 3, 2, 0, 0, 2, 0, 4,\n",
      "        3, 0, 4, 1, 4, 0, 3, 4, 1, 1, 0, 1, 3, 4, 0, 4], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[408], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m num_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     38\u001b[0m num_iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m---> 40\u001b[0m centroids, assignments \u001b[38;5;241m=\u001b[39m \u001b[43mkmeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_clusters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[408], line 17\u001b[0m, in \u001b[0;36mkmeans\u001b[0;34m(X, num_clusters, num_iterations, device, tol)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Update centroids and compute the error\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(closest)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_clusters):\n\u001b[1;32m     19\u001b[0m     cluster_points \u001b[38;5;241m=\u001b[39m X[closest \u001b[38;5;241m==\u001b[39m i]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def kmeans(X, num_clusters, num_iterations, device='cpu', tol=1e-4):\n",
    "    # Initialize centroids randomly from the data points\n",
    "    indices = torch.randperm(X.shape[0])[:num_clusters]\n",
    "    centroids = X[indices].to(device)\n",
    "    prev_error = None\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        # Assign each data point to the closest centroid\n",
    "        distances = torch.cdist(X.to(device), centroids)\n",
    "        closest = distances.argmin(dim=1)\n",
    "        error = 0\n",
    "\n",
    "        # Update centroids and compute the error\n",
    "        print(closest)\n",
    "        raise\n",
    "        for i in range(num_clusters):\n",
    "            cluster_points = X[closest == i]\n",
    "            if len(cluster_points) > 0:\n",
    "                new_centroid = cluster_points.mean(dim=0)\n",
    "                error += torch.sum((cluster_points - new_centroid).pow(2))\n",
    "                centroids[i] = new_centroid\n",
    "\n",
    "        # Check for convergence\n",
    "        if prev_error is not None and torch.abs(prev_error - error) < tol:\n",
    "            print(f\"Converged at iteration {_+1}\")\n",
    "            break\n",
    "\n",
    "        prev_error = error\n",
    "\n",
    "    return centroids, closest\n",
    "\n",
    "# Example usage\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "X = torch.randn(1000, 3)  # 1000 data points with 2 features each\n",
    "num_clusters = 5\n",
    "num_iterations = 100\n",
    "\n",
    "centroids, assignments = kmeans(X, num_clusters, num_iterations, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = x.norm(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma, mi = n.argmax(0), n.argmin(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.1554,  3.1600,  0.4453]),\n",
       " tensor([ 0.0844, -0.2514,  0.2357]),\n",
       " tensor([-0.2523, -0.1596, -0.1337]))"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "x[ma], x[mi], x.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
